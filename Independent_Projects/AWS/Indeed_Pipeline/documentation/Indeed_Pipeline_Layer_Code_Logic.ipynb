{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "92b94fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import docx\n",
    "from docx import Document\n",
    "import re\n",
    "import csv \n",
    "import copy\n",
    "import docx2txt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1ed2993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "raw_files = str(current_dir) + r\"\\raw_files\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71d9638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0e668b3-8368-4aee-a3d6-48c1846a059a",
   "metadata": {},
   "source": [
    "# Each word document collected contains about 75 job description div containers, below is how one looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cce1914a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "76\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data analyst_dc_2024-03-22_page1\\n\\n\\n\\n\\n\\n<div id=\"viewJobSSRRoot\"><div id=\"mosaic-aboveViewjobNav\" class=\"mosaic-zone\"></div><div class=\"fastviewjob jobsearch-ViewJobLayout--standalone css-10576t8 eu4oa1w0 hydrated\" role=\"main\"><div class=\"css-amnpyw e37uo190\"></div><div class=\"css-8ua0kf eu4oa1w0\"><div class=\"css-1xwak0u eu4oa1w0\"><div class=\"css-jr3hje eu4oa1w0\"><form action=\"/jobs\" method=\"get\" class=\"css-z48huh e37uo190\"><span class=\" css-1fr7b65 e6fjgti1\"><label id=\"text-input-what-label\" for=\"text-input-what\" aria-hidden=\"true\" class=\" css-1ndt6qv eu4oa1w0\">What</label><input type=\"text\" aria-invalid=\"false\" id=\"text-input-what\" aria-label=\"what: job title, keywords, or company\" name=\"q\" autocomplete=\"off\" placeholder=\"Job title, keywords, or company\" value=\"\" class=\"css-33odco e1jgz0i3\"></span><span class=\" css-1lglf0j e6fjgti1\"><label id=\"text-input-where-label\" for=\"text-input-where\" aria-hidden=\"true\" class=\" css-1ndt6qv eu4oa1w0\">Where</label><input type=\"text\" aria-invalid=\"false\" id=\"text-input-where\" aria-label=\"where: city, state, or zip code\" name=\"l\" autocomplete=\"off\" placeholder=\"City, state, zip code, or &quot;remote&quot;\" value=\"washington, dc\" class=\"css-byulhf e1jgz0i3\"></span><button type=\"submit\" class=\"css-au3piq e8ju0x51\">Find Jobs</button></form></div></div><div id=\"jobsearch-ViewJobLayout-rowSpacingLine\" class=\"css-1w4b90s eu4oa1w0\"></div></div><div class=\"css-1quav7f eu4oa1w0\"><div class=\"css-a8ixh3 eu4oa1w0\"><div class=\"css-52a5z7 eu4oa1w0\"><div class=\"jobsearch-JobComponent css-u4y1in eu4oa1w0\"><div class=\"jobsearch-DesktopStickyContainerTrigger css-s3qg96 eu4oa1w0\"></div><div class=\"jobsearch-InfoHeaderContainer jobsearch-DesktopStickyContainer css-zt53js eu4oa1w0\"><div><div class=\"jobsearch-JobInfoHeader-title-container  css-bbq8li eu4oa1w0\"><h1 class=\"jobsearch-JobInfoHeader-title css-1hwk56k e1tiznh50\" lang=\"en\" dir=\"ltr\" data-testid=\"jobsearch-JobInfoHeader-title\"><span>Data Analyst</span></h1></div><div data-testid=\"jobsearch-CompanyInfoContainer\" class=\"css-2wyr5j eu4oa1w0\"><div class=\"jobsearch-CompanyInfoWithoutHeaderImage css-kyg8or eu4oa1w0\"><div><div class=\"css-39gvaf eu4oa1w0\"><div class=\"css-1h46us2 eu4oa1w0\"><div data-company-name=\"true\" data-testid=\"inlineHeader-companyName\" elementtiming=\"significant-render\" class=\"css-hon9z8 eu4oa1w0\"><span class=\"css-1cxc9zk e1wnkr790\"><a href=\"https://www.indeed.com/cmp/The-Squires-Group-3?campaignid=mobvjcmp&amp;from=mobviewjob&amp;tk=1hpje6reqikfq800&amp;fromjk=0e0683f6e0411f6c\" target=\"_blank\" class=\"css-1f8zkg3 e19afand0\">The Squires Group</a></span></div></div><div data-testid=\"inlineHeader-companyLocation\" class=\"css-17cdm7w eu4oa1w0\"><div data-testid=\"job-location\" class=\"css-1ojh0uo eu4oa1w0\">The Squires Group in Arlington, VA 22209</div></div><div class=\"css-17cdm7w eu4oa1w0\"><div>Hybrid work</div></div></div></div></div></div></div><div data-testid=\"jobsearch-OtherJobDetailsContainer\" class=\"css-kyg8or eu4oa1w0\"><div><div id=\"salaryInfoAndJobType\" class=\"css-1xkrvql eu4oa1w0\"><span class=\"css-k5flys eu4oa1w0\">Full-time, Contract</span></div></div></div><div class=\"css-gjyqp7 eu4oa1w0\"><div class=\"css-7hagea eu4oa1w0\"></div></div><div class=\"css-1scistc e37uo190\"><div class=\"jobsearch-StickyContainerDivider-line css-8q3sg6 eu4oa1w0\"></div></div><div class=\" css-kyg8or eu4oa1w0\"></div></div><div class=\" css-1omm75o eu4oa1w0\" tabindex=\"0\"><div class=\"jobsearch-BodyContainer\"><div></div><div class=\"jobsearch-JobComponent-description css-10ybyod eu4oa1w0\"><div id=\"mosaic-vjBelowViewJobHeader\" class=\"mosaic-zone\"></div><div id=\"mosaic-vjProfileInsights\" class=\"mosaic-zone\" data-testid=\"vjProfileInsights-test\"><div id=\"js-match-insights-provider\" class=\"mosaic js-match-insights-provider mosaic-provider-hydrated\"><div class=\"css-kyg8or eu4oa1w0\"><div class=\"js-match-insights-provider-kyg8or eu4oa1w0\"><div class=\"js-match-insights-provider-xu0md5 eu4oa1w0\"><div class=\"js-match-insights-provider-36vfsm eu4oa1w0\"><div class=\"js-match-insights-provider-1283b4z eu4oa1w0\"><h2 class=\"js-match-insights-provider-14dlqhn e1tiznh50\">Profile insights</h2><span class=\"js-match-insights-provider-xvmbeo e1wnkr790\"><span>Hereâ€™s how the job qualifications align with your <a href=\"https://profile.indeed.com\" aria-label=\"profile (opens in a new window)\" target=\"_blank\" rel=\"noopener\" class=\"js-match-insights-provider-1cr09u7 e19afand0\">profile<svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 24 24\" aria-hidden=\"true\" class=\" js-match-insights-provider-r5jz5s eac13zx0\"><path d=\"M14.504 3a.5.5 0 00-.5.5v1a.5.5 0 00.5.5h3.085l-9.594 9.594a.5.5 0 000 .707l.707.708a.5.5 0 00.707 0l9.594-9.595V9.5a.5.5 0 00.5.5h1a.5.5 0 00.5-.5v-6a.5.5 0 00-.5-.5h-6z\"></path><path d=\"M5 3.002a2 2 0 00-2 2v13.996a2 2 0 001.996 2.004h14a2 2 0 002-2v-6.5a.5.5 0 00-.5-.5h-1a.5.5 0 00-.5.5v6.5L5 18.998V5.002L11.5 5a.495.495 0 00.496-.498v-1a.5.5 0 00-.5-.5H5z\"></path></svg></a>.</span></span></div><div class=\"js-match-insights-provider-h05mm8 e37uo190\"><div role=\"group\" tabindex=\"0\" aria-label=\"Licenses\" class=\"js-match-insights-provider-16m282m e37uo190\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 20 20\" data-testid=\"section-icon\" aria-hidden=\"true\" class=\"js-match-insights-provider-1pdva1a eac13zx0\"><path fill-rule=\"evenodd\" d=\"M10 3C6.884 3 6 6 6 6H2.5a.5.5 0 00-.5.5v10a.5.5 0 00.5.5h15a.5.5 0 00.5-.5v-10a.5.5 0 00-.5-.5H14s-.884-3-4-3zm0 1.5C8.27 4.5 7.63 6 7.63 6h4.74S11.73 4.5 10 4.5zm3.184 3.995a.5.5 0 01.708 0l.353.353a.5.5 0 010 .707l-4.95 4.95a.5.5 0 01-.707 0L5.76 11.677a.5.5 0 010-.707l.353-.354a.5.5 0 01.707 0l2.122 2.122 4.242-4.243z\" clip-rule=\"evenodd\"></path></svg><div class=\"js-match-insights-provider-e6s05i eu4oa1w0\"><h3 class=\"js-match-insights-provider-11n8e9a e1tiznh50\">Licenses</h3><ul class=\"js-match-insights-provider-1o7r14h eu4oa1w0\"><li data-testid=\"list-item\" class=\"js-match-insights-provider-10zb82q eu4oa1w0\"><button data-testid=\"Secret Clearance-tile\" aria-label=\"Licenses Secret Clearance missing qualification\" class=\"js-match-insights-provider-nmba9p e1xnxm2i0\"><div class=\"js-match-insights-provider-g6kqeb ecydgvn0\"><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\">Secret Clearance</div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><span class=\"js-match-insights-provider-1gxm6ci e1wnkr790\">&nbsp;(Required)</span></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 18 18\" aria-live=\"polite\" aria-label=\"missing qualification\" aria-hidden=\"false\" class=\"js-match-insights-provider-1jyq4ny eac13zx0\"><path d=\"M5.113 7.693a.5.5 0 010-.707l.212-.213a.5.5 0 01.707 0L9 9.741l2.969-2.968a.5.5 0 01.707 0l.212.213a.5.5 0 010 .707l-3.313 3.312a.387.387 0 01-.009.01l-.212.211a.5.5 0 01-.707 0l-.212-.212-.007-.006-3.315-3.315z\"></path></svg></div></div></button></li></ul><fieldset class=\"js-match-insights-provider-1umaq0k e37uo190\"><legend class=\"js-match-insights-provider-h9kwwf eu4oa1w0\"><div class=\"js-match-insights-provider-kyg8or eu4oa1w0\">Do you have a valid <b>Secret Clearance</b> license?</div></legend><div class=\"js-match-insights-provider-1lps5p3 e37uo190\"><button data-testid=\"Do you have a valid [Secret Clearance|attribute] license?-yes\" class=\"js-match-insights-provider-s1k8bj e8ju0x51\"><span>Yes</span></button><button data-testid=\"Do you have a valid [Secret Clearance|attribute] license?-no\" class=\"js-match-insights-provider-1b1q3os e8ju0x51\"><span>No</span></button><button data-testid=\"Do you have a valid [Secret Clearance|attribute] license?-skip\" class=\"js-match-insights-provider-1ocbp6o e8ju0x51\"><span>Skip</span></button></div></fieldset></div></div><div role=\"group\" tabindex=\"0\" aria-label=\"Skills\" class=\"js-match-insights-provider-16m282m e37uo190\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 20 20\" data-testid=\"section-icon\" aria-hidden=\"true\" class=\"js-match-insights-provider-1pdva1a eac13zx0\"><path d=\"M9.75 1a.5.5 0 00-.5.5v2a.5.5 0 00.5.5h.5a.5.5 0 00.5-.5v-2a.5.5 0 00-.5-.5h-.5zM16 10.25a.5.5 0 00.5.5h2a.5.5 0 00.5-.5v-.5a.5.5 0 00-.5-.5h-2a.5.5 0 00-.5.5v.5zm-14.5.5a.5.5 0 01-.5-.5v-.5a.5.5 0 01.5-.5h2a.5.5 0 01.5.5v.5a.5.5 0 01-.5.5h-2zm2.379-6.518a.5.5 0 000 .707l.707.707a.5.5 0 00.707 0l.354-.353a.5.5 0 000-.707l-.707-.707a.5.5 0 00-.708 0l-.353.353zm12.242.708a.5.5 0 000-.708l-.353-.353a.5.5 0 00-.708 0l-.707.707a.5.5 0 000 .707l.354.353a.5.5 0 00.707 0l.707-.707zM7.5 16v1.5a.5.5 0 00.5.5h4a.5.5 0 00.5-.5V16h-5zm5-2.877a4 4 0 10-5 0V14.5h5v-1.377z\"></path></svg><div class=\"js-match-insights-provider-e6s05i eu4oa1w0\"><h3 class=\"js-match-insights-provider-11n8e9a e1tiznh50\">Skills</h3><ul class=\"js-match-insights-provider-1o7r14h eu4oa1w0\"><li data-testid=\"list-item\" class=\"js-match-insights-provider-10zb82q eu4oa1w0\"><button data-testid=\"Analytics-tile\" aria-label=\"Skills Analytics missing qualification\" class=\"js-match-insights-provider-nmba9p e1xnxm2i0\"><div class=\"js-match-insights-provider-g6kqeb ecydgvn0\"><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\">Analytics</div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 18 18\" aria-live=\"polite\" aria-label=\"missing qualification\" aria-hidden=\"false\" class=\"js-match-insights-provider-1jyq4ny eac13zx0\"><path d=\"M5.113 7.693a.5.5 0 010-.707l.212-.213a.5.5 0 01.707 0L9 9.741l2.969-2.968a.5.5 0 01.707 0l.212.213a.5.5 0 010 .707l-3.313 3.312a.387.387 0 01-.009.01l-.212.211a.5.5 0 01-.707 0l-.212-.212-.007-.006-3.315-3.315z\"></path></svg></div></div></button></li><li data-testid=\"list-item\" class=\"js-match-insights-provider-10zb82q eu4oa1w0\"><button data-testid=\"Tableau-tile\" aria-label=\"Skills Tableau matching qualification\" class=\"js-match-insights-provider-1h4aef e1xnxm2i0\"><div class=\"js-match-insights-provider-g6kqeb ecydgvn0\"><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 24 24\" aria-label=\"matching qualification\" aria-hidden=\"true\" data-icon-type=\"CheckIcon\" class=\"js-match-insights-provider-142yl9z eac13zx0\"><title>matching qualification</title><path d=\"M20.137 7.402a.5.5 0 000-.708l-.709-.708a.5.5 0 00-.707 0L9.167 15.54l-3.892-3.892a.5.5 0 00-.707 0l-.706.706a.5.5 0 000 .707l4.953 4.954a.5.5 0 00.71-.001L20.137 7.402z\"></path></svg></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\">Tableau</div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 18 18\" aria-live=\"polite\" aria-label=\"matching qualification\" aria-hidden=\"false\" class=\"js-match-insights-provider-1jyq4ny eac13zx0\"><path d=\"M5.113 7.693a.5.5 0 010-.707l.212-.213a.5.5 0 01.707 0L9 9.741l2.969-2.968a.5.5 0 01.707 0l.212.213a.5.5 0 010 .707l-3.313 3.312a.387.387 0 01-.009.01l-.212.211a.5.5 0 01-.707 0l-.212-.212-.007-.006-3.315-3.315z\"></path></svg></div></div></button></li><li data-testid=\"list-item\" class=\"js-match-insights-provider-10zb82q eu4oa1w0\"><button data-testid=\"SQL-tile\" aria-label=\"Skills SQL matching qualification\" class=\"js-match-insights-provider-1h4aef e1xnxm2i0\"><div class=\"js-match-insights-provider-g6kqeb ecydgvn0\"><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 24 24\" aria-label=\"matching qualification\" aria-hidden=\"true\" data-icon-type=\"CheckIcon\" class=\"js-match-insights-provider-142yl9z eac13zx0\"><title>matching qualification</title><path d=\"M20.137 7.402a.5.5 0 000-.708l-.709-.708a.5.5 0 00-.707 0L9.167 15.54l-3.892-3.892a.5.5 0 00-.707 0l-.706.706a.5.5 0 000 .707l4.953 4.954a.5.5 0 00.71-.001L20.137 7.402z\"></path></svg></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\">SQL</div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 18 18\" aria-live=\"polite\" aria-label=\"matching qualification\" aria-hidden=\"false\" class=\"js-match-insights-provider-1jyq4ny eac13zx0\"><path d=\"M5.113 7.693a.5.5 0 010-.707l.212-.213a.5.5 0 01.707 0L9 9.741l2.969-2.968a.5.5 0 01.707 0l.212.213a.5.5 0 010 .707l-3.313 3.312a.387.387 0 01-.009.01l-.212.211a.5.5 0 01-.707 0l-.212-.212-.007-.006-3.315-3.315z\"></path></svg></div></div></button></li><li class=\"js-match-insights-provider-6oegoj eu4oa1w0\"><button class=\"js-match-insights-provider-1hvu6ko e19afand0\">+ show more</button></li></ul><fieldset class=\"js-match-insights-provider-1umaq0k e37uo190\"><legend class=\"js-match-insights-provider-h9kwwf eu4oa1w0\"><div class=\"js-match-insights-provider-kyg8or eu4oa1w0\">Do you have experience in <b>Analytics</b>?</div></legend><div class=\"js-match-insights-provider-1lps5p3 e37uo190\"><button data-testid=\"Do you have experience in [Analytics|attribute]?-yes\" class=\"js-match-insights-provider-s1k8bj e8ju0x51\"><span>Yes</span></button><button data-testid=\"Do you have experience in [Analytics|attribute]?-no\" class=\"js-match-insights-provider-1b1q3os e8ju0x51\"><span>No</span></button><button data-testid=\"Do you have experience in [Analytics|attribute]?-skip\" class=\"js-match-insights-provider-1ocbp6o e8ju0x51\"><span>Skip</span></button></div></fieldset></div></div><div role=\"group\" tabindex=\"0\" aria-label=\"Education\" class=\"js-match-insights-provider-16m282m e37uo190\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 20 20\" data-testid=\"section-icon\" aria-hidden=\"true\" class=\"js-match-insights-provider-1pdva1a eac13zx0\"><path fill-rule=\"evenodd\" d=\"M2.107 5.385A.423.423 0 002 5.676v9.59c0 .481.749.805 1.17.573.649-.355 1.58-.675 2.83-.675 1.552 0 2.613.494 3.25.935V4.837C8.613 4.396 7.552 3.9 6 3.9c-2.321 0-3.543 1.106-3.893 1.484zm8.643-.548v11.262c.637-.44 1.697-.935 3.25-.935 1.25 0 2.181.32 2.83.675.421.232 1.17-.092 1.17-.573v-9.59a.423.423 0 00-.107-.29C17.543 5.006 16.32 3.9 14 3.9c-1.553 0-2.613.495-3.25.936z\" clip-rule=\"evenodd\"></path></svg><div class=\"js-match-insights-provider-e6s05i eu4oa1w0\"><h3 class=\"js-match-insights-provider-11n8e9a e1tiznh50\">Education</h3><ul class=\"js-match-insights-provider-1o7r14h eu4oa1w0\"><li data-testid=\"list-item\" class=\"js-match-insights-provider-10zb82q eu4oa1w0\"><button data-testid=\"Bachelor\\'s degree-tile\" aria-label=\"Education Bachelor\\'s degree missing qualification\" class=\"js-match-insights-provider-nmba9p e1xnxm2i0\"><div class=\"js-match-insights-provider-g6kqeb ecydgvn0\"><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\">Bachelor\\'s degree</div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 18 18\" aria-live=\"polite\" aria-label=\"missing qualification\" aria-hidden=\"false\" class=\"js-match-insights-provider-1jyq4ny eac13zx0\"><path d=\"M5.113 7.693a.5.5 0 010-.707l.212-.213a.5.5 0 01.707 0L9 9.741l2.969-2.968a.5.5 0 01.707 0l.212.213a.5.5 0 010 .707l-3.313 3.312a.387.387 0 01-.009.01l-.212.211a.5.5 0 01-.707 0l-.212-.212-.007-.006-3.315-3.315z\"></path></svg></div></div></button></li></ul><fieldset class=\"js-match-insights-provider-1umaq0k e37uo190\"><legend class=\"js-match-insights-provider-h9kwwf eu4oa1w0\"><div class=\"js-match-insights-provider-kyg8or eu4oa1w0\">Do you have a <b>Bachelor\\'s degree</b>?</div></legend><div class=\"js-match-insights-provider-1lps5p3 e37uo190\"><button data-testid=\"Do you have a [Bachelor\\'s degree|attribute]?-yes\" class=\"js-match-insights-provider-s1k8bj e8ju0x51\"><span>Yes</span></button><button data-testid=\"Do you have a [Bachelor\\'s degree|attribute]?-no\" class=\"js-match-insights-provider-1b1q3os e8ju0x51\"><span>No</span></button><button data-testid=\"Do you have a [Bachelor\\'s degree|attribute]?-skip\" class=\"js-match-insights-provider-1ocbp6o e8ju0x51\"><span>Skip</span></button></div></fieldset></div></div></div></div><div role=\"separator\" aria-orientation=\"horizontal\" class=\"js-match-insights-provider-1lugej0 e15p7aqh1\"><span class=\"js-match-insights-provider-1b6omqv esbq1260\"><span>&amp;nbsp;</span></span></div></div></div></div></div></div><div id=\"mosaic-belowVjProfileInsights\" class=\"mosaic-zone\"></div><div id=\"mosaic-vjJobDetails\" class=\"mosaic-zone\" data-testid=\"vjJobDetails-test\"><div id=\"js-match-insights-provider-job-details\" class=\"mosaic js-match-insights-provider-job-details\"><div class=\"css-kyg8or eu4oa1w0\"><div id=\"jobDetailsSection\" class=\"js-match-insights-provider-kyg8or eu4oa1w0\"><div class=\"js-match-insights-provider-xu0md5 eu4oa1w0\"><div class=\"js-match-insights-provider-36vfsm eu4oa1w0\"><div class=\"js-match-insights-provider-1283b4z eu4oa1w0\"><h2 class=\"js-match-insights-provider-14dlqhn e1tiznh50\">Job details</h2><span class=\"js-match-insights-provider-xvmbeo e1wnkr790\"><span>Hereâ€™s how the job details align with your <a href=\"https://profile.indeed.com\" aria-label=\"job preferences (opens in a new window)\" target=\"_blank\" rel=\"noopener\" class=\"js-match-insights-provider-1cr09u7 e19afand0\">profile<svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 24 24\" aria-hidden=\"true\" class=\" js-match-insights-provider-r5jz5s eac13zx0\"><path d=\"M14.504 3a.5.5 0 00-.5.5v1a.5.5 0 00.5.5h3.085l-9.594 9.594a.5.5 0 000 .707l.707.708a.5.5 0 00.707 0l9.594-9.595V9.5a.5.5 0 00.5.5h1a.5.5 0 00.5-.5v-6a.5.5 0 00-.5-.5h-6z\"></path><path d=\"M5 3.002a2 2 0 00-2 2v13.996a2 2 0 001.996 2.004h14a2 2 0 002-2v-6.5a.5.5 0 00-.5-.5h-1a.5.5 0 00-.5.5v6.5L5 18.998V5.002L11.5 5a.495.495 0 00.496-.498v-1a.5.5 0 00-.5-.5H5z\"></path></svg></a>.</span></span></div><div class=\"js-match-insights-provider-h05mm8 e37uo190\"><div role=\"group\" tabindex=\"0\" aria-label=\"Job type\" class=\"js-match-insights-provider-16m282m e37uo190\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 20 20\" data-testid=\"section-icon\" aria-hidden=\"true\" class=\"js-match-insights-provider-1pdva1a eac13zx0\"><path fill-rule=\"evenodd\" d=\"M10 3C7 3 6 6 6 6H2.5a.5.5 0 00-.5.5V9h16V6.5a.5.5 0 00-.5-.5H14s-1-3-4-3zm2.5 3h-5s1-1.5 2.5-1.5S12.5 6 12.5 6z\" clip-rule=\"evenodd\"></path><path d=\"M8 11H2v5.5a.5.5 0 00.5.5h15a.5.5 0 00.5-.5V11h-6c0 1-1 2-2 2s-2-1-2-2z\"></path></svg><div class=\"js-match-insights-provider-e6s05i eu4oa1w0\"><h3 class=\"js-match-insights-provider-11n8e9a e1tiznh50\">Job type</h3><ul class=\"js-match-insights-provider-1o7r14h eu4oa1w0\"><li data-testid=\"list-item\" class=\"js-match-insights-provider-10zb82q eu4oa1w0\"><button data-testid=\"Full-time-tile\" aria-label=\"Job type Full-time matching preference\" class=\"js-match-insights-provider-1h4aef e1xnxm2i0\"><div class=\"js-match-insights-provider-g6kqeb ecydgvn0\"><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 24 24\" aria-label=\"matching preference\" aria-hidden=\"true\" data-icon-type=\"HeartIcon\" class=\"js-match-insights-provider-142yl9z eac13zx0\"><title>matching preference</title><path d=\"M11.666 20.787a.5.5 0 00.668 0l1.116-1C18.6 15.21 22 12.188 22 8.486c0-3.022-2.425-5.4-5.5-5.4-1.738 0-3.412 1.311-4.5 2.571-1.088-1.26-2.762-2.571-4.5-2.571-3.075 0-5.5 2.378-5.5 5.4 0 3.702 3.4 6.724 8.55 11.314 0-.009.688.604 1.116.987z\"></path></svg></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\">Full-time</div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 18 18\" aria-live=\"polite\" aria-label=\"matching preference\" aria-hidden=\"false\" class=\"js-match-insights-provider-1jyq4ny eac13zx0\"><path d=\"M5.113 7.693a.5.5 0 010-.707l.212-.213a.5.5 0 01.707 0L9 9.741l2.969-2.968a.5.5 0 01.707 0l.212.213a.5.5 0 010 .707l-3.313 3.312a.387.387 0 01-.009.01l-.212.211a.5.5 0 01-.707 0l-.212-.212-.007-.006-3.315-3.315z\"></path></svg></div></div></button></li><li data-testid=\"list-item\" class=\"js-match-insights-provider-10zb82q eu4oa1w0\"><button data-testid=\"Contract-tile\" aria-label=\"Job type Contract missing preference\" class=\"js-match-insights-provider-nmba9p e1xnxm2i0\"><div class=\"js-match-insights-provider-g6kqeb ecydgvn0\"><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\">Contract</div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 18 18\" aria-live=\"polite\" aria-label=\"missing preference\" aria-hidden=\"false\" class=\"js-match-insights-provider-1jyq4ny eac13zx0\"><path d=\"M5.113 7.693a.5.5 0 010-.707l.212-.213a.5.5 0 01.707 0L9 9.741l2.969-2.968a.5.5 0 01.707 0l.212.213a.5.5 0 010 .707l-3.313 3.312a.387.387 0 01-.009.01l-.212.211a.5.5 0 01-.707 0l-.212-.212-.007-.006-3.315-3.315z\"></path></svg></div></div></button></li></ul></div></div><div role=\"group\" tabindex=\"0\" aria-label=\"Shift and schedule\" class=\"js-match-insights-provider-16m282m e37uo190\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 20 20\" data-testid=\"section-icon\" aria-hidden=\"true\" class=\"js-match-insights-provider-1pdva1a eac13zx0\"><path fill-rule=\"evenodd\" d=\"M10 18a8 8 0 100-16 8 8 0 000 16zM9.25 6.25A.25.25 0 019.5 6h1a.25.25 0 01.25.25v3.886l2.809 1.621a.25.25 0 01.091.341l-.5.866a.25.25 0 01-.341.092L9.25 11.002 9.252 11H9.25V6.25z\" clip-rule=\"evenodd\"></path></svg><div class=\"js-match-insights-provider-e6s05i eu4oa1w0\"><h3 class=\"js-match-insights-provider-11n8e9a e1tiznh50\">Shift and schedule</h3><ul class=\"js-match-insights-provider-1o7r14h eu4oa1w0\"><li data-testid=\"list-item\" class=\"js-match-insights-provider-10zb82q eu4oa1w0\"><button data-testid=\"Monday to Friday-tile\" aria-label=\"Shift and schedule Monday to Friday missing preference\" class=\"js-match-insights-provider-nmba9p e1xnxm2i0\"><div class=\"js-match-insights-provider-g6kqeb ecydgvn0\"><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\">Monday to Friday</div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 18 18\" aria-live=\"polite\" aria-label=\"missing preference\" aria-hidden=\"false\" class=\"js-match-insights-provider-1jyq4ny eac13zx0\"><path d=\"M5.113 7.693a.5.5 0 010-.707l.212-.213a.5.5 0 01.707 0L9 9.741l2.969-2.968a.5.5 0 01.707 0l.212.213a.5.5 0 010 .707l-3.313 3.312a.387.387 0 01-.009.01l-.212.211a.5.5 0 01-.707 0l-.212-.212-.007-.006-3.315-3.315z\"></path></svg></div></div></button></li></ul></div></div><div role=\"group\" tabindex=\"0\" aria-label=\"Work setting\" class=\"js-match-insights-provider-16m282m e37uo190\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"none\" viewBox=\"0 0 20 20\" width=\"20\" height=\"20\" data-testid=\"section-icon\" aria-hidden=\"true\" class=\"js-match-insights-provider-1pdva1a eac13zx0\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M3.5 3C3.22386 3 3 3.22386 3 3.5V16.5C3 16.7761 3.22386 17 3.5 17H5V12H9V17H16.5C16.7761 17 17 16.7761 17 16.5V9.5C17 9.22386 16.7761 9 16.5 9H12V3.5C12 3.22386 11.7761 3 11.5 3H3.5ZM5 8H10V9H5V8ZM10 5H5V6H10V5Z\" fill=\"#767676\"></path></svg><div class=\"js-match-insights-provider-e6s05i eu4oa1w0\"><h3 class=\"js-match-insights-provider-11n8e9a e1tiznh50\">Work setting</h3><ul class=\"js-match-insights-provider-1o7r14h eu4oa1w0\"><li data-testid=\"list-item\" class=\"js-match-insights-provider-10zb82q eu4oa1w0\"><button data-testid=\"Hybrid work-tile\" aria-label=\"Work setting Hybrid work missing preference\" class=\"js-match-insights-provider-nmba9p e1xnxm2i0\"><div class=\"js-match-insights-provider-g6kqeb ecydgvn0\"><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\">Hybrid work</div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 18 18\" aria-live=\"polite\" aria-label=\"missing preference\" aria-hidden=\"false\" class=\"js-match-insights-provider-1jyq4ny eac13zx0\"><path d=\"M5.113 7.693a.5.5 0 010-.707l.212-.213a.5.5 0 01.707 0L9 9.741l2.969-2.968a.5.5 0 01.707 0l.212.213a.5.5 0 010 .707l-3.313 3.312a.387.387 0 01-.009.01l-.212.211a.5.5 0 01-.707 0l-.212-.212-.007-.006-3.315-3.315z\"></path></svg></div></div></button></li></ul></div></div></div><div class=\"js-match-insights-provider-bbq8li eu4oa1w0\"></div></div><div role=\"separator\" aria-orientation=\"horizontal\" class=\"js-match-insights-provider-1lugej0 e15p7aqh1\"><span class=\"js-match-insights-provider-1b6omqv esbq1260\"><span>&amp;nbsp;</span></span></div></div></div></div></div></div><div id=\"mosaic-aboveFullJobDescription\" class=\"mosaic-zone\"></div><div id=\"mosaic-aboveExtractedJobDescription\" class=\"mosaic-zone\"></div><div id=\"jobLocationSectionWrapper\" class=\"css-1udddjt eu4oa1w0\"><h2 id=\"jobLocationSectionTitle\" class=\"css-1yytfzy e1tiznh50\">Location</h2><div id=\"jobLocationWrapper\" class=\"css-1htjwgd e37uo190\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 24 24\" aria-hidden=\"true\" id=\"jobLocationIcon\" class=\"css-r72e78 eac13zx0\"><path d=\"M12 2C8.13 2 5 5.13 5 9c0 4.523 5.195 11.093 6.634 12.826a.47.47 0 00.732 0C13.805 20.093 19 13.523 19 9c0-3.87-3.13-7-7-7zm0 9.5a2.5 2.5 0 010-5 2.5 2.5 0 010 5z\"></path></svg><div id=\"jobLocationText\" class=\"css-1tlxeot eu4oa1w0\"><div data-testid=\"job-location\" class=\"css-1ojh0uo eu4oa1w0\">The Squires Group in Arlington, VA 22209</div></div></div></div><div id=\"jobDescriptionTitle\"><h2 tabindex=\"-1\" id=\"jobDescriptionTitleHeading\" class=\"css-wpzt8u e1tiznh50\">Full job description</h2></div><div id=\"jobDescriptionText\" class=\"jobsearch-jobDescriptionText jobsearch-JobComponent-description css-10og78z eu4oa1w0\"><p><b>Data Analyst </b></p>\\n\\n<p><b>Arlington, Va (One time per week onsite)</b></p>\\n\\n<p><b>Secret Clearance Required</b></p>\\n\\n<p>The Squires Group has an excellent opportunity for an experienced <b>Data &amp; Analytics Practitioner</b> to work on a Federal Project in Arlington, VA. In this role you will be responsible for providing guidance in defining analytics strategies, priorities, and objectives. You will oversee the development and implementation of analytics projects; while also designing and creating visually compelling and informative dashboards/reports utilizing Power BI, Tableau and SQL.</p>\\n\\n<p>This position is hybrid and <i>will require some days onsite in the DC Metro area</i>. Per our Federal Government Contract, candidates must be <i><b>U.S. Citizens with at least an active Interim Secret Clearance.</b></i></p>\\n\\n<p><b>Responsibilities: </b></p>\\n\\n<ul>\\n\\n<li>Provide guidance and support in defining analytics strategies, priorities, and objectives.</li>\\n\\n<li>Oversee the development and implementation of analytics projects using Power BI.</li>\\n\\n<li>Design and create visually compelling and informative dashboards and reports.</li>\\n\\n<li>Analyze large datasets to identify trends, patterns, and opportunities for improvement.</li>\\n\\n<li>Collaborate with cross-functional teams to understand business requirements and align analytics initiatives with organizational goals.</li>\\n\\n<li>Develop and implement strategies to improve data quality, accuracy, and accessibility.</li>\\n\\n<li>Serve as the subject matter expert in Power BI, staying current with the latest features and updates.</li>\\n\\n<li>Design and optimize data models, ensuring efficient and scalable solutions.</li>\\n\\n<li>Collaborate with IT and other departments to ensure seamless integration of analytics solutions.</li>\\n\\n<li>Work closely with stakeholders to gather requirements and provide insights that drive business decisions.</li>\\n\\n<li>Identify opportunities to enhance existing processes and tools, promoting a culture of continuous improvement.</li>\\n\\n<li>Stay abreast of industry trends and emerging technologies in analytics.</li>\\n\\n</ul>\\n\\n<p><b>Qualifications:</b></p>\\n\\n<ul>\\n\\n<li>Proven experience in an analytics or business intelligence team.</li>\\n\\n<li>Experience with Palantir required</li>\\n\\n<li>Strong proficiency in Power BI, with a track record of designing and implementing effective dashboards and reports.</li>\\n\\n<li>Experience with Data Models</li>\\n\\n<li>Advanced skills in data analysis, SQL, and data visualization techniques.</li>\\n\\n<li>Excellent communication and interpersonal skills, with the ability to convey complex findings to both technical and non-technical stakeholders.</li>\\n\\n<li>Strong organizational and project management skills.</li>\\n\\n<li>Proactive problem solver with a strategic mindset.</li>\\n\\n</ul>\\n\\n<p>Job Types: Full-time, Contract</p>\\n\\n<p>Experience level:</p>\\n\\n<ul>\\n\\n<li>10 years</li>\\n\\n<li>5 years</li>\\n\\n<li>6 years</li>\\n\\n<li>7 years</li>\\n\\n<li>8 years</li>\\n\\n<li>9 years</li>\\n\\n</ul>\\n\\n<p>Schedule:</p>\\n\\n<ul>\\n\\n<li>Monday to Friday</li>\\n\\n</ul>\\n\\n<p>Application Question(s):</p>\\n\\n<ul>\\n\\n<li>Due to the nature of the role, at least a Secret Security Clearance is required. Do you posses a Security Clearance? If so, which level?</li>\\n\\n</ul>\\n\\n<p>Education:</p>\\n\\n<ul>\\n\\n<li>Bachelor\\'s (Preferred)</li>\\n\\n</ul>\\n\\n<p>Experience:</p>\\n\\n<ul>\\n\\n<li>Data analytics: 4 years (Required)</li>\\n\\n<li>Power BI: 4 years (Preferred)</li>\\n\\n<li>SQL: 4 years (Preferred)</li>\\n\\n<li>Palantir: 1 year (Required)</li>\\n\\n</ul>\\n\\n<p>Security clearance:</p>\\n\\n<ul>\\n\\n<li>Secret (Required)</li>\\n\\n</ul>\\n\\n<p>Ability to Commute:</p>\\n\\n<ul>\\n\\n<li>Arlington, VA 22209 (Required)</li>\\n\\n</ul>\\n\\n<p>Ability to Relocate:</p>\\n\\n<ul>\\n\\n<li>Arlington, VA 22209: Relocate before starting work (Required)</li>\\n\\n</ul>\\n\\n<p>Work Location: Hybrid remote in Arlington, VA 22209</p></div></div><div id=\"mosaic-belowFullJobDescription\" class=\"mosaic-zone\"></div><div class=\"jobsearch-JobMetadataFooter jobsearch-JobMetadataFooter-desktop-standalone css-yxghko eu4oa1w0\"><div id=\"successfullySignedInModal\"></div><div class=\"indeedApplyAdaNoticeContainer css-z3ct0g eu4oa1w0\">If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer\\'s application process.</div><div class=\" css-11lsxj6 eu4oa1w0\"><div id=\"jobsearch-ViewJobButtons-container\" class=\"jobsearch-ViewJobButtons-container css-15enpd4 eu4oa1w0\"><div id=\"mosaic-aboveViewjobButtons\" class=\"mosaic-zone\"></div><div class=\"jobsearch-ButtonContainer-inlineBlock icl-u-lg-inlineBlock css-p3dq3b eu4oa1w0\"><div class=\"jobsearch-IndeedApplyButton css-1cs7d6u eu4oa1w0\"><div class=\"bfae6dbf0e08a7fb053c09f21f1ece0d45dabd6a5d667cf9d6ec5ef6cc1408be\"><span id=\"ec1d854b870e9160705fdcd1e68a58e9c1b067088c785f2c616619eb154aebf6\" class=\"ec1d854b870e9160705fdcd1e68a58e9c1b067088c785f2c616619eb154aebf6 indeed-apply-status-not-applied\" data-indeed-apply-jobcountry=\"US\" data-indeed-apply-jk=\"0e0683f6e0411f6c\" data-indeed-apply-clientmeta=\"{&amp;quot;vtk&amp;quot;:&amp;quot;1hpje6reqikfq800&amp;quot;,&amp;quot;tk&amp;quot;:&amp;quot;1hpje3pjej4qu81f&amp;quot;}\" data-indeed-apply-continueurl=\"http://www.indeed.com/viewjob?jk=0e0683f6e0411f6c&amp;q=data+analyst&amp;l=washington%2C+dc&amp;tk=1hpje3pjej4qu81f&amp;from=web&amp;advn=3909472159286479&amp;adid=428284350&amp;pub=4a1b367933fd867b19b072952f68dceb&amp;camk=4HOcmqOLYrAiOxyB78jmug%3D%3D&amp;xkcb=SoBX6_M3DzUA4cwSMx0LbzkdCdPP&amp;xpse=SoBR6_I3DzUUsVwjnR0JbzkdCdPP&amp;xfps=34a05441-2564-40c7-b441-2d6fdd64f9a4&amp;vjs=3&amp;applied=1\" data-indeed-apply-recentsearchquery=\"{&quot;what&quot;:&quot;data analyst&quot;,&quot;where&quot;:&quot;washington, dc&quot;}\" data-indeed-apply-joburl=\"https://www.indeed.com/viewjob?jk=0e0683f6e0411f6c\" data-indeed-apply-pingbackurl=\"https://gdc.indeed.com/conv/orgIndApp?co=US&amp;vjtk=1hpje6reqikfq800&amp;jk=0e0683f6e0411f6c&amp;mvj=0&amp;acct_key=352c48d0c7eb3ee4&amp;tk=1hpje3pjej4qu81f&amp;trk.origin=jobsearch&amp;sj=1&amp;vjfrom=web&amp;advn=3909472159286479&amp;adid=428284350&amp;ad=-6NYlbfkN0CAaddY1llEc-bCBM6ayv9R7ZjSra3bt--3XFr0tpCDJhEd0QZmF7DypKGJuFl4DJYEGkKdDcG0Ah7K9tznrDGc1UdkuY0g5jKg4TVOFRLTSi9k0-xYFeITs-pX36eIBZVapmIS4zIl4fIkFcAd3ai4l1SsfGpBInUIR71fQrk9NnAEDw87BCz03MSTa2qU_3eX3eHaYDZjc9ojsLGmAOQ-7emHtVAvGwgwTl3ze3kEx6ACenD1_sQvja4kDdH8d94Cth_S4fiuOqg1shO2AkXxcQVowmU2-4loaryZBDJQoRV_E21luwPEdZee0K5kqDEJhTQ4rqk7SxgE25ARKmrOX3vb3JmNLFMli5fVlj6GZXLdPGIFDrs_OQRULjhdXDR3yuluAbHpeaH5NLvbXpdml0L2Ee_1bWOq_kI-4lp3wii6tSbvS57l6Q5idk4RiNzxzUG1d1oPf9DoP-bSdRrp1kzRob7VQzIKKHq_eMvB_u4tS8WowC0RaOewhgjjfrc%3D&amp;xkcb=SoBX6_M3DzUA4cwSMx0LbzkdCdPP&amp;xfps=34a05441-2564-40c7-b441-2d6fdd64f9a4&amp;xpse=SoBR6_I3DzUUsVwjnR0JbzkdCdPP&amp;astse=1446ba2b04586dbb&amp;assa=4630\" data-indeed-apply-onready=\"_onButtonReady\" data-indeed-apply-onappliedstatus=\"_updateIndeedApplyStatus\" data-indeed-apply-advnum=\"3909472159286479\" data-indeed-apply-nobuttonui=\"true\" data-click-handler=\"attached\"><div class=\"jobsearch-IndeedApplyButton-buttonWrapper css-kyg8or eu4oa1w0\"><button id=\"indeedApplyButton\" class=\" css-t8wchy e8ju0x51\" aria-label=\"Apply now \"><div class=\"jobsearch-IndeedApplyButton-contentWrapper\"><span class=\"jobsearch-IndeedApplyButton-newDesign css-1hjxf1u eu4oa1w0\">Apply now</span></div></button></div></span></div></div></div><div id=\"saveJobButtonContainer\" aria-hidden=\"false\" class=\"css-e9f4jv eu4oa1w0\"><div class=\" css-kyg8or eu4oa1w0\"><div aria-live=\"assertive\"><button data-dd-action-name=\"save-job\" aria-label=\"Save this job\" class=\"css-1fm9hy1 e8ju0x51\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 24 24\" class=\"css-1xqhio eac13zx0\"><title>save-icon</title><path fill-rule=\"evenodd\" d=\"M12 14.616l6 3.905V4H6v14.52l6-3.905zM4.383 21.96A.25.25 0 014 21.748V2.502a.5.5 0 01.5-.5h15a.5.5 0 01.5.5v19.246a.25.25 0 01-.383.212L12 17.002 4.383 21.96z\" clip-rule=\"evenodd\"></path></svg></button></div></div></div><div id=\"mosaic-belowViewjobButtons\" class=\"mosaic-zone\"></div><div role=\"separator\" aria-orientation=\"horizontal\" class=\"css-eud5o9 e15p7aqh1\"><span class=\"css-1b6omqv esbq1260\"><span>&amp;nbsp;</span></span></div></div></div><div class=\"css-1my0t14 e37uo190\"><div class=\"css-173agvp eu4oa1w0\"><div id=\"mosaic-provider-reportcontent\" class=\"mosaic mosaic-provider-reportcontent\"><div class=\"mosaic-reportcontent-wrapper button\"><button class=\"mosaic-reportcontent-button desktop css-16q8vsx e8ju0x51\"><span class=\"mosaic-reportcontent-button-icon\"></span>Report job</button><div class=\"mosaic-reportcontent-content\"></div></div></div></div></div></div></div></div></div><div class=\"css-2oc8b5 eu4oa1w0\"><div id=\"relatedLinks\" class=\"css-1rqye1z eu4oa1w0\"><div class=\"css-dday9n eu4oa1w0\"><div class=\"css-e6s05i eu4oa1w0\"><div class=\"css-6thzq4 eu4oa1w0\"><a class=\"jobsearch-RelatedLinks-link css-lvbt0r emf9s7v0\" href=\"/q-data-analyst-l-arlington,-va-jobs.html\">Data Analyst jobs in Arlington, VA</a></div><div class=\"css-6thzq4 eu4oa1w0\"><a class=\"jobsearch-RelatedLinks-link css-lvbt0r emf9s7v0\" href=\"/q-the-squires-group-l-arlington,-va-jobs.html\">Jobs at The Squires Group in Arlington, VA</a></div><div class=\"css-6thzq4 eu4oa1w0\"><a class=\"jobsearch-RelatedLinks-link css-lvbt0r emf9s7v0\" href=\"/salary?from=vj&amp;q1=Data+Analyst&amp;l1=Arlington%2C+VA\">Data Analyst salaries in Arlington, VA</a></div></div></div></div></div></div></div></div></div></div>\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_files = os.listdir(raw_files)\n",
    "\n",
    "my_text = docx2txt.process(raw_files+ \"//\" + list_of_files[0])\n",
    "print(type(my_text))\n",
    "#print(my_text)\n",
    "\n",
    "separate_job_objects = my_text.split('\\n\\n--------------------')\n",
    "print(len(separate_job_objects))\n",
    "separate_job_objects[0]\n",
    "#document_to_string = read_word(raw_files+ \"//\" + list_of_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103fe3e4-8c21-455f-9d79-98db6b7e67e8",
   "metadata": {},
   "source": [
    "# We want to extract the job description portion and clean up all remaining html tags and grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "24b729a9-59eb-4772-b360-f472eae09419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_word(path: str) -> str:\n",
    "    ''' Creates a string object from a word document of job descriptions div containers\n",
    "    \n",
    "        Args: \n",
    "        path (str): file path to word document locaiton \n",
    "        \n",
    "        Returns: \n",
    "        doc (str): a string made of the text of the word document \n",
    "       \n",
    "    '''\n",
    "    document = Document(path)\n",
    "    \n",
    "    # Initialize an empty string to hold the text block\n",
    "    text_block = \"\"\n",
    "\n",
    "    # Iterate through each paragraph in the document\n",
    "    for para in document.paragraphs:\n",
    "    # Append each paragraph's text to the text block, followed by a newline character\n",
    "        text_block += para.text + \"\\n\"\n",
    "    \n",
    "    return text_block \n",
    "    \n",
    "def clean_job_description_paragraph(job_desc_list: list):\n",
    "    ''' Take dirty job description html code already separated using ('-------'), and cleans html formatting using regex expressions\n",
    "    \n",
    "        Args: \n",
    "        job_desc_list (list): dirty job description html code \n",
    "        \n",
    "        Returns: \n",
    "        job_description_string (str): string object of cleaned job description text\n",
    "        \n",
    "       \n",
    "    '''\n",
    "    \n",
    "    job_description_string = ''\n",
    "    \n",
    "    phone_number_pattern = r'\\(?\\b\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b'\n",
    "    date_pattern = r'\\(?\\b\\d{4}\\)?[-.\\s]?\\d{2}[-.\\s]?\\d{2}\\b'\n",
    "    remove_top_html_pattern = r'</div><div id=\"jobDescriptionText\"'\n",
    "    remove_bottom_html_pattern = r'</div></div>'\n",
    "    merged_tag_pattern = r'<(\\w{1,2})(\\w+)'\n",
    "    replacement = r'\\2'\n",
    "    first_line_pattern = re.compile(r' class=\"[^\"]*\">\\s')\n",
    "    \n",
    "    tags_to_remove = ['<div>', '</div>', '<p>', '</p>', '<br>', '</br>', '<ul>', '</ul>', '<i>', '</i>', '<b>', '</b>', '<li>', '</li>', '\\n', '\\n+', '<i>', \"'\", '<h4>', \n",
    "                        '</h4>', '</h3>', '<h3>', '<h2>', '</h2>', \"â€™\", r'/', r'\\.00\\b','  +']\n",
    "    \n",
    "    \n",
    "    for job_desc_html in job_desc_list:\n",
    "        \n",
    "        try:\n",
    "            job_desc_html_v2 = job_desc_html.split(remove_top_html_pattern)[1]\n",
    "            job_desc_html_v3 = job_desc_html_v2.split(remove_bottom_html_pattern)[0]\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "\n",
    "\n",
    "        for items in tags_to_remove: \n",
    "            job_desc_html_v3 = re.sub(items, ' ', job_desc_html_v3)\n",
    "\n",
    "        \n",
    "        for regex_fliters in [first_line_pattern, phone_number_pattern, date_pattern]:\n",
    "            job_desc_html_v3 = re.sub(regex_fliters, '', job_desc_html_v3)\n",
    "\n",
    "        \n",
    "        refined_job_desc = re.sub(merged_tag_pattern, replacement, job_desc_html_v3)\n",
    "\n",
    "        job_description_string += \"\".join(\"Job Description:  \") + refined_job_desc +  \"\\n\\n -------------------------------------------------------------------------------------- \\n\\n \"\n",
    "        \n",
    "    return job_description_string.lower()\n",
    "\n",
    "def docx_to_csv(csv_file_path: str, input_string_list: list):\n",
    "    ''' Takes the separated job description list and creates a dataframe to readablility in AWS Glue\n",
    "    \n",
    "        Args: \n",
    "        csv_file_path (str): file path to save dataframe using word_docx title rename \n",
    "        input_string_list (list): list of job descriptions split using ('------') separator\n",
    "        \n",
    "       \n",
    "    '''\n",
    "    job_dataframe = pd.DataFrame()\n",
    "    job_dataframe['numb_description'] = range(1, len(input_string_list))\n",
    "    job_dataframe['job_description'] = input_string_list[:-1]\n",
    "    job_dataframe.to_csv(csv_file_path, index=False)\n",
    "    \n",
    "\n",
    "\n",
    "input_folder = str(current_dir) + r\"\\raw_files\" \n",
    "docx_output_folder = str(current_dir) + r\"\\processed_word_docs\"\n",
    "cleansed_folder = str(current_dir) + r\"\\cleansed_files\"\n",
    "\n",
    "list_of_files = os.listdir(input_folder)\n",
    "\n",
    "\n",
    "for word_docx in list_of_files: \n",
    "    \n",
    "    document_to_string = read_word(raw_files + r\"\\\\\" +  word_docx)\n",
    "    separate_job_objects = document_to_string.split('\\n\\n--------------------') # Separate the job from a divider marker my bot set\n",
    "    \n",
    "    repaired_doc = clean_job_description_paragraph(separate_job_objects)\n",
    "    \n",
    "    cleansed_page = Document()\n",
    "    cleansed_page.add_paragraph(repaired_doc)\n",
    "    \n",
    "    \n",
    "    new_file_path = os.path.join(docx_output_folder, word_docx)\n",
    "    cleansed_page.save(new_file_path)\n",
    "    \n",
    "    \n",
    "    csv_split_file = repaired_doc.split('--------------------------------------------------------------------------------------')\n",
    "    docx_to_csv(cleansed_folder + r\"/\" + word_docx[:-5] + '.csv', csv_split_file)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "efaa670f-40d9-4c80-9801-8aa7e44cd5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "Python int too large to convert to C long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader:\n\u001b[0;32m     11\u001b[0m             \u001b[38;5;28mprint\u001b[39m(row)  \u001b[38;5;66;03m# This should now handle larger fields without error\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcurrent_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcleansed_files\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata-analyst_dmv_2024-05-05_page1.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[55], line 6\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(csv_file_path)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_csv\u001b[39m(csv_file_path):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Increase the maximum field size allowed\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mcsv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfield_size_limit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(csv_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      9\u001b[0m         reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(file, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mOverflowError\u001b[0m: Python int too large to convert to C long"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(str(current_dir) + r\"\\cleansed_files\\data-analyst_dmv_2024-05-05_page1.csv\")\n",
    "test.head()\n",
    "\n",
    "def read_csv(csv_file_path):\n",
    "    # Increase the maximum field size allowed\n",
    "    csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "    with open(csv_file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            print(row)  # This should now handle larger fields without error\n",
    "            \n",
    "read_csv((str(current_dir) + r\"\\cleansed_files\\data-analyst_dmv_2024-05-05_page1.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf07d407-8e44-4237-8742-97eb0aad87c2",
   "metadata": {},
   "source": [
    "# This Cleansed Layer will not take the cleaned up job description paragraphs and use dictionaries to extract word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0871dbf-ed15-40fc-b1c2-ecff6d23d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary_from_csv(path_to_csv: str) -> dict:\n",
    "    ''' Creates environment folders and partitioning folders when new job title is created \n",
    "    \n",
    "        Args: \n",
    "        path_to_csv (str): string path to csv file needed to make a frequency dictionary\n",
    "        \n",
    "        Returns:\n",
    "        freq_dictionary (dict): blank frequency dictionary '''\n",
    "    \n",
    "    freq_dictionary = {} \n",
    "\n",
    "    with open(path_to_csv, mode='r', newline='') as file: \n",
    "        reader = csv.reader(file)\n",
    "        next(reader, None) # Skips header\n",
    "        for row in reader: \n",
    "            if row:\n",
    "                freq_dictionary[row[0]] = 0\n",
    "    \n",
    "    return freq_dictionary\n",
    "\n",
    "def create_cleansed_enviornment(list_of_files_in_raw: list, output_folder: str) -> list:\n",
    "    ''' Creates environment folders and partitioning folders when new job title is created \n",
    "    \n",
    "        Args: \n",
    "        list_of_files_in_raw (list): list of all files collected using os.listdir\n",
    "        \n",
    "        Returns:\n",
    "        all_job_names (list): list of all unique names collected from raw files listed'''\n",
    "        \n",
    "    partition_folders = ['education', 'programming_languages', 'personality_traits', 'skillset', 'software', 'security_clearance', 'experience', 'salary']\n",
    "    all_job_names = []\n",
    "    \n",
    "    for word_docx_filename in list_of_files_in_raw:\n",
    "\n",
    "        position_title = word_docx_filename.split('_')[0]\n",
    "    \n",
    "        if position_title not in all_job_names: all_job_names.append(position_title)\n",
    "            \n",
    "\n",
    "    for items in partition_folders:\n",
    "        if not os.path.exists(output_folder + \"//\" + items):\n",
    "            os.makedirs(output_folder + \"//\" + items)\n",
    "        \n",
    "\n",
    "        \n",
    "    return all_job_names\n",
    "\n",
    "def extract_salary_from_paragraph(job_id: str, output_folder: str, word_docx_filename: str, document_to_string: str):\n",
    "    ''' Cleans salary tuple in order to convert it into two dataframe columns after\n",
    "        \n",
    "        Args: \n",
    "        salary_tuple: tuple of low end, high end, and hourly/salary pay grades \n",
    "        \n",
    "        Returns: \n",
    "        Tuple: a tuple of job low/high end pay ranges '''\n",
    "    file_name_split = word_docx_filename.split('_')\n",
    "    state = file_name_split[1]\n",
    "    report_year = file_name_split[2].split('-')[0]\n",
    "    \n",
    "    salary_tuple = re.findall(r'(\\$?\\d{1,3}(?:k|,\\d{1,3}|\\d{1,3}))\\s*?(?:to|-)\\s*?(\\$?\\d{1,3}(?:k|,\\d{1,3}|\\d{1,3}))(?:\\s*(?:per\\s+|a\\s+)?(hour|annually|year|yearly))?', document_to_string)\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' # remove special characters\n",
    "    low_end_pay = [] \n",
    "    high_end_pay = [] \n",
    "    \n",
    "    \n",
    "    for matches in salary_tuple: \n",
    "        low = re.sub(pattern, '', matches[0]) # remove special characters\n",
    "        high = re.sub(pattern, '', matches[1]) # remove special characters\n",
    "        \n",
    "        low = re.sub('k', '000', low) # replace k with 000\n",
    "        high = re.sub('k', '000', high) \n",
    "        \n",
    "        \n",
    "        if int(low) < 20000 or int(high) < 20000:\n",
    "            \n",
    "            if matches[2] == 'year' or matches[2] == 'annually':\n",
    "                \n",
    "                if len(low) < 5: low = int(low) * 1000\n",
    "                if len(high) < 5: high = int(high) * 1000\n",
    "                \n",
    "                if int(low) < 20000 or int(high) < 20000:\n",
    "                    continue\n",
    "                    \n",
    "            elif matches[2] == 'hour' or matches[2] == 'hourly':\n",
    "                low = int(low) * 40 * 52 # Convert to salary\n",
    "                high = int(high) * 40 * 52 # Convert to salary\n",
    "                \n",
    "            else: \n",
    "                continue\n",
    "        \n",
    "        low_end_pay.append(int(low))\n",
    "        high_end_pay.append(int(high))\n",
    "        \n",
    "    salary_dataframe = pd.DataFrame()\n",
    "    salary_dataframe['pay_low_end'] = low_end_pay\n",
    "    salary_dataframe['pay_high_end'] = high_end_pay\n",
    "    salary_dataframe['job_id'] = job_id\n",
    "    salary_dataframe['state'] = state\n",
    "    salary_dataframe['report_year'] = report_year\n",
    "    \n",
    "    salary_dataframe.to_csv(output_folder + \"\\\\\" + 'salary' + \"\\\\\"  + word_docx_filename + '.csv', index=False)\n",
    "\n",
    "def extract_experience_from_paragraph(job_id: str, output_folder: str, word_docx_filename: str, document_to_string: str):\n",
    "    \"\"\"Retrieve or assign a unique job ID based on the job title.\n",
    "    \n",
    "    Args:\n",
    "    list_of_experience: a list of all experience years collected from job descriptions\n",
    "    \n",
    "    Returns:\n",
    "    years_experience: a cleaned list of professional experience\n",
    "    \"\"\"\n",
    "    file_name_split = word_docx_filename.split('_')\n",
    "    state = file_name_split[1]\n",
    "    report_year = file_name_split[2].split('-')[0]\n",
    "    \n",
    "    years_experience = []\n",
    "    list_of_experience = re.findall(re.compile(r'(\\d+\\+?|\\d+\\s*[-â€“to]\\s*\\d+)\\s*(years?)'), document_to_string)\n",
    "    \n",
    "    for items in list_of_experience:\n",
    "        years = re.sub(r'[^a-zA-Z0-9-]', '', items[0])\n",
    "        \n",
    "        if '-' in years: \n",
    "            years_array = years.split('-') \n",
    "            ranged_item = list(range(int(years_array[0]), int(years_array[1]) + 1))\n",
    "            years_experience.extend(ranged_item)\n",
    "        elif int(years) > 15: \n",
    "            continue\n",
    "        else: years_experience.append(int(years))\n",
    "    \n",
    "    experience_db = pd.DataFrame()\n",
    "    experience_db['years_experience_recorded'] = years_experience\n",
    "    experience_db['job_id'] = job_id\n",
    "    experience_db['state'] = state\n",
    "    experience_db['report_year'] = report_year\n",
    "    \n",
    "    experience_db.to_csv(output_folder + \"\\\\\" + 'experience' + \"\\\\\"  + word_docx_filename + '.csv', index=False)\n",
    "\n",
    "def get_job_id(job_name: str, file_path: str) -> int:\n",
    "    \"\"\"Retrieve or assign a unique job ID based on the job title.\n",
    "\n",
    "    Args:\n",
    "        job_name (str): The name of the job to retrieve or create an ID for.\n",
    "        file_path (str): Path to the CSV file containing job categories.\n",
    "\n",
    "    Returns:\n",
    "        int: The job ID.\n",
    "    \"\"\"\n",
    "    job_id_db = pd.read_csv(file_path)\n",
    "    \n",
    "    if not (job_id_db['job_title'].eq(job_name)).any():  # If the job title doesn't exist in job_categories.csv\n",
    "        new_job_id = len(job_id_db)  # Assign the next ID\n",
    "        new_data = pd.DataFrame({'job_id': [new_job_id], 'job_title': [job_name]})\n",
    "        job_id_db = pd.concat([job_id_db, new_data], ignore_index=True)\n",
    "        job_id_db.to_csv(file_path, index=False)  # Save the updated list\n",
    "        return new_job_id\n",
    "    else:\n",
    "        # Return existing ID for the job title\n",
    "        return job_id_db.loc[job_id_db['job_title'] == job_name, 'job_id'].iloc[0]\n",
    "       \n",
    "def write_dictionary_to_cleansed_layer(measurement_dictionary: dict, job_id: str, output_folder: str, folder: str, word_docx_filename: str): \n",
    "    \"\"\"Write creates a dataframe object from dictionary passed, and writes it into a csv with the same file name\n",
    "    \n",
    "    Args:\n",
    "    measurement_dictionary (dict): frequency dictionary.\n",
    "    job_id (int): unique job id.\n",
    "    output_folder (str): path to cleansed layer.\n",
    "    folder (str): job specific folder in cleansed layer.\n",
    "    word_docx_filename (str): name of file being used\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    file_name_split = word_docx_filename.split('_')\n",
    "    state = file_name_split[1]\n",
    "    report_year = file_name_split[2].split('-')[0]\n",
    "    \n",
    "    for name, dictionary in measurement_dictionary.items():\n",
    "        \n",
    "        dataframe = pd.DataFrame(list(measurement_dictionary.items()), columns = [folder, 'frequency'])\n",
    "        dataframe['job_id'] = job_id  # Adding job_id column\n",
    "        dataframe['state'] = state  # Adding state column\n",
    "        dataframe['report_year'] = report_year  # Adding report year column\n",
    "        dataframe.to_csv(output_folder + \"\\\\\" + folder + \"\\\\\"  + word_docx_filename + '.csv', index=False)\n",
    "        \n",
    "def mark_phrases(word_doc_text: str, words_for_marking_desc: list) -> str:\n",
    "    ''' Function takes multiple worded phrases from dictionaries and replaced space with '-' in order to mark them before counting \n",
    "        \n",
    "        Args: \n",
    "        word_doc_text (str): word documented converted into a string \n",
    "        words_for_marking (list) : list of phrases that will be marked to count accurately \n",
    "        \n",
    "        Returns: \n",
    "        word_doc_text (str): word document with marked text '''\n",
    "    for phrase in words_for_marking_desc:\n",
    "\n",
    "        word_doc_text = re.sub(phrase, phrase.replace(' ', '-'), word_doc_text)\n",
    "        \n",
    "    return word_doc_text\n",
    "\n",
    "def find_special_characters(s: str):\n",
    "    '''\n",
    "    Function removes non-alphanumeric characters\n",
    "    \n",
    "    Args: \n",
    "    s (str): text based value \n",
    "    \n",
    "    Returns: \n",
    "    s (str): cleaned text \n",
    "    \n",
    "    '''\n",
    "    # This pattern matches any character that is not a letter or a number\n",
    "    pattern = re.compile('[^a-zA-Z0-9]')\n",
    "    # Find all non-alphanumeric characters in the string\n",
    "    special_chars = pattern.findall(s)\n",
    "    unique_special_chars = set(special_chars)\n",
    "    \n",
    "    for special_character in unique_special_chars: \n",
    "        s = s.replace(special_character, \"\\\\\" + special_character)\n",
    "    return s\n",
    "\n",
    "def create_key_glossary_from_dict_shells(dictionary_skeletons_folder: str):\n",
    "    # Specify the path to the file\n",
    "    glossary_path = dictionary_skeletons_folder + '\\dict_key_glossary.csv'\n",
    "    \n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.exists(glossary_path):\n",
    "        glossary_db = pd.read_csv(glossary_path)\n",
    "    else:\n",
    "        glossary_db = pd.DataFrame(columns=['keys'])\n",
    "        \n",
    "        for file in os.listdir(dictionary_skeletons_folder):\n",
    "            if file.endswith('.csv'):\n",
    "                keyword_list = pd.read_csv(dictionary_skeletons_folder + r\"\\\\\" +  file).iloc[:, 0].tolist()\n",
    "                new_df = pd.DataFrame(keyword_list, columns=['keys'])\n",
    "                glossary_db = pd.concat([glossary_db, new_df], ignore_index=True)\n",
    "        \n",
    "        glossary_db.to_csv(glossary_path, index=False)\n",
    "        \n",
    "    list_of_hypend_words = [x for x in glossary_db.iloc[:, 0].tolist() if \"-\" in x]\n",
    "    words_without_hyphens = list(map(lambda word: word.replace('-', ' '), list_of_hypend_words))\n",
    "    words_without_hyphens_desc = sorted(words_without_hyphens, key=len, reverse=True)\n",
    "    \n",
    "    return words_without_hyphens_desc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1c78d54-3258-4034-97bb-62f40c990b75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-analyst_dmv_2024-05-05_page1.docx\n",
      "data-analyst_dmv_2024-05-05_page10.docx\n",
      "data-analyst_dmv_2024-05-05_page11.docx\n",
      "data-analyst_dmv_2024-05-05_page12.docx\n",
      "data-analyst_dmv_2024-05-05_page2.docx\n",
      "data-analyst_dmv_2024-05-05_page3.docx\n",
      "data-analyst_dmv_2024-05-05_page4.docx\n",
      "data-analyst_dmv_2024-05-05_page5.docx\n",
      "data-analyst_dmv_2024-05-05_page6.docx\n",
      "data-analyst_dmv_2024-05-05_page7.docx\n",
      "data-analyst_dmv_2024-05-05_page8.docx\n",
      "data-analyst_dmv_2024-05-05_page9.docx\n",
      "software-engineer_dmv_2024-05-05_page1.docx\n",
      "software-engineer_dmv_2024-05-05_page10.docx\n",
      "software-engineer_dmv_2024-05-05_page11.docx\n",
      "software-engineer_dmv_2024-05-05_page12.docx\n",
      "software-engineer_dmv_2024-05-05_page13.docx\n",
      "software-engineer_dmv_2024-05-05_page14.docx\n",
      "software-engineer_dmv_2024-05-05_page15.docx\n",
      "software-engineer_dmv_2024-05-05_page16.docx\n",
      "software-engineer_dmv_2024-05-05_page17.docx\n",
      "software-engineer_dmv_2024-05-05_page18.docx\n",
      "software-engineer_dmv_2024-05-05_page19.docx\n",
      "software-engineer_dmv_2024-05-05_page2.docx\n",
      "software-engineer_dmv_2024-05-05_page3.docx\n",
      "software-engineer_dmv_2024-05-05_page4.docx\n",
      "software-engineer_dmv_2024-05-05_page5.docx\n",
      "software-engineer_dmv_2024-05-05_page6.docx\n",
      "software-engineer_dmv_2024-05-05_page7.docx\n",
      "software-engineer_dmv_2024-05-05_page8.docx\n",
      "software-engineer_dmv_2024-05-05_page9.docx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "input_folder = str(current_dir) + r\"\\cleansed_files\"\n",
    "output_folder = str(current_dir) + r\"\\curated_files\"\n",
    "job_category_folder = str(current_dir) + r\"\\artifacts\\dependencies\\job_categories.csv\"\n",
    "dictionary_skeletons_folder = str(current_dir) + r\"\\artifacts\\dictionary_shells\"\n",
    "\n",
    "# Pre-load dictionary shells\n",
    "dictionary_shells = {\n",
    "    \"education\": create_dictionary_from_csv(os.path.join(dictionary_skeletons_folder, \"education.csv\")),\n",
    "    \"programming_languages\": create_dictionary_from_csv(os.path.join(dictionary_skeletons_folder, \"programming_languages.csv\")),\n",
    "    \"personality_traits\": create_dictionary_from_csv(os.path.join(dictionary_skeletons_folder, \"personality_traits.csv\")),\n",
    "    \"skills\": create_dictionary_from_csv(os.path.join(dictionary_skeletons_folder, \"skills.csv\")),\n",
    "    \"software\": create_dictionary_from_csv(os.path.join(dictionary_skeletons_folder, \"software.csv\")),\n",
    "    \"security_clearance\": create_dictionary_from_csv(os.path.join(dictionary_skeletons_folder, \"security_clearance.csv\"))\n",
    "}\n",
    "\n",
    "words_for_marking_desc = create_key_glossary_from_dict_shells(dictionary_skeletons_folder)\n",
    "list_of_files = os.listdir(input_folder)\n",
    "unique_job_names = create_cleansed_enviornment(list_of_files, output_folder)\n",
    "\n",
    "for job_titles in unique_job_names:\n",
    "    \n",
    "    job_id = get_job_id(job_titles, job_category_folder)\n",
    "    \n",
    "    job_specific_files = [x for x in list_of_files if job_titles in x]\n",
    "    \n",
    "    for word_docx_filename in job_specific_files:\n",
    "        print(word_docx_filename)\n",
    "        \n",
    "        document_to_string = read_word(os.path.join(input_folder, word_docx_filename))\n",
    "        \n",
    "        document_to_string_v2 = mark_phrases(document_to_string, words_for_marking_desc)\n",
    "\n",
    "        extract_experience_from_paragraph(job_id, output_folder, word_docx_filename, document_to_string_v2)\n",
    "        extract_salary_from_paragraph(job_id, output_folder, word_docx_filename, document_to_string_v2)\n",
    "\n",
    "        # Reset dictionaries to initial template by deep copying\n",
    "        dictionaries = {k: copy.deepcopy(v) for k, v in dictionary_shells.items()}\n",
    "        \n",
    "        partition_folders = ['education', 'programming_languages', 'personality_traits', 'skillset', 'software', 'security_clearance']\n",
    "\n",
    "        for counter, (category, dictionary) in enumerate(dictionaries.items()):\n",
    "            for key in dictionary:\n",
    "                regex_key = find_special_characters(key)\n",
    "                pattern = r\"(?<=\\s)[\\.,(]*\" + regex_key + r\"[\\.,)]*(?=\\s)\"\n",
    "                dictionary[key] = len(re.findall(pattern, document_to_string_v2))\n",
    "            write_dictionary_to_cleansed_layer(dictionary, job_id, output_folder, partition_folders[counter], word_docx_filename[:-5])\n",
    "            counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aed8038-03c8-414a-8637-cca8eb58950d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9a309b9a-43fa-4183-a074-16933ab80925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d68d9e33-4392-45c3-aaab-dc546188d0c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reporting Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f920619d-ee1d-4768-82e1-45aa112093d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('headstart_pipeline_code') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb79f3b3-42b8-4625-bd53-3f1f66ee99d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Education Reporting Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "5e030258-dad3-4562-aba8-106901a53abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+---------+-----+-----------+\n",
      "|job_id|degree_category_id|frequency|state|report_year|\n",
      "+------+------------------+---------+-----+-----------+\n",
      "|     0| associates degree|      1.0|  dmv|       2024|\n",
      "|     0|  bachelors degree|     48.0|  dmv|       2024|\n",
      "|     0|        highschool|      0.0|  dmv|       2024|\n",
      "|     0|    masters degree|      9.0|  dmv|       2024|\n",
      "|     0|               phd|      3.0|  dmv|       2024|\n",
      "+------+------------------+---------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_input = str(current_dir) + r\"\\curated_files\"\n",
    "s3_output = str(current_dir) + r\"\\reporting_layer\"\n",
    "skeletons_mapping_folder = str(current_dir) + r\"\\artifacts\\category_mapping\"\n",
    "\n",
    "\n",
    "education = spark.read.option(\"header\", True).csv('education_test.csv')\n",
    "education_map = spark.read.option(\"header\", True).csv(skeletons_mapping_folder + r\"\\education_category_mapping.csv\")\n",
    "\n",
    "education.createOrReplaceTempView(\"education_database\") \n",
    "education_map.createOrReplaceTempView(\"edu_map\") \n",
    "\n",
    "mapped_database = spark.sql('''\n",
    "        SELECT ed.job_id, \n",
    "               em.degree_category_id, \n",
    "               SUM(ed.frequency) AS frequency, \n",
    "               ed.state, \n",
    "               ed.report_year\n",
    "        FROM education_database ed\n",
    "        LEFT JOIN edu_map em \n",
    "            ON ed.education = em.degree \n",
    "        GROUP BY em.degree_category_id, ed.job_id, ed.state, ed.report_year\n",
    "        ORDER BY ed.job_id ASC, em.degree_category_id, report_year DESC\n",
    "    ''')\n",
    "\n",
    "\n",
    "mapped_database.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bba4e6b-7de9-448a-af70-10bcb062e231",
   "metadata": {},
   "source": [
    "# Job Description Experience In Years For Positions Reporting Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c4cb960e-7fd6-4f73-9f2a-e5f5b764ecc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+---------+-----+-----------+\n",
      "|job_id|years_experience|frequency|state|report_year|\n",
      "+------+----------------+---------+-----+-----------+\n",
      "|     0|               0|        1|  dmv|       2024|\n",
      "|     0|               1|        5|  dmv|       2024|\n",
      "|     0|               2|       16|  dmv|       2024|\n",
      "|     0|               3|       22|  dmv|       2024|\n",
      "|     0|               4|       19|  dmv|       2024|\n",
      "|     0|               5|       18|  dmv|       2024|\n",
      "|     0|               6|        6|  dmv|       2024|\n",
      "|     0|               7|        4|  dmv|       2024|\n",
      "|     0|               8|        8|  dmv|       2024|\n",
      "|     0|               9|        4|  dmv|       2024|\n",
      "|     0|              10|        9|  dmv|       2024|\n",
      "|     0|              11|        3|  dmv|       2024|\n",
      "|     0|              12|        1|  dmv|       2024|\n",
      "|     0|              14|        2|  dmv|       2024|\n",
      "+------+----------------+---------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_input = str(current_dir) + r\"\\curated_files\"\n",
    "s3_output = str(current_dir) + r\"\\reporting_layer\"\n",
    "\n",
    "experience_db = spark.read.option(\"header\", True).csv('experience_test.csv')\n",
    "\n",
    "experience_db.createOrReplaceTempView('experience_database') \n",
    "\n",
    "result_db = spark.sql(''' SELECT job_id, \n",
    "                                 CAST(years_experience_recorded AS INT) AS years_experience, \n",
    "                                 COUNT(years_experience_recorded) AS frequency,\n",
    "                                 state, \n",
    "                                 report_year\n",
    "                          FROM experience_database \n",
    "                          GROUP BY years_experience_recorded, job_id, state, report_year\n",
    "                          ORDER BY job_id ASC, years_experience ASC, report_year DESC\n",
    "    ''')\n",
    "\n",
    "result_db.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99febec6-6816-485c-a369-b64bf037824f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Job Description Personality Trait Frequencies For Positions Reporting Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5d9af6c0-7a49-4f04-8117-c08ab2a78e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+--------------+-----+-----------+\n",
      "|job_id|personality_traits|sum(frequency)|state|report_year|\n",
      "+------+------------------+--------------+-----+-----------+\n",
      "|     0|          abrasive|           5.0|  dmv|       2024|\n",
      "|     0|          abrasive|          10.0|  dmv|       2023|\n",
      "|     1|          abrasive|           0.0|  dmv|       2024|\n",
      "|     0|            abrupt|           0.0|  dmv|       2024|\n",
      "|     1|            abrupt|           0.0|  dmv|       2024|\n",
      "|     0|      absentminded|           0.0|  dmv|       2024|\n",
      "|     1|      absentminded|           0.0|  dmv|       2024|\n",
      "|     0|        accessible|           2.0|  dmv|       2024|\n",
      "|     1|        accessible|           0.0|  dmv|       2024|\n",
      "|     0|            active|          22.0|  dmv|       2024|\n",
      "|     1|            active|           0.0|  dmv|       2024|\n",
      "|     0|         adaptable|           1.0|  dmv|       2024|\n",
      "|     1|         adaptable|           0.0|  dmv|       2024|\n",
      "|     0|         admirable|           0.0|  dmv|       2024|\n",
      "|     1|         admirable|           0.0|  dmv|       2024|\n",
      "|     0|       adventurous|           0.0|  dmv|       2024|\n",
      "|     1|       adventurous|           0.0|  dmv|       2024|\n",
      "|     0|        aggressive|           0.0|  dmv|       2024|\n",
      "|     1|        aggressive|           1.0|  dmv|       2024|\n",
      "|     0|         agonizing|           0.0|  dmv|       2024|\n",
      "+------+------------------+--------------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_input = str(current_dir) + r\"\\curated_files\"\n",
    "s3_output = str(current_dir) + r\"\\reporting_layer\"\n",
    "\n",
    "personality_db = spark.read.option(\"header\", True).csv('personality_test.csv')\n",
    "\n",
    "personality_db.createOrReplaceTempView(\"personality_db\")\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "                        SELECT job_id, \n",
    "                               personality_traits, \n",
    "                               SUM(frequency), \n",
    "                               state, \n",
    "                               report_year\n",
    "                        FROM personality_db \n",
    "                        GROUP BY personality_traits, job_id, state, report_year\n",
    "                        ORDER BY personality_traits ASC, job_id ASC, report_year DESC\n",
    "                        \n",
    "                      \"\"\")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926325f7-9ed9-4330-9e77-a71d72bc7fb9",
   "metadata": {},
   "source": [
    "# Programming Language Reporting Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "edccde97-d7f2-48e9-9431-40341d75aa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------+---------+-----+-----------+\n",
      "|job_id|programming_languages|frequency|state|report_year|\n",
      "+------+---------------------+---------+-----+-----------+\n",
      "|     0|                 abap|      0.0|  dmv|       2024|\n",
      "|     0|         actionscript|      0.0|  dmv|       2024|\n",
      "|     0|                  ada|      2.0|  dmv|       2024|\n",
      "|     0|          angelscript|      0.0|  dmv|       2024|\n",
      "|     0|                 apex|      0.0|  dmv|       2024|\n",
      "|     0|                  apl|      0.0|  dmv|       2024|\n",
      "|     0|          applescript|      0.0|  dmv|       2024|\n",
      "|     0|    assembly-language|      0.0|  dmv|       2024|\n",
      "|     0|                  awk|      0.0|  dmv|       2024|\n",
      "|     0|            ballerina|      0.0|  dmv|       2024|\n",
      "|     0|                bison|      0.0|  dmv|       2024|\n",
      "|     0|                   c#|      0.0|  dmv|       2024|\n",
      "|     0|                  c++|      1.0|  dmv|       2024|\n",
      "|     0|               ceylon|      0.0|  dmv|       2024|\n",
      "|     0|               chapel|      0.0|  dmv|       2024|\n",
      "|     0|              clojure|      0.0|  dmv|       2024|\n",
      "|     0|                cobol|      0.0|  dmv|       2024|\n",
      "|     0|          cobolscript|      0.0|  dmv|       2024|\n",
      "|     0|         coffeescript|      0.0|  dmv|       2024|\n",
      "|     0|          common-lisp|      0.0|  dmv|       2024|\n",
      "+------+---------------------+---------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_input = str(current_dir) + r\"\\curated_files\"\n",
    "s3_output = str(current_dir) + r\"\\reporting_layer\"\n",
    "\n",
    "programming_db = spark.read.option(\"header\", True).csv('programming_test.csv')\n",
    "programming_db.createOrReplaceTempView(\"programming_db\")\n",
    "\n",
    "result_df = spark.sql(\"\"\" \n",
    "                         SELECT job_id, \n",
    "                                programming_languages,\n",
    "                                SUM(frequency) AS frequency, \n",
    "                                state, \n",
    "                                report_year\n",
    "                         FROM programming_db\n",
    "                         GROUP BY programming_languages, job_id, state, report_year\n",
    "                         ORDER BY programming_languages ASC, job_id ASC, report_year DESC\n",
    "                         \n",
    "                      \"\"\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c912f14-7f84-48a3-b7f8-e41e7bc5f9eb",
   "metadata": {},
   "source": [
    "# Salary Data Reporting Layer Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "0175e278-8004-46da-9d57-49917a3c0d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pay_low_end: integer (nullable = true)\n",
      " |-- pay_high_end: integer (nullable = true)\n",
      " |-- job_id: integer (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- report_year: integer (nullable = true)\n",
      "\n",
      "+------+--------------+---------------+-----+-----------+\n",
      "|job_id|low_end_salary|high_end_salary|state|report_year|\n",
      "+------+--------------+---------------+-----+-----------+\n",
      "|     0|         39000|          59007|  dmv|       2024|\n",
      "|     0|         45400|          93000|  dmv|       2024|\n",
      "|     0|         45400|          93000|  dmv|       2024|\n",
      "|     0|         52100|         119000|  dmv|       2024|\n",
      "|     0|         52100|         119000|  dmv|       2024|\n",
      "|     0|         57500|         117900|  dmv|       2024|\n",
      "|     0|         68000|          78000|  dmv|       2024|\n",
      "|     0|         72000|         155000|  dmv|       2024|\n",
      "|     0|         72000|         155000|  dmv|       2024|\n",
      "|     0|         73100|         166000|  dmv|       2024|\n",
      "|     0|         75570|         128480|  dmv|       2024|\n",
      "|     0|         88984|         100000|  dmv|       2024|\n",
      "|     0|         94400|         198300|  dmv|       2024|\n",
      "|     0|        104700|         190400|  dmv|       2024|\n",
      "|     0|        113100|         174460|  dmv|       2024|\n",
      "|     0|        113100|         174460|  dmv|       2024|\n",
      "|     0|        114000|         135000|  dmv|       2024|\n",
      "|     0|        119000|         137000|  dmv|       2024|\n",
      "|     0|        130000|         140000|  dmv|       2024|\n",
      "|     0|        130000|         150000|  dmv|       2024|\n",
      "+------+--------------+---------------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_input = str(current_dir) + r\"\\curated_files\"\n",
    "s3_output = str(current_dir) + r\"\\reporting_layer\"\n",
    "\n",
    "salary_db = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv('salary_test.csv')\n",
    "salary_db.printSchema()\n",
    "\n",
    "salary_db.createOrReplaceTempView(\"salary_db\")\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "                        SELECT job_id, \n",
    "                               pay_low_end AS low_end_salary, \n",
    "                               pay_high_end AS high_end_salary, \n",
    "                               state, \n",
    "                               report_year\n",
    "                        FROM salary_db \n",
    "                        ORDER BY pay_low_end ASC\n",
    "                      \"\"\"\n",
    "                     )\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befec160-c024-4147-a455-b27b55c9726e",
   "metadata": {},
   "source": [
    "# Security Clearance Reporting Layer Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "cea04004-fab7-46ba-8d32-787ef15583ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+---------+-----+-----------+\n",
      "|job_id|  clearance_category|frequency|state|report_year|\n",
      "+------+--------------------+---------+-----+-----------+\n",
      "|     0|        public trust|        7|  dmv|       2024|\n",
      "|     0|        public trust|        6|  dmv|       2023|\n",
      "|     1|        public trust|        5|  dmv|       2024|\n",
      "|     2|        public trust|        4|  dmv|       2023|\n",
      "|     0|    secret clearance|       27|  dmv|       2024|\n",
      "|     0|special access pr...|        0|  dmv|       2024|\n",
      "|     0|top secret clearance|       21|  dmv|       2024|\n",
      "+------+--------------------+---------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_input = str(current_dir) + r\"\\curated_files\"\n",
    "s3_output = str(current_dir) + r\"\\reporting_layer\"\n",
    "skeletons_mapping_folder = str(current_dir) + r\"\\artifacts\\category_mapping\"\n",
    "\n",
    "\n",
    "security_clearance_db = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv('security_clearance_test.csv')\n",
    "security_clearance_mapping = spark.read.option(\"header\", True).csv(skeletons_mapping_folder + r\"\\security_clearance_mapping.csv\")\n",
    "\n",
    "security_clearance_db.createOrReplaceTempView(\"security_clearance_db\")\n",
    "security_clearance_mapping.createOrReplaceTempView(\"security_clearance_mapping_db\")\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "                        \n",
    "                        SELECT scdb.job_id,\n",
    "                               map.clearance_category, \n",
    "                               SUM(scdb.frequency) AS frequency,\n",
    "                               scdb.state,\n",
    "                               scdb.report_year\n",
    "                               FROM security_clearance_db scdb\n",
    "                                   LEFT JOIN security_clearance_mapping_db map \n",
    "                                   ON scdb.security_clearance = map.security_clearance\n",
    "                               GROUP BY map.clearance_category, scdb.job_id, scdb.state, scdb.report_year\n",
    "                               ORDER BY map.clearance_category ASC, scdb.job_id ASC, scdb.report_year DESC\n",
    "                      \"\"\"\n",
    "                     )\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe224d79-5ec9-4d85-8253-cf6154b97369",
   "metadata": {},
   "source": [
    "# Skillset Reporting Layer Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "b9a8f44e-26ad-47d2-8f3d-d2056bb2a80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+--------------------+---------+-----+-----------+\n",
      "|job_id|   skill_category|            skillset|frequency|state|report_year|\n",
      "+------+-----------------+--------------------+---------+-----+-----------+\n",
      "|     0|management skills|    active-listening|        0|  dmv|       2024|\n",
      "|     0|management skills|        adaptability|        0|  dmv|       2024|\n",
      "|     0|management skills|       assertiveness|        0|  dmv|       2024|\n",
      "|     0|management skills| attention-to-detail|       15|  dmv|       2024|\n",
      "|     0|management skills|cloud-storage-and...|        0|  dmv|       2024|\n",
      "|     0|management skills|       collaboration|       16|  dmv|       2024|\n",
      "|     0|management skills|          compassion|        0|  dmv|       2024|\n",
      "|     0|management skills|          confidence|        2|  dmv|       2024|\n",
      "|     0|management skills|          creativity|        3|  dmv|       2024|\n",
      "|     0|management skills|critical-observation|        0|  dmv|       2024|\n",
      "|     0|management skills|   critical-thinking|        4|  dmv|       2024|\n",
      "|     0|management skills|  cultural-awareness|        0|  dmv|       2024|\n",
      "|     0|management skills|           curiosity|        4|  dmv|       2024|\n",
      "|     0|management skills|customer-orientation|        0|  dmv|       2024|\n",
      "|     0|management skills|cybersecurity-basics|        0|  dmv|       2024|\n",
      "|     0|management skills|data-privacy-awar...|        0|  dmv|       2024|\n",
      "|     0|management skills|     decision-making|       12|  dmv|       2024|\n",
      "|     0|management skills|          dedication|        4|  dmv|       2024|\n",
      "|     0|management skills|    digital-literacy|        0|  dmv|       2024|\n",
      "|     0|management skills|digital-nomadism-...|        0|  dmv|       2024|\n",
      "+------+-----------------+--------------------+---------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_input = str(current_dir) + r\"\\curated_files\"\n",
    "s3_output = str(current_dir) + r\"\\reporting_layer\"\n",
    "skeletons_mapping_folder = str(current_dir) + r\"\\artifacts\\category_mapping\"\n",
    "\n",
    "\n",
    "skillset_db = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv('skillset_test.csv')\n",
    "skills_category_map = spark.read.option(\"header\", True).csv(skeletons_mapping_folder + r\"\\skills_category_mapping.csv\")\n",
    "\n",
    "skillset_db.createOrReplaceTempView(\"skillset_db\") \n",
    "skills_category_map.createOrReplaceTempView(\"skills_category_map\") \n",
    "\n",
    "result_df = spark.sql(\"\"\" \n",
    "                        SELECT skills.job_id,\n",
    "                               skills_map.skill_category,\n",
    "                               skills.skillset,\n",
    "                               SUM(skills.frequency) AS frequency, \n",
    "                               skills.state, \n",
    "                               skills.report_year\n",
    "                        FROM skillset_db skills\n",
    "                            LEFT JOIN skills_category_map skills_map\n",
    "                            ON skills.skillset = skills_map.skillset\n",
    "                        GROUP BY skills.skillset, skills.job_id, skills_map.skill_category, skills.state, skills.report_year\n",
    "                        ORDER BY skills_map.skill_category ASC, skills.skillset ASC, skills.job_id ASC, skills.report_year DESC\n",
    "                      \"\"\"\n",
    "                     )\n",
    "\n",
    "result_df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b2e7a-0bde-47af-8d4c-14d35ac48cd6",
   "metadata": {},
   "source": [
    "# Software Reporting Layer Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "02c124ae-8271-4751-abbb-c2f1654d4f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-----------------+---------+-----+-----------+\n",
      "|job_id|            software|software_category|frequency|state|report_year|\n",
      "+------+--------------------+-----------------+---------+-----+-----------+\n",
      "|     0|      autodesk-revit|        3d design|        0|  dmv|       2024|\n",
      "|     0|            sketchup|        3d design|        0|  dmv|       2024|\n",
      "|     0|       alibaba-cloud|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|                 aws|  cloud platforms|        7|  dmv|       2023|\n",
      "|     0|                 aws|  cloud platforms|       10|  dmv|       2024|\n",
      "|     1|                 aws|  cloud platforms|        8|  dmv|       2023|\n",
      "|     1|                 aws|  cloud platforms|        9|  dmv|       2024|\n",
      "|     0|               azure|  cloud platforms|        3|  dmv|       2024|\n",
      "|     0|azure-virtual-mac...|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|       cloud-foundry|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|        digitalocean|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|        google-cloud|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|google-cloud-func...|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|google-cloud-storage|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|google-compute-en...|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|              heroku|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|           ibm-cloud|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|           openstack|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|              oracle|  cloud platforms|        3|  dmv|       2024|\n",
      "|     0|   red-hat-openshift|  cloud platforms|        0|  dmv|       2024|\n",
      "+------+--------------------+-----------------+---------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_input = str(current_dir) + r\"\\curated_files\"\n",
    "s3_output = str(current_dir) + r\"\\reporting_layer\"\n",
    "skeletons_mapping_folder = str(current_dir) + r\"\\artifacts\\category_mapping\"\n",
    "\n",
    "\n",
    "software_db = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv('software_test.csv')\n",
    "software_map = spark.read.option(\"header\", True).csv(skeletons_mapping_folder + r\"\\software_category_mapping.csv\")\n",
    "\n",
    "\n",
    "software_db.createOrReplaceTempView(\"software_db\") \n",
    "software_map.createOrReplaceTempView(\"software_map\") \n",
    "    \n",
    "result_df = spark.sql(\"\"\"\n",
    "                        SELECT sdb.job_id, \n",
    "                               sdb.software,\n",
    "                               map.software_category, \n",
    "                               SUM(sdb.frequency) AS frequency, \n",
    "                               sdb.state, \n",
    "                               sdb.report_year \n",
    "                        FROM software_db sdb\n",
    "                            LEFT JOIN software_map map \n",
    "                            ON sdb.software = map.software_name\n",
    "                        GROUP BY sdb.software, sdb.job_id, map.software_category, sdb.state, sdb.report_year\n",
    "                        ORDER BY map.software_category ASC, sdb.software ASC, sdb.job_id ASC, sdb.report_year ASC\n",
    "                      \"\"\"\n",
    "                     )\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ed98d-4165-47d5-a1ba-e73e2a9703b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define regex patterns as constants\n",
    "PHONE_NUMBER_PATTERN = re.compile(r'\\(?\\b\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b')\n",
    "DATE_PATTERN = re.compile(r'\\(?\\b\\d{4}\\)?[-.\\s]?\\d{2}[-.\\s]?\\d{2}\\b')\n",
    "REMOVE_TOP_HTML_PATTERN = re.compile(r'</div><div id=\"jobDescriptionText\"')\n",
    "REMOVE_BOTTOM_HTML_PATTERN = re.compile(r'</div></div>')\n",
    "MERGED_TAG_PATTERN = re.compile(r'<(\\w{1,2})(\\w+)')\n",
    "FIRST_LINE_PATTERN = re.compile(r' class=\"[^\"]*\">\\s')\n",
    "\n",
    "# Define tags and characters to remove\n",
    "TAGS_TO_REMOVE = re.compile(r'|'.join([\n",
    "    '<div>', '</div>', '<p>', '</p>', '<br>', '</br>', '<ul>', '</ul>',\n",
    "    '<i>', '</i>', '<b>', '</b>', '<li>', '</li>', '\\n', '<i>', \"'\", \n",
    "    '<h4>', '</h4>', '<h3>', '</h3>', '<h2>', '</h2>', \"â€™\", r'/', r'\\.00\\b', '  +'\n",
    "]))\n",
    "\n",
    "def clean_job_description_paragraph(job_desc_list):\n",
    "    job_description_string = []\n",
    "\n",
    "    for job_desc_html in job_desc_list:\n",
    "        try:\n",
    "            # Extract the relevant part of the HTML\n",
    "            job_desc_html_part = REMOVE_TOP_HTML_PATTERN.split(job_desc_html)[1].split(REMOVE_BOTTOM_HTML_PATTERN.pattern)[0]\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "        # Remove specified tags and characters\n",
    "        job_desc_html_part = TAGS_TO_REMOVE.sub(' ', job_desc_html_part)\n",
    "\n",
    "        # Apply regex filters\n",
    "        for regex_filter in [FIRST_LINE_PATTERN, PHONE_NUMBER_PATTERN, DATE_PATTERN]:\n",
    "            job_desc_html_part = regex_filter.sub('', job_desc_html_part)\n",
    "\n",
    "        # Refine the job description\n",
    "        refined_job_desc = MERGED_TAG_PATTERN.sub(r'\\2', job_desc_html_part)\n",
    "\n",
    "        # Append to the result list with a formatted header\n",
    "        job_description_string.append(\"\\u0332\".join(\"Job Description:  \") + refined_job_desc + \"\\n\\n -------------------------------------------------------------------------------------- \\n\\n \")\n",
    "\n",
    "    return ''.join(job_description_string).lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf65c5-6ab6-49ab-99f7-111d5eb881f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50781a-d7df-4cf9-b535-0e0a9bcc9789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae1c266-3c8a-46e3-8fb9-56e20b1e7233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4c5855-9164-4be3-94ca-8eafe4a30bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965b9bb-3ec3-4a81-b356-a398f1ac1c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f933707a-0e57-4212-a146-a9702a235728",
   "metadata": {},
   "outputs": [],
   "source": [
    "        for job_name in all_job_names: \n",
    "            job_name_prefix = prefix + job_name + \"/\"\n",
    "            job_name_response = s3.list_objects_v2(Bucket=bucket_name, Prefix=job_name_prefix)\n",
    "            \n",
    "            if 'Contents' in job_name_response:\n",
    "                # Folder exists\n",
    "                logger.info(job_name + \" folder exists and code will begin executing\")\n",
    "            else:\n",
    "                logger.info(job_name + \" folder not found, will add it now\")   \n",
    "                s3.put_object(Bucket=bucket_name, Key=job_name_prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
