{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92b94fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import re\n",
    "import csv \n",
    "import copy\n",
    "import docx2txt\n",
    "import sys\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import docx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958df1db-6187-416a-8431-6a0de161c77c",
   "metadata": {},
   "source": [
    "# Generate raw file folder path for script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b71d9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate path for raw file folder\n",
    "current_dir = os.getcwd()\n",
    "raw_files = str(current_dir)[:-14] + r\"\\databucket\\raw\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e668b3-8368-4aee-a3d6-48c1846a059a",
   "metadata": {},
   "source": [
    "# Each word document contains about 75 job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cce1914a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_word(path: str) -> str:\n",
    "    ''' Creates a string object from a word document of job descriptions div containers\n",
    "    \n",
    "        Args: \n",
    "        path (str): file path to word document locaiton \n",
    "        \n",
    "        Returns: \n",
    "        doc (str): a string made of the text of the word document \n",
    "       \n",
    "    '''\n",
    "    document = Document(path)\n",
    "    \n",
    "    # Initialize an empty string to hold the text block\n",
    "    text_block = \"\"\n",
    "\n",
    "    # Iterate through each paragraph in the document\n",
    "    for para in document.paragraphs:\n",
    "    # Append each paragraph's text to the text block, followed by a newline character\n",
    "        text_block += para.text + \"\\n\"\n",
    "    \n",
    "    return text_block \n",
    "\n",
    "\n",
    "# Get list of raw word files\n",
    "list_of_files = os.listdir(raw_files)\n",
    "\n",
    "# Read word document \n",
    "my_text = read_word(raw_files+ \"//\" + list_of_files[0])\n",
    "\n",
    "# Separate job descriptions into a split based on separator\n",
    "separate_job_objects = my_text.split('\\n\\n--------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8780e54a-7ee4-4cd8-9b4a-6cf6b87fa306",
   "metadata": {},
   "source": [
    "# An example of a raw job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5e24765-6dd8-4dcc-b912-6381690f2159",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'back-end-dev_dmv_2024-07-13_page1\\n\\n\\n<div id=\"viewJobSSRRoot\"><div id=\"mosaic-aboveViewjobNav\" class=\"mosaic mosaic-empty-zone\"></div><div class=\"fastviewjob jobsearch-ViewJobLayout--standalone css-r07ztj eu4oa1w0 hydrated\" role=\"main\"><div class=\"css-amnpyw e37uo190\"></div><div class=\"css-8ua0kf eu4oa1w0\"><div class=\"css-1xwak0u eu4oa1w0\"><div class=\"css-jr3hje eu4oa1w0\"><form action=\"/jobs\" method=\"get\" class=\"css-z48huh e37uo190\"><span class=\" css-1fr7b65 e6fjgti1\"><label id=\"text-input-what-label\" for=\"text-input-what\" aria-hidden=\"true\" class=\" css-1ndt6qv eu4oa1w0\">What</label><input aria-invalid=\"false\" id=\"text-input-what\" aria-label=\"what: job title, keywords, or company\" type=\"text\" name=\"q\" autocomplete=\"off\" placeholder=\"Job title, keywords, or company\" class=\"css-33odco e1jgz0i3\" value=\"\"></span><span class=\" css-1lglf0j e6fjgti1\"><label id=\"text-input-where-label\" for=\"text-input-where\" aria-hidden=\"true\" class=\" css-1ndt6qv eu4oa1w0\">Where</label><input aria-invalid=\"false\" id=\"text-input-where\" aria-label=\"where: city, state, or zip code\" type=\"text\" name=\"l\" autocomplete=\"off\" placeholder=\"City, state, zip code, or &quot;remote&quot;\" class=\"css-byulhf e1jgz0i3\" value=\"washington, dc\"></span><button type=\"submit\" class=\"css-au3piq e8ju0x51\">Find Jobs</button></form></div></div><div id=\"jobsearch-ViewJobLayout-rowSpacingLine\" class=\"css-1w4b90s eu4oa1w0\"></div></div><div class=\"css-1quav7f eu4oa1w0\"><div class=\"css-a8ixh3 eu4oa1w0\"><div class=\"css-52a5z7 eu4oa1w0\"><div class=\"jobsearch-JobComponent css-u4y1in eu4oa1w0\"><div class=\"jobsearch-DesktopStickyContainerTrigger css-s3qg96 eu4oa1w0\"></div><div class=\"jobsearch-InfoHeaderContainer jobsearch-DesktopStickyContainer css-zt53js eu4oa1w0\"><div><div class=\"jobsearch-JobInfoHeader-title-container  css-bbq8li eu4oa1w0\"><h1 class=\"jobsearch-JobInfoHeader-title css-1b4cr5z e1tiznh50\" lang=\"en\" dir=\"ltr\" data-testid=\"jobsearch-JobInfoHeader-title\"><span>Software Engineer (Full Stack Developer)</span></h1></div><div data-testid=\"jobsearch-CompanyInfoContainer\" class=\"css-1moflg3 eu4oa1w0\"><div class=\"css-kyg8or eu4oa1w0\"><div><div class=\"css-39gvaf eu4oa1w0\"><div class=\"css-1h46us2 eu4oa1w0\"><div data-company-name=\"true\" data-testid=\"inlineHeader-companyName\" elementtiming=\"significant-render\" class=\"css-hon9z8 eu4oa1w0\"><span class=\"css-1saizt3 e1wnkr790\"><a href=\"https://www.indeed.com/cmp/Techno--sciences,-LLC?campaignid=mobvjcmp&amp;from=mobviewjob&amp;tk=1i2ms6evji9rc800&amp;fromjk=f4f05cf59277de9c\" target=\"_blank\" aria-label=\"Techno-Sciences, llc (opens in a new tab)\" class=\"css-1ioi40n e19afand0\">Techno-Sciences, llc<svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 24 24\" aria-hidden=\"true\" class=\" css-r5jz5s eac13zx0\"><path d=\"M14.504 3a.5.5 0 00-.5.5v1a.5.5 0 00.5.5h3.085l-9.594 9.594a.5.5 0 000 .707l.707.708a.5.5 0 00.707 0l9.594-9.595V9.5a.5.5 0 00.5.5h1a.5.5 0 00.5-.5v-6a.5.5 0 00-.5-.5h-6z\"></path><path d=\"M5 3.002a2 2 0 00-2 2v13.996a2 2 0 001.996 2.004h14a2 2 0 002-2v-6.5a.5.5 0 00-.5-.5h-1a.5.5 0 00-.5.5v6.5L5 18.998V5.002L11.5 5a.495.495 0 00.496-.498v-1a.5.5 0 00-.5-.5H5z\"></path></svg></a></span></div></div><div data-testid=\"inlineHeader-companyLocation\" class=\"css-waniwe eu4oa1w0\"><div data-testid=\"job-location\" class=\"css-1ojh0uo eu4oa1w0\">4296 Forbes Boulevard, Lanham, MD 20706</div></div><div class=\"css-17cdm7w eu4oa1w0\"></div></div></div></div></div></div><div data-testid=\"jobsearch-OtherJobDetailsContainer\" class=\"css-kyg8or eu4oa1w0\"><div><div id=\"salaryInfoAndJobType\" class=\"css-1xkrvql eu4oa1w0\"><span class=\"css-19j1a75 eu4oa1w0\">From $110,000 a year</span><span class=\"css-k5flys eu4oa1w0\"> -  <!-- -->Full-time</span></div></div></div><div class=\"css-gjyqp7 eu4oa1w0\"><div class=\"css-7hagea eu4oa1w0\"></div></div><div class=\"css-1scistc e37uo190\"><div class=\"jobsearch-StickyContainerDivider-line css-8q3sg6 eu4oa1w0\"></div></div><div class=\" css-kyg8or eu4oa1w0\"></div></div><div class=\" css-1omm75o eu4oa1w0\" tabindex=\"0\"><div class=\"jobsearch-BodyContainer css-kyg8or eu4oa1w0\"><div></div><div class=\"jobsearch-JobComponent-description css-10ybyod eu4oa1w0\"><div id=\"mosaic-vjBelowViewJobHeader\" class=\"mosaic mosaic-empty-zone\"></div><div id=\"mosaic-vjProfileInsights\" class=\"mosaic-zone\" data-testid=\"vjProfileInsights-test\"><div id=\"js-match-insights-provider\" class=\"mosaic js-match-insights-provider mosaic-provider-hydrated\"><div class=\"css-kyg8or eu4oa1w0\"><div class=\"js-match-insights-provider-kyg8or eu4oa1w0\"><div class=\"js-match-insights-provider-xu0md5 eu4oa1w0\"><div class=\"js-match-insights-provider-36vfsm eu4oa1w0\"><div class=\"js-match-insights-provider-1eobdek eu4oa1w0\"><h2 class=\"js-match-insights-provider-14dlqhn e1tiznh50\">Profile insights</h2><span class=\"js-match-insights-provider-g3j9ld e1wnkr790\"><span>Your <a href=\"https://profile.indeed.com/\" aria-label=\"profile (opens in a new window)\" target=\"_blank\" rel=\"noopener\" class=\"js-match-insights-provider-1cr09u7 e19afand0\">profile<svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 24 24\" aria-hidden=\"true\" class=\" js-match-insights-provider-r5jz5s eac13zx0\"><path d=\"M14.504 3a.5.5 0 00-.5.5v1a.5.5 0 00.5.5h3.085l-9.594 9.594a.5.5 0 000 .707l.707.708a.5.5 0 00.707 0l9.594-9.595V9.5a.5.5 0 00.5.5h1a.5.5 0 00.5-.5v-6a.5.5 0 00-.5-.5h-6z\"></path><path d=\"M5 3.002a2 2 0 00-2 2v13.996a2 2 0 001.996 2.004h14a2 2 0 002-2v-6.5a.5.5 0 00-.5-.5h-1a.5.5 0 00-.5.5v6.5L5 18.998V5.002L11.5 5a.495.495 0 00.496-.498v-1a.5.5 0 00-.5-.5H5z\"></path></svg></a> might be <span class=\"js-match-insights-provider-igfe2n e1wnkr790\">missing</span> qualifications mentioned in the job description</span></span></div><div class=\"js-match-insights-provider-h05mm8 e37uo190\"><div role=\"group\" tabindex=\"0\" aria-label=\"Skills\" class=\"js-match-insights-provider-16m282m e37uo190\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 20 20\" data-testid=\"section-icon\" aria-hidden=\"true\" class=\"js-match-insights-provider-1pdva1a eac13zx0\"><path d=\"M9.75 1a.5.5 0 00-.5.5v2a.5.5 0 00.5.5h.5a.5.5 0 00.5-.5v-2a.5.5 0 00-.5-.5h-.5zM16 10.25a.5.5 0 00.5.5h2a.5.5 0 00.5-.5v-.5a.5.5 0 00-.5-.5h-2a.5.5 0 00-.5.5v.5zm-14.5.5a.5.5 0 01-.5-.5v-.5a.5.5 0 01.5-.5h2a.5.5 0 01.5.5v.5a.5.5 0 01-.5.5h-2zm2.379-6.518a.5.5 0 000 .707l.707.707a.5.5 0 00.707 0l.354-.353a.5.5 0 000-.707l-.707-.707a.5.5 0 00-.708 0l-.353.353zm12.242.708a.5.5 0 000-.708l-.353-.353a.5.5 0 00-.708 0l-.707.707a.5.5 0 000 .707l.354.353a.5.5 0 00.707 0l.707-.707zM7.5 16v1.5a.5.5 0 00.5.5h4a.5.5 0 00.5-.5V16h-5zm5-2.877a4 4 0 10-5 0V14.5h5v-1.377z\"></path></svg><div class=\"js-match-insights-provider-e6s05i eu4oa1w0\"><h3 class=\"js-match-insights-provider-11n8e9a e1tiznh50\">Skills</h3><div class=\"js-match-insights-provider-kyg8or eu4oa1w0\"><ul class=\"js-match-insights-provider-h884c4 eu4oa1w0\"><li data-testid=\"list-item\" class=\"js-match-insights-provider-1sqoe60 eu4oa1w0\"><button data-testid=\"UX-tile\" aria-label=\"Skills UX missing qualification\" class=\"js-match-insights-provider-13z8r1w e1xnxm2i0\"><div class=\"js-match-insights-provider-g6kqeb ecydgvn0\"><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\">UX</div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 18 18\" aria-live=\"polite\" aria-label=\"missing qualification\" aria-hidden=\"false\" class=\"js-match-insights-provider-f0fy6x eac13zx0\"><path d=\"M5.113 7.693a.5.5 0 010-.707l.212-.213a.5.5 0 01.707 0L9 9.741l2.969-2.968a.5.5 0 01.707 0l.212.213a.5.5 0 010 .707l-3.313 3.312a.387.387 0 01-.009.01l-.212.211a.5.5 0 01-.707 0l-.212-.212-.007-.006-3.315-3.315z\"></path></svg></div></div></button></li><li data-testid=\"list-item\" class=\"js-match-insights-provider-1sqoe60 eu4oa1w0\"><button data-testid=\"UI-tile\" aria-label=\"Skills UI missing qualification\" class=\"js-match-insights-provider-13z8r1w e1xnxm2i0\"><div class=\"js-match-insights-provider-g6kqeb ecydgvn0\"><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\">UI</div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 18 18\" aria-live=\"polite\" aria-label=\"missing qualification\" aria-hidden=\"false\" class=\"js-match-insights-provider-f0fy6x eac13zx0\"><path d=\"M5.113 7.693a.5.5 0 010-.707l.212-.213a.5.5 0 01.707 0L9 9.741l2.969-2.968a.5.5 0 01.707 0l.212.213a.5.5 0 010 .707l-3.313 3.312a.387.387 0 01-.009.01l-.212.211a.5.5 0 01-.707 0l-.212-.212-.007-.006-3.315-3.315z\"></path></svg></div></div></button></li><li data-testid=\"list-item\" class=\"js-match-insights-provider-1sqoe60 eu4oa1w0\"><button data-testid=\"Software troubleshooting-tile\" aria-label=\"Skills Software troubleshooting missing qualification\" class=\"js-match-insights-provider-13z8r1w e1xnxm2i0\"><div class=\"js-match-insights-provider-g6kqeb ecydgvn0\"><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\">Software troubleshooting</div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 18 18\" aria-live=\"polite\" aria-label=\"missing qualification\" aria-hidden=\"false\" class=\"js-match-insights-provider-f0fy6x eac13zx0\"><path d=\"M5.113 7.693a.5.5 0 010-.707l.212-.213a.5.5 0 01.707 0L9 9.741l2.969-2.968a.5.5 0 01.707 0l.212.213a.5.5 0 010 .707l-3.313 3.312a.387.387 0 01-.009.01l-.212.211a.5.5 0 01-.707 0l-.212-.212-.007-.006-3.315-3.315z\"></path></svg></div></div></button></li><li class=\"js-match-insights-provider-1gidxug eu4oa1w0\"><button class=\"js-match-insights-provider-1hvu6ko e19afand0\">+ show more</button></li></ul></div><fieldset class=\"js-match-insights-provider-1umaq0k e37uo190\"><legend class=\"js-match-insights-provider-14msz00 e1wnkr790\"><div class=\"js-match-insights-provider-kyg8or eu4oa1w0\">Do you have experience in <b>UX</b>?</div></legend><div class=\"js-match-insights-provider-1lps5p3 e37uo190\"><button data-testid=\"Do you have experience in [UX|attribute]?-yes\" class=\"js-match-insights-provider-s1k8bj e8ju0x51\"><span>Yes</span></button><button data-testid=\"Do you have experience in [UX|attribute]?-no\" class=\"js-match-insights-provider-1b1q3os e8ju0x51\"><span>No</span></button><button data-testid=\"Do you have experience in [UX|attribute]?-skip\" class=\"js-match-insights-provider-1ocbp6o e8ju0x51\"><span>Skip</span></button></div></fieldset></div></div><div role=\"group\" tabindex=\"0\" aria-label=\"Education\" class=\"js-match-insights-provider-16m282m e37uo190\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 20 20\" data-testid=\"section-icon\" aria-hidden=\"true\" class=\"js-match-insights-provider-1pdva1a eac13zx0\"><path fill-rule=\"evenodd\" d=\"M2.107 5.385A.423.423 0 002 5.676v9.59c0 .481.749.805 1.17.573.649-.355 1.58-.675 2.83-.675 1.552 0 2.613.494 3.25.935V4.837C8.613 4.396 7.552 3.9 6 3.9c-2.321 0-3.543 1.106-3.893 1.484zm8.643-.548v11.262c.637-.44 1.697-.935 3.25-.935 1.25 0 2.181.32 2.83.675.421.232 1.17-.092 1.17-.573v-9.59a.423.423 0 00-.107-.29C17.543 5.006 16.32 3.9 14 3.9c-1.553 0-2.613.495-3.25.936z\" clip-rule=\"evenodd\"></path></svg><div class=\"js-match-insights-provider-e6s05i eu4oa1w0\"><h3 class=\"js-match-insights-provider-11n8e9a e1tiznh50\">Education</h3><div class=\"js-match-insights-provider-kyg8or eu4oa1w0\"><ul class=\"js-match-insights-provider-h884c4 eu4oa1w0\"><li data-testid=\"list-item\" class=\"js-match-insights-provider-1sqoe60 eu4oa1w0\"><button data-testid=\"Bachelor\\'s degree-tile\" aria-label=\"Education Bachelor\\'s degree missing qualification\" class=\"js-match-insights-provider-13z8r1w e1xnxm2i0\"><div class=\"js-match-insights-provider-g6kqeb ecydgvn0\"><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\">Bachelor\\'s degree</div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><span class=\"js-match-insights-provider-e627ne e1wnkr790\">&nbsp;(Required)</span></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 18 18\" aria-live=\"polite\" aria-label=\"missing qualification\" aria-hidden=\"false\" class=\"js-match-insights-provider-f0fy6x eac13zx0\"><path d=\"M5.113 7.693a.5.5 0 010-.707l.212-.213a.5.5 0 01.707 0L9 9.741l2.969-2.968a.5.5 0 01.707 0l.212.213a.5.5 0 010 .707l-3.313 3.312a.387.387 0 01-.009.01l-.212.211a.5.5 0 01-.707 0l-.212-.212-.007-.006-3.315-3.315z\"></path></svg></div></div></button></li></ul></div><fieldset class=\"js-match-insights-provider-1umaq0k e37uo190\"><legend class=\"js-match-insights-provider-14msz00 e1wnkr790\"><div class=\"js-match-insights-provider-kyg8or eu4oa1w0\">Do you have a <b>Bachelor\\'s degree</b>?</div></legend><div class=\"js-match-insights-provider-1lps5p3 e37uo190\"><button data-testid=\"Do you have a [Bachelor\\'s degree|attribute]?-yes\" class=\"js-match-insights-provider-s1k8bj e8ju0x51\"><span>Yes</span></button><button data-testid=\"Do you have a [Bachelor\\'s degree|attribute]?-no\" class=\"js-match-insights-provider-1b1q3os e8ju0x51\"><span>No</span></button><button data-testid=\"Do you have a [Bachelor\\'s degree|attribute]?-skip\" class=\"js-match-insights-provider-1ocbp6o e8ju0x51\"><span>Skip</span></button></div></fieldset></div></div></div></div><div role=\"separator\" aria-orientation=\"horizontal\" class=\"js-match-insights-provider-1lugej0 e15p7aqh1\"><span class=\"js-match-insights-provider-1b6omqv esbq1260\"><span>&amp;nbsp;</span></span></div></div></div></div></div></div><div id=\"mosaic-belowVjProfileInsights\" class=\"mosaic mosaic-empty-zone\"></div><div id=\"mosaic-vjJobDetails\" class=\"mosaic-zone\" data-testid=\"vjJobDetails-test\"><div id=\"js-match-insights-provider-job-details\" class=\"mosaic js-match-insights-provider-job-details mosaic-provider-hydrated\"><div class=\"css-kyg8or eu4oa1w0\"><div id=\"jobDetailsSection\" class=\"js-match-insights-provider-kyg8or eu4oa1w0\"><div class=\"js-match-insights-provider-xu0md5 eu4oa1w0\"><div class=\"js-match-insights-provider-36vfsm eu4oa1w0\"><div class=\"js-match-insights-provider-1eobdek eu4oa1w0\"><h2 class=\"js-match-insights-provider-14dlqhn e1tiznh50\">Job details</h2><span class=\"js-match-insights-provider-g3j9ld e1wnkr790\"><span>Here’s how the job details align with your <a href=\"https://profile.indeed.com/\" aria-label=\"job preferences (opens in a new window)\" target=\"_blank\" rel=\"noopener\" class=\"js-match-insights-provider-1cr09u7 e19afand0\">profile<svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 24 24\" aria-hidden=\"true\" class=\" js-match-insights-provider-r5jz5s eac13zx0\"><path d=\"M14.504 3a.5.5 0 00-.5.5v1a.5.5 0 00.5.5h3.085l-9.594 9.594a.5.5 0 000 .707l.707.708a.5.5 0 00.707 0l9.594-9.595V9.5a.5.5 0 00.5.5h1a.5.5 0 00.5-.5v-6a.5.5 0 00-.5-.5h-6z\"></path><path d=\"M5 3.002a2 2 0 00-2 2v13.996a2 2 0 001.996 2.004h14a2 2 0 002-2v-6.5a.5.5 0 00-.5-.5h-1a.5.5 0 00-.5.5v6.5L5 18.998V5.002L11.5 5a.495.495 0 00.496-.498v-1a.5.5 0 00-.5-.5H5z\"></path></svg></a>.</span></span></div><div class=\"js-match-insights-provider-h05mm8 e37uo190\"><div role=\"group\" tabindex=\"0\" aria-label=\"Pay\" class=\"js-match-insights-provider-16m282m e37uo190\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 20 20\" data-testid=\"section-icon\" aria-hidden=\"true\" class=\"js-match-insights-provider-1pdva1a eac13zx0\"><path fill-rule=\"evenodd\" d=\"M4.452 9.604a2.5 2.5 0 00-.952-.19V7.586A2.5 2.5 0 005.996 5.09h5.094a2.5 2.5 0 002.5 2.5v1.836a2.5 2.5 0 00-2.488 2.482H5.995a2.5 2.5 0 00-1.543-2.304zM2 4.09a.5.5 0 01.5-.5h12.09a.5.5 0 01.5.5v8.818a.5.5 0 01-.5.5H2.5a.5.5 0 01-.5-.5V4.09zm6.544 6.41a2 2 0 100-4 2 2 0 000 4zM16.6 7.8v7.11H4.76a.5.5 0 00-.5.5v.5a.5.5 0 00.5.5h12.663c.442 0 .677-.222.677-.663V7.8a.5.5 0 00-.5-.5h-.5a.5.5 0 00-.5.5z\" clip-rule=\"evenodd\"></path></svg><div class=\"js-match-insights-provider-e6s05i eu4oa1w0\"><h3 class=\"js-match-insights-provider-11n8e9a e1tiznh50\">Pay</h3><div class=\"js-match-insights-provider-kyg8or eu4oa1w0\"><ul class=\"js-match-insights-provider-h884c4 eu4oa1w0\"><li data-testid=\"list-item\" class=\"js-match-insights-provider-1sqoe60 eu4oa1w0\"><button data-testid=\"From $110,000 a year-tile\" aria-label=\"Pay From $110,000 a year matching preference\" class=\"js-match-insights-provider-1seqhc e1xnxm2i0\"><div class=\"js-match-insights-provider-g6kqeb ecydgvn0\"><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 24 24\" aria-label=\"matching preference\" aria-hidden=\"true\" data-icon-type=\"HeartIcon\" class=\"js-match-insights-provider-1xqhio eac13zx0\"><title>matching preference</title><path d=\"M11.666 20.787a.5.5 0 00.668 0l1.116-1C18.6 15.21 22 12.188 22 8.486c0-3.022-2.425-5.4-5.5-5.4-1.738 0-3.412 1.311-4.5 2.571-1.088-1.26-2.762-2.571-4.5-2.571-3.075 0-5.5 2.378-5.5 5.4 0 3.702 3.4 6.724 8.55 11.314 0-.009.688.604 1.116.987z\"></path></svg></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\">From $110,000 a year</div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 18 18\" aria-live=\"polite\" aria-label=\"matching preference\" aria-hidden=\"false\" class=\"js-match-insights-provider-f0fy6x eac13zx0\"><path d=\"M5.113 7.693a.5.5 0 010-.707l.212-.213a.5.5 0 01.707 0L9 9.741l2.969-2.968a.5.5 0 01.707 0l.212.213a.5.5 0 010 .707l-3.313 3.312a.387.387 0 01-.009.01l-.212.211a.5.5 0 01-.707 0l-.212-.212-.007-.006-3.315-3.315z\"></path></svg></div></div></button></li></ul></div></div></div><div role=\"group\" tabindex=\"0\" aria-label=\"Job type\" class=\"js-match-insights-provider-16m282m e37uo190\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 20 20\" data-testid=\"section-icon\" aria-hidden=\"true\" class=\"js-match-insights-provider-1pdva1a eac13zx0\"><path fill-rule=\"evenodd\" d=\"M10 3C7 3 6 6 6 6H2.5a.5.5 0 00-.5.5V9h16V6.5a.5.5 0 00-.5-.5H14s-1-3-4-3zm2.5 3h-5s1-1.5 2.5-1.5S12.5 6 12.5 6z\" clip-rule=\"evenodd\"></path><path d=\"M8 11H2v5.5a.5.5 0 00.5.5h15a.5.5 0 00.5-.5V11h-6c0 1-1 2-2 2s-2-1-2-2z\"></path></svg><div class=\"js-match-insights-provider-e6s05i eu4oa1w0\"><h3 class=\"js-match-insights-provider-11n8e9a e1tiznh50\">Job type</h3><div class=\"js-match-insights-provider-kyg8or eu4oa1w0\"><ul class=\"js-match-insights-provider-h884c4 eu4oa1w0\"><li data-testid=\"list-item\" class=\"js-match-insights-provider-1sqoe60 eu4oa1w0\"><button data-testid=\"Full-time-tile\" aria-label=\"Job type Full-time matching preference\" class=\"js-match-insights-provider-1seqhc e1xnxm2i0\"><div class=\"js-match-insights-provider-g6kqeb ecydgvn0\"><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 24 24\" aria-label=\"matching preference\" aria-hidden=\"true\" data-icon-type=\"HeartIcon\" class=\"js-match-insights-provider-1xqhio eac13zx0\"><title>matching preference</title><path d=\"M11.666 20.787a.5.5 0 00.668 0l1.116-1C18.6 15.21 22 12.188 22 8.486c0-3.022-2.425-5.4-5.5-5.4-1.738 0-3.412 1.311-4.5 2.571-1.088-1.26-2.762-2.571-4.5-2.571-3.075 0-5.5 2.378-5.5 5.4 0 3.702 3.4 6.724 8.55 11.314 0-.009.688.604 1.116.987z\"></path></svg></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\">Full-time</div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 18 18\" aria-live=\"polite\" aria-label=\"matching preference\" aria-hidden=\"false\" class=\"js-match-insights-provider-f0fy6x eac13zx0\"><path d=\"M5.113 7.693a.5.5 0 010-.707l.212-.213a.5.5 0 01.707 0L9 9.741l2.969-2.968a.5.5 0 01.707 0l.212.213a.5.5 0 010 .707l-3.313 3.312a.387.387 0 01-.009.01l-.212.211a.5.5 0 01-.707 0l-.212-.212-.007-.006-3.315-3.315z\"></path></svg></div></div></button></li></ul></div></div></div><div role=\"group\" tabindex=\"0\" aria-label=\"Shift and schedule\" class=\"js-match-insights-provider-16m282m e37uo190\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 20 20\" data-testid=\"section-icon\" aria-hidden=\"true\" class=\"js-match-insights-provider-1pdva1a eac13zx0\"><path fill-rule=\"evenodd\" d=\"M10 18a8 8 0 100-16 8 8 0 000 16zM9.25 6.25A.25.25 0 019.5 6h1a.25.25 0 01.25.25v3.886l2.809 1.621a.25.25 0 01.091.341l-.5.866a.25.25 0 01-.341.092L9.25 11.002 9.252 11H9.25V6.25z\" clip-rule=\"evenodd\"></path></svg><div class=\"js-match-insights-provider-e6s05i eu4oa1w0\"><h3 class=\"js-match-insights-provider-11n8e9a e1tiznh50\">Shift and schedule</h3><div class=\"js-match-insights-provider-kyg8or eu4oa1w0\"><ul class=\"js-match-insights-provider-h884c4 eu4oa1w0\"><li data-testid=\"list-item\" class=\"js-match-insights-provider-1sqoe60 eu4oa1w0\"><button data-testid=\"Monday to Friday-tile\" aria-label=\"Shift and schedule Monday to Friday missing preference\" class=\"js-match-insights-provider-13z8r1w e1xnxm2i0\"><div class=\"js-match-insights-provider-g6kqeb ecydgvn0\"><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\">Monday to Friday</div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 18 18\" aria-live=\"polite\" aria-label=\"missing preference\" aria-hidden=\"false\" class=\"js-match-insights-provider-f0fy6x eac13zx0\"><path d=\"M5.113 7.693a.5.5 0 010-.707l.212-.213a.5.5 0 01.707 0L9 9.741l2.969-2.968a.5.5 0 01.707 0l.212.213a.5.5 0 010 .707l-3.313 3.312a.387.387 0 01-.009.01l-.212.211a.5.5 0 01-.707 0l-.212-.212-.007-.006-3.315-3.315z\"></path></svg></div></div></button></li></ul></div></div></div><div role=\"group\" tabindex=\"0\" aria-label=\"Work setting\" class=\"js-match-insights-provider-16m282m e37uo190\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"none\" viewBox=\"0 0 20 20\" width=\"20\" height=\"20\" data-testid=\"section-icon\" aria-hidden=\"true\" class=\"js-match-insights-provider-1pdva1a eac13zx0\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M3.5 3C3.22386 3 3 3.22386 3 3.5V16.5C3 16.7761 3.22386 17 3.5 17H5V12H9V17H16.5C16.7761 17 17 16.7761 17 16.5V9.5C17 9.22386 16.7761 9 16.5 9H12V3.5C12 3.22386 11.7761 3 11.5 3H3.5ZM5 8H10V9H5V8ZM10 5H5V6H10V5Z\" fill=\"#767676\"></path></svg><div class=\"js-match-insights-provider-e6s05i eu4oa1w0\"><h3 class=\"js-match-insights-provider-11n8e9a e1tiznh50\">Work setting</h3><div class=\"js-match-insights-provider-kyg8or eu4oa1w0\"><ul class=\"js-match-insights-provider-h884c4 eu4oa1w0\"><li data-testid=\"list-item\" class=\"js-match-insights-provider-1sqoe60 eu4oa1w0\"><button data-testid=\"In-person-tile\" aria-label=\"Work setting In-person missing preference\" class=\"js-match-insights-provider-13z8r1w e1xnxm2i0\"><div class=\"js-match-insights-provider-g6kqeb ecydgvn0\"><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\">In-person</div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"></div><div class=\"js-match-insights-provider-tvvxwd ecydgvn1\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 18 18\" aria-live=\"polite\" aria-label=\"missing preference\" aria-hidden=\"false\" class=\"js-match-insights-provider-f0fy6x eac13zx0\"><path d=\"M5.113 7.693a.5.5 0 010-.707l.212-.213a.5.5 0 01.707 0L9 9.741l2.969-2.968a.5.5 0 01.707 0l.212.213a.5.5 0 010 .707l-3.313 3.312a.387.387 0 01-.009.01l-.212.211a.5.5 0 01-.707 0l-.212-.212-.007-.006-3.315-3.315z\"></path></svg></div></div></button></li></ul></div></div></div></div><div class=\"js-match-insights-provider-bbq8li eu4oa1w0\"></div></div><div role=\"separator\" aria-orientation=\"horizontal\" class=\"js-match-insights-provider-1lugej0 e15p7aqh1\"><span class=\"js-match-insights-provider-1b6omqv esbq1260\"><span>&amp;nbsp;</span></span></div></div></div></div></div></div><div id=\"mosaic-aboveFullJobDescription\" class=\"mosaic mosaic-empty-zone\"></div><div id=\"mosaic-aboveExtractedJobDescription\" class=\"mosaic mosaic-empty-zone\"></div><div id=\"jobLocationSectionWrapper\" class=\"css-1iyxsfc eu4oa1w0\"><h2 id=\"jobLocationSectionTitle\" class=\"css-1yytfzy e1tiznh50\">Location</h2><div class=\"css-ii1scm e37uo190\"><div class=\"css-1shv1ew eu4oa1w0\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 24 24\" aria-hidden=\"true\" class=\"css-r72e78 eac13zx0\"><path fill-rule=\"evenodd\" d=\"M3.997 10l1.545-4.632A2 2 0 017.44 4h9.12a2 2 0 011.898 1.368L20.003 10H21.5a.5.5 0 01.5.5v.5a.5.5 0 01-.5.5h-.497V16c0 .74-.402 1.387-1 1.732V19.5a.5.5 0 01-.5.5h-2.001a.5.5 0 01-.5-.5V18H6.998v1.5a.5.5 0 01-.5.5H4.497a.5.5 0 01-.5-.5v-1.768c-.598-.345-1-.992-1-1.732v-4.5H2.5A.5.5 0 012 11v-.5a.5.5 0 01.5-.5h1.497zm2 0h12.005l-1.22-3.658A.5.5 0 0016.308 6H7.692a.5.5 0 00-.475.342L5.997 10zm.5 5a1.5 1.5 0 10.001-3 1.5 1.5 0 000 3zm9.504-1.5a1.5 1.5 0 113 0 1.5 1.5 0 01-3 0z\" clip-rule=\"evenodd\"></path></svg></div><div class=\"css-1tlxeot eu4oa1w0\"><h2 class=\"css-9duxnq e1tiznh50\">Estimated commute</h2><span class=\"css-3rjdy0 e1wnkr790\"><a role=\"button\" class=\"css-1f8zkg3 e19afand0\">Add your address</a> to estimate commute</span></div></div><div id=\"jobLocationWrapper\" class=\"css-1u9px01 e37uo190\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 24 24\" aria-hidden=\"true\" id=\"jobLocationIcon\" class=\"css-r72e78 eac13zx0\"><path d=\"M12 2C8.13 2 5 5.13 5 9c0 4.523 5.195 11.093 6.634 12.826a.47.47 0 00.732 0C13.805 20.093 19 13.523 19 9c0-3.87-3.13-7-7-7zm0 9.5a2.5 2.5 0 010-5 2.5 2.5 0 010 5z\"></path></svg><div id=\"jobLocationText\" class=\"css-1tlxeot eu4oa1w0\"><h2 class=\"css-9duxnq e1tiznh50\">Job address</h2><div data-testid=\"job-location\" class=\"css-1ojh0uo eu4oa1w0\">4296 Forbes Boulevard, Lanham, MD 20706</div></div></div><div role=\"separator\" aria-orientation=\"horizontal\" class=\"css-1shvrmd e15p7aqh1\"><span class=\"css-1b6omqv esbq1260\"><span>&amp;nbsp;</span></span></div></div><div id=\"benefits\" data-testid=\"benefits-test\" class=\"css-eynugf eu4oa1w0\"><h2 id=\"benefitsSectionTitle\" class=\"css-gfuqm7 e1tiznh50\">Benefits<div class=\"css-1opnqdm e1wnkr790\">Pulled from the full job description</div></h2><div class=\"css-1oelwk6 eu4oa1w0\"><div class=\"css-k3ey05 eu4oa1w0\"><span class=\"css-c62ar3 e1wnkr790\"><ul class=\"css-8tnble eu4oa1w0\"><li class=\"css-kyg8or eu4oa1w0\">401(k)</li><li class=\"css-kyg8or eu4oa1w0\">401(k) matching</li><li class=\"css-kyg8or eu4oa1w0\">Dental insurance</li><li class=\"css-kyg8or eu4oa1w0\">Employee assistance program</li><li class=\"css-kyg8or eu4oa1w0\">Flexible schedule</li><li class=\"css-kyg8or eu4oa1w0\">Flexible spending account</li><li class=\"css-kyg8or eu4oa1w0\">Health insurance</li></ul></span></div><div class=\"css-1o3wkzu eu4oa1w0\"></div></div><button data-testid=\"collapsedBenefitsButton\" class=\"css-ybnwnu e8ju0x51\"><span class=\"css-1ysvuef e1wnkr790\">Show more</span><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 24 24\" aria-hidden=\"true\" class=\" css-v7l8ke eac13zx0\"><title>chevron down</title><path d=\"M18.002 9.888c.2-.2.204-.521.008-.716l-.707-.708a.506.506 0 00-.716.01L12 13.06 7.413 8.474a.507.507 0 00-.716-.01l-.708.707a.507.507 0 00.01.717l5.647 5.648c.1.1.234.148.367.143a.491.491 0 00.34-.144h.001l.008-.008 5.64-5.64z\"></path></svg></button><div role=\"separator\" aria-orientation=\"horizontal\" class=\"css-12n77mq e15p7aqh1\"><span class=\"css-1b6omqv esbq1260\"><span>&amp;nbsp;</span></span></div></div><div id=\"jobDescriptionTitle\"><h2 tabindex=\"-1\" id=\"jobDescriptionTitleHeading\" class=\"css-wpzt8u e1tiznh50\">Full job description</h2></div><div id=\"jobDescriptionText\" class=\"jobsearch-JobComponent-description css-16y4thd eu4oa1w0\"><p>Techno-Sciences, LLC (TSi) is currently looking for a Software Engineer to design, develop, implement, and maintain our next generation life-saving software as part of our Search and Rescue (SAR) Satellite Mission Control Center (MCC) and Rescue Coordination Center (RCC) solutions. The successful candidate will work as part of an experienced and growing engineering team on a variety of challenging software development projects and activities spanning the entire engineering lifecycle from research and development to maintenance. TSi has developed its own SAR software solution and will continue to maintain, improve, and modernize its software technology stack.</p>\\n<p><b>Specific Responsibilities include:</b></p>\\n<p>· Developing new applications and features to expand TSi’s SAR products</p>\\n<p>· Enhancing and sustaining existing MCC and RCC software products</p>\\n<p>· Supporting software packaging and release management</p>\\n<p>· Writing unit, UI, and end-to-end tests for new features and fixed bugs</p>\\n<p>· Serving as Tier-3 support for deployed systems</p>\\n<p>· Providing technical training to system and field engineers</p>\\n<p><b>Required</b> <b>Skills/Experience: </b></p>\\n<p>· Experience with implementing front-end and back-end web development stacks</p>\\n<p>· Experience with object-oriented languages like Java, C++, and PHP</p>\\n<p>· Experience with developing web APIs</p>\\n<p>· Experience with managing development and production databases</p>\\n<p>· Must be a self-starter who can work well remotely across geographic locations and time zones</p>\\n<p><b>Additional Knowledge/Skills/Experience that would be helpful: </b></p>\\n<p>· Skill and experience in UI and/or UX design</p>\\n<p>· Experience with the Laravel PHP framework</p>\\n<p>· Experience with Docker, Vagrant, Git, GitHub, AWS</p>\\n<p>· Experience with GIS applications and utility programs</p>\\n<p>· Experience with Python</p>\\n<p>· Ability to travel to customer sites in the US and internationally</p>\\n<p>. Experience with working in multi-cultural environments</p>\\n<p><b>About TSi: </b>We are an advanced technology company and a global leader in Search and Rescue (SAR) Satellite Ground Station and Software solutions. Our mission is to provide high performance and reliable system to save lives around the world through innovation and global support.</p>\\n<p>Job Type: Full-time</p>\\n<p>Pay: From $110,000.00 per year</p>\\n<p>Benefits:</p>\\n<ul>\\n<li>401(k)</li>\\n<li>401(k) matching</li>\\n<li>Dental insurance</li>\\n<li>Employee assistance program</li>\\n<li>Flexible schedule</li>\\n<li>Flexible spending account</li>\\n<li>Health insurance</li>\\n<li>Health savings account</li>\\n<li>Life insurance</li>\\n<li>Paid time off</li>\\n<li>Referral program</li>\\n<li>Relocation assistance</li>\\n<li>Tuition reimbursement</li>\\n<li>Vision insurance</li>\\n</ul>\\n<p>Compensation package:</p>\\n<ul>\\n<li>Yearly bonus</li>\\n</ul>\\n<p>Experience level:</p>\\n<ul>\\n<li>4 years</li>\\n</ul>\\n<p>Schedule:</p>\\n<ul>\\n<li>Monday to Friday</li>\\n</ul>\\n<p>Education:</p>\\n<ul>\\n<li>Bachelor\\'s (Required)</li>\\n</ul>\\n<p>Experience:</p>\\n<ul>\\n<li>object-oriented programming (C++, Java, PHP,..): 4 years (Required)</li>\\n<li>work: 4 years (Required)</li>\\n<li>web APIs development: 2 years (Required)</li>\\n<li>front-end and back-end web development stack: 4 years (Required)</li>\\n</ul>\\n<p>Work Location: In person</p></div><div role=\"separator\" aria-orientation=\"horizontal\" class=\"css-1v43wac e15p7aqh1\"><span class=\"css-1b6omqv esbq1260\"><span>&amp;nbsp;</span></span></div></div><div id=\"mosaic-belowFullJobDescription\" class=\"mosaic mosaic-empty-zone\"></div><div class=\"jobsearch-JobMetadataFooter css-10ebk7k eu4oa1w0\"><div id=\"successfullySignedInModal\"></div><div class=\"indeedApplyAdaNoticeContainer css-1h9nsi5 eu4oa1w0\">If you require alternative methods of application or screening, you must approach the employer directly to request this as Indeed is not responsible for the employer\\'s application process.</div><div class=\" css-11lsxj6 eu4oa1w0\"><div id=\"jobsearch-ViewJobButtons-container\" class=\"jobsearch-ViewJobButtons-container css-15enpd4 eu4oa1w0\"><div id=\"mosaic-aboveViewjobButtons\" class=\"mosaic mosaic-empty-zone\"></div><div class=\"00d54257835779fe0770d329b301cd3311b7e1ea4c80f9d087e19d2d8f471f0d icl-u-lg-inlineBlock css-p3dq3b eu4oa1w0\"><div class=\"3d49940678559f5efaee2f120db04f62a4b4986fe0cd0f8405ce6fb3bf5d4dd3 css-19nzaev eu4oa1w0\"><div class=\"cd2b9dd601ab8402511d46cc06d51c01e87f2d8dacd3c853d7b599cfa7aa2f0c\"><span data-testid=\"4f4389e17128fc4763f2d51086b1df7c20d03b421e33c80d3d6e7b526ebf3ed9\" class=\"4f4389e17128fc4763f2d51086b1df7c20d03b421e33c80d3d6e7b526ebf3ed9 indeed-apply-status-not-applied\" data-indeed-apply-jobcountry=\"US\" data-indeed-apply-jk=\"f4f05cf59277de9c\" data-indeed-apply-clientmeta=\"{&amp;quot;vtk&amp;quot;:&amp;quot;1i2ms6evji9rc800&amp;quot;,&amp;quot;tk&amp;quot;:&amp;quot;1i2ms2o6nlhf2800&amp;quot;}\" data-indeed-apply-continueurl=\"http://www.indeed.com/viewjob?jk=f4f05cf59277de9c&amp;q=back+end+developer&amp;l=washington%2C+dc&amp;tk=1i2ms2o6nlhf2800&amp;from=web&amp;advn=3740170943484135&amp;adid=433132365&amp;pub=4a1b367933fd867b19b072952f68dceb&amp;camk=nUmJqO2E8rjO6R0BHdiF5w%3D%3D&amp;xkcb=SoCB6_M3-WwE1ywgbZ0LbzkdCdPP&amp;xpse=SoCz6_I3-WwWB33XAh0ObzkdCdPP&amp;xfps=385a695a-cb7f-462b-be6f-2313665ce468&amp;vjs=3&amp;applied=1\" data-indeed-apply-recentsearchquery=\"{&quot;what&quot;:&quot;back end developer&quot;,&quot;where&quot;:&quot;washington, dc&quot;}\" data-indeed-apply-joburl=\"https://www.indeed.com/viewjob?jk=f4f05cf59277de9c\" data-indeed-apply-pingbackurl=\"https://gdc.indeed.com/conv/orgIndApp?co=US&amp;vjtk=1i2ms6evji9rc800&amp;jk=f4f05cf59277de9c&amp;mvj=0&amp;asub=mob-norsvp-noapply&amp;acct_key=352c48d0c7eb3ee4&amp;tk=1i2ms2o6nlhf2800&amp;trk.origin=jobsearch&amp;sj=1&amp;vjfrom=web&amp;advn=3740170943484135&amp;adid=433132365&amp;xkcb=SoCB6_M3-WwE1ywgbZ0LbzkdCdPP&amp;xfps=385a695a-cb7f-462b-be6f-2313665ce468&amp;xpse=SoCz6_I3-WwWB33XAh0ObzkdCdPP&amp;astse=5e2fb3ffab846b5d&amp;assa=1631\" data-indeed-apply-onready=\"_onButtonReady\" data-indeed-apply-onappliedstatus=\"_updateIndeedApplyStatus\" data-indeed-apply-advnum=\"3740170943484135\" data-indeed-apply-nobuttonui=\"true\" data-click-handler=\"attached\"><div class=\"5ce252b12c52759551e9f3a3478d0ddc18d2e749ee67ff7c9a77c34a7da8af97 css-kyg8or eu4oa1w0\"><button id=\"b758815b6d6c23681e02ecb5a3f5b48f2686104853d8b71624a56e095cf77aa1\" title=\"\" class=\" css-t8wchy e8ju0x51\" aria-label=\"Apply now \"><div class=\"da9ac4f4af7f561e6bcd8e6299345fb6af5b49ade5e7ab1ba0b358b221156262\"><span class=\"a9de6606c2cdc5d7e72f652c1af24bec958c908eb50565688e634067306a94fc css-1hjxf1u eu4oa1w0\">Apply now</span></div></button></div></span></div></div></div><div id=\"saveJobButtonContainer\" aria-hidden=\"false\" class=\"css-e9f4jv eu4oa1w0\"><div class=\" css-kyg8or eu4oa1w0\"><div aria-live=\"assertive\"><button data-dd-action-name=\"save-job\" aria-label=\"Save this job\" class=\"css-1fm9hy1 e8ju0x51\"><svg xmlns=\"http://www.w3.org/2000/svg\" focusable=\"false\" role=\"img\" fill=\"currentColor\" viewBox=\"0 0 24 24\" class=\"css-1xqhio eac13zx0\"><title>save-icon</title><path fill-rule=\"evenodd\" d=\"M12 14.616l6 3.905V4H6v14.52l6-3.905zM4.383 21.96A.25.25 0 014 21.748V2.502a.5.5 0 01.5-.5h15a.5.5 0 01.5.5v19.246a.25.25 0 01-.383.212L12 17.002 4.383 21.96z\" clip-rule=\"evenodd\"></path></svg></button></div></div></div><div id=\"mosaic-belowViewjobButtons\" class=\"mosaic mosaic-empty-zone\"></div><div role=\"separator\" aria-orientation=\"horizontal\" class=\"css-eud5o9 e15p7aqh1\"><span class=\"css-1b6omqv esbq1260\"><span>&amp;nbsp;</span></span></div></div></div><div class=\"css-1my0t14 e37uo190\"><div class=\"css-173agvp eu4oa1w0\"><div id=\"mosaic-provider-reportcontent\" class=\"mosaic mosaic-provider-reportcontent\"><div class=\"mosaic-reportcontent-wrapper button\"><button class=\"mosaic-reportcontent-button desktop css-16q8vsx e8ju0x51\"><span class=\"mosaic-reportcontent-button-icon\"></span>Report job</button><div class=\"mosaic-reportcontent-content\"></div></div></div></div></div></div></div></div></div><div class=\"css-2oc8b5 eu4oa1w0\"><div role=\"separator\" aria-orientation=\"horizontal\" class=\"css-1shvrmd e15p7aqh1\"><span class=\"css-1b6omqv esbq1260\"><span>&amp;nbsp;</span></span></div><div id=\"relatedLinks\" data-testid=\"relatedLinks\" class=\"css-bqm003 eu4oa1w0\"><div class=\"css-t4mc7m eu4oa1w0\"><div class=\"css-e6s05i eu4oa1w0\"><div class=\"css-6thzq4 eu4oa1w0\"><a class=\"jobsearch-RelatedLinks-link css-lvbt0r emf9s7v0\" href=\"/q-full-stack-developer-l-lanham,-md-jobs.html\">Full Stack Developer jobs in Lanham, MD</a></div><div class=\"css-6thzq4 eu4oa1w0\"><a class=\"jobsearch-RelatedLinks-link css-lvbt0r emf9s7v0\" href=\"/q-techno-sciences-l-lanham%2c-md-jobs.html\">Jobs at Techno-Sciences in Lanham, MD</a></div><div class=\"css-6thzq4 eu4oa1w0\"><a class=\"jobsearch-RelatedLinks-link css-lvbt0r emf9s7v0\" href=\"/salary?from=vj&amp;q1=Full+Stack+Developer&amp;l1=Lanham%2C+MD\">Full Stack Developer salaries in Lanham, MD</a></div></div></div></div></div></div></div></div></div></div>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of job descriptions in list object\n",
    "print(len(separate_job_objects))\n",
    "\n",
    "# Print first job_description in list\n",
    "separate_job_objects[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103fe3e4-8c21-455f-9d79-98db6b7e67e8",
   "metadata": {},
   "source": [
    "# We want to extract the job description portion and clean up all remaining html tags and grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24b729a9-59eb-4772-b360-f472eae09419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_word(path: str) -> str:\n",
    "    ''' Creates a string object from a word document of job descriptions div containers\n",
    "    \n",
    "        Args: \n",
    "        path (str): file path to word document locaiton \n",
    "        \n",
    "        Returns: \n",
    "        doc (str): a string made of the text of the word document \n",
    "       \n",
    "    '''\n",
    "    document = Document(path)\n",
    "    \n",
    "    # Initialize an empty string to hold the text block\n",
    "    text_block = \"\"\n",
    "\n",
    "    # Iterate through each paragraph in the document\n",
    "    for para in document.paragraphs:\n",
    "    # Append each paragraph's text to the text block, followed by a newline character\n",
    "        text_block += para.text + \"\\n\"\n",
    "    \n",
    "    return text_block \n",
    "    \n",
    "def clean_job_description_paragraph(job_desc_list: list):\n",
    "    ''' Take dirty job description html code already separated using ('-------'), and cleans html formatting using regex expressions\n",
    "    \n",
    "        Args: \n",
    "        job_desc_list (list): dirty job description html code \n",
    "        \n",
    "        Returns: \n",
    "        job_description_string (str): string object of cleaned job description text\n",
    "        \n",
    "       \n",
    "    '''\n",
    "    \n",
    "    job_description_string = ''\n",
    "    \n",
    "    phone_number_pattern = r'\\(?\\b\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b'\n",
    "    date_pattern = r'\\(?\\b\\d{4}\\)?[-.\\s]?\\d{2}[-.\\s]?\\d{2}\\b'\n",
    "    remove_top_html_pattern = r'</div><div id=\"jobDescriptionText\"'\n",
    "    remove_bottom_html_pattern = r'</div></div>'\n",
    "    merged_tag_pattern = r'<(\\w{1,2})(\\w+)'\n",
    "    replacement = r'\\2'\n",
    "    first_line_pattern = re.compile(r' class=\"[^\"]*\">\\s')\n",
    "    \n",
    "    tags_to_remove = ['<div>', '</div>', '<p>', '</p>', '<br>', '</br>', '<ul>', '</ul>', '<i>', '</i>', '<b>', '</b>', '<li>', '</li>', '\\n', '\\n+', '<i>', \"'\", '<h4>', \n",
    "                        '</h4>', '</h3>', '<h3>', '<h2>', '</h2>', \"’\", r'/', r'\\.00\\b','  +', '<span>']\n",
    "    \n",
    "    \n",
    "    for job_desc_html in job_desc_list:\n",
    "        \n",
    "        try:\n",
    "            job_desc_html_v2 = job_desc_html.split(remove_top_html_pattern)[1]\n",
    "            job_desc_html_v3 = job_desc_html_v2.split(remove_bottom_html_pattern)[0]\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "\n",
    "\n",
    "        for items in tags_to_remove: \n",
    "            job_desc_html_v3 = re.sub(items, ' ', job_desc_html_v3)\n",
    "\n",
    "        \n",
    "        for regex_fliters in [first_line_pattern, phone_number_pattern, date_pattern]:\n",
    "            job_desc_html_v3 = re.sub(regex_fliters, '', job_desc_html_v3)\n",
    "\n",
    "        \n",
    "        refined_job_desc = re.sub(merged_tag_pattern, replacement, job_desc_html_v3)\n",
    "\n",
    "        job_description_string += \"\".join(\"Job Description:  \") + refined_job_desc +  \"\\n\\n -------------------------------------------------------------------------------------- \\n\\n \"\n",
    "        \n",
    "    return job_description_string.lower()\n",
    "\n",
    "def docx_to_csv(csv_file_path: str, input_string_list: list):\n",
    "    ''' Takes the separated job description list and creates a dataframe to readablility in AWS Glue\n",
    "    \n",
    "        Args: \n",
    "        csv_file_path (str): file path to save dataframe using word_docx title rename \n",
    "        input_string_list (list): list of job descriptions split using ('------') separator\n",
    "        \n",
    "       \n",
    "    '''\n",
    "    job_dataframe = pd.DataFrame()\n",
    "    job_dataframe['numb_description'] = range(1, len(input_string_list))\n",
    "    job_dataframe['job_description'] = input_string_list[:-1]\n",
    "    job_dataframe.to_csv(csv_file_path, index=False)\n",
    "    \n",
    "\n",
    "\n",
    "input_folder = str(current_dir)[:-14] + r\"\\databucket\\raw\"\n",
    "docx_output_folder = str(current_dir)[:-14] + r\"\\databucket\\processed_word_docs\"\n",
    "cleansed_folder = str(current_dir)[:-14] + r\"\\databucket\\cleansed_files\"\n",
    "\n",
    "list_of_files = os.listdir(input_folder)\n",
    "\n",
    "\n",
    "for word_docx in list_of_files: \n",
    "    \n",
    "    document_to_string = read_word(raw_files + r\"\\\\\" +  word_docx)\n",
    "    separate_job_objects = document_to_string.split('\\n\\n--------------------') # Separate the job from a divider marker my bot set\n",
    "    \n",
    "    repaired_doc = clean_job_description_paragraph(separate_job_objects)\n",
    "    \n",
    "    cleansed_page = Document()\n",
    "    cleansed_page.add_paragraph(repaired_doc)\n",
    "    \n",
    "    \n",
    "    new_file_path = os.path.join(docx_output_folder, word_docx)\n",
    "    cleansed_page.save(new_file_path)\n",
    "    \n",
    "    \n",
    "    csv_split_file = repaired_doc.split('--------------------------------------------------------------------------------------')\n",
    "    docx_to_csv(cleansed_folder + r\"/\" + word_docx[:-5] + '.csv', csv_split_file)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "efaa670f-40d9-4c80-9801-8aa7e44cd5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "Python int too large to convert to C long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader:\n\u001b[0;32m     11\u001b[0m             \u001b[38;5;28mprint\u001b[39m(row)  \u001b[38;5;66;03m# This should now handle larger fields without error\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcurrent_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcleansed_files\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata-analyst_dmv_2024-05-05_page1.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[55], line 6\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(csv_file_path)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_csv\u001b[39m(csv_file_path):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Increase the maximum field size allowed\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mcsv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfield_size_limit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(csv_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      9\u001b[0m         reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(file, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mOverflowError\u001b[0m: Python int too large to convert to C long"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(str(current_dir) + r\"\\cleansed_files\\data-analyst_dmv_2024-05-05_page1.csv\")\n",
    "test.head()\n",
    "\n",
    "def read_csv(csv_file_path):\n",
    "    # Increase the maximum field size allowed\n",
    "    csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "    with open(csv_file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            print(row)  # This should now handle larger fields without error\n",
    "            \n",
    "read_csv((str(current_dir) + r\"\\cleansed_files\\data-analyst_dmv_2024-05-05_page1.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf07d407-8e44-4237-8742-97eb0aad87c2",
   "metadata": {},
   "source": [
    "# This Cleansed Layer will not take the cleaned up job description paragraphs and use dictionaries to extract word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0871dbf-ed15-40fc-b1c2-ecff6d23d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary_from_csv(path_to_csv: str) -> dict:\n",
    "    ''' Creates environment folders and partitioning folders when new job title is created \n",
    "    \n",
    "        Args: \n",
    "        path_to_csv (str): string path to csv file needed to make a frequency dictionary\n",
    "        \n",
    "        Returns:\n",
    "        freq_dictionary (dict): blank frequency dictionary '''\n",
    "    \n",
    "    freq_dictionary = {} \n",
    "\n",
    "    with open(path_to_csv, mode='r', newline='') as file: \n",
    "        reader = csv.reader(file)\n",
    "        next(reader, None) # Skips header\n",
    "        for row in reader: \n",
    "            if row:\n",
    "                freq_dictionary[row[0]] = 0\n",
    "    \n",
    "    return freq_dictionary\n",
    "\n",
    "def create_cleansed_enviornment(list_of_files_in_raw: list, output_folder: str) -> list:\n",
    "    ''' Creates environment folders and partitioning folders when new job title is created \n",
    "    \n",
    "        Args: \n",
    "        list_of_files_in_raw (list): list of all files collected using os.listdir\n",
    "        \n",
    "        Returns:\n",
    "        all_job_names (list): list of all unique names collected from raw files listed'''\n",
    "        \n",
    "    partition_folders = ['education', 'programming_languages', 'personality_traits', 'skillset', 'software', 'security_clearance', 'experience', 'salary']\n",
    "    all_job_names = []\n",
    "    \n",
    "    for word_docx_filename in list_of_files_in_raw:\n",
    "\n",
    "        position_title = word_docx_filename.split('_')[0]\n",
    "    \n",
    "        if position_title not in all_job_names: all_job_names.append(position_title)\n",
    "            \n",
    "\n",
    "    for items in partition_folders:\n",
    "        if not os.path.exists(output_folder + \"//\" + items):\n",
    "            os.makedirs(output_folder + \"//\" + items)\n",
    "        \n",
    "\n",
    "        \n",
    "    return all_job_names\n",
    "\n",
    "def extract_salary_from_paragraph(job_id: str, output_folder: str, word_docx_filename: str, document_to_string: str):\n",
    "    ''' Cleans salary tuple in order to convert it into two dataframe columns after\n",
    "        \n",
    "        Args: \n",
    "        salary_tuple: tuple of low end, high end, and hourly/salary pay grades \n",
    "        \n",
    "        Returns: \n",
    "        Tuple: a tuple of job low/high end pay ranges '''\n",
    "    file_name_split = word_docx_filename.split('_')\n",
    "    state = file_name_split[1]\n",
    "    report_year = file_name_split[2].split('-')[0]\n",
    "    \n",
    "    salary_tuple = re.findall(r'(\\$?\\d{1,3}(?:k|,\\d{1,3}|\\d{1,3}))\\s*?(?:to|-)\\s*?(\\$?\\d{1,3}(?:k|,\\d{1,3}|\\d{1,3}))(?:\\s*(?:per\\s+|a\\s+)?(hour|annually|year|yearly))?', document_to_string)\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' # remove special characters\n",
    "    low_end_pay = [] \n",
    "    high_end_pay = [] \n",
    "    \n",
    "    \n",
    "    for matches in salary_tuple: \n",
    "        low = re.sub(pattern, '', matches[0]) # remove special characters\n",
    "        high = re.sub(pattern, '', matches[1]) # remove special characters\n",
    "        \n",
    "        low = re.sub('k', '000', low) # replace k with 000\n",
    "        high = re.sub('k', '000', high) \n",
    "        \n",
    "        \n",
    "        if int(low) < 20000 or int(high) < 20000:\n",
    "            \n",
    "            if matches[2] == 'year' or matches[2] == 'annually':\n",
    "                \n",
    "                if len(low) < 5: low = int(low) * 1000\n",
    "                if len(high) < 5: high = int(high) * 1000\n",
    "                \n",
    "                if int(low) < 20000 or int(high) < 20000:\n",
    "                    continue\n",
    "                    \n",
    "            elif matches[2] == 'hour' or matches[2] == 'hourly':\n",
    "                low = int(low) * 40 * 52 # Convert to salary\n",
    "                high = int(high) * 40 * 52 # Convert to salary\n",
    "                \n",
    "            else: \n",
    "                continue\n",
    "        \n",
    "        low_end_pay.append(int(low))\n",
    "        high_end_pay.append(int(high))\n",
    "        \n",
    "    salary_dataframe = pd.DataFrame()\n",
    "    salary_dataframe['pay_low_end'] = low_end_pay\n",
    "    salary_dataframe['pay_high_end'] = high_end_pay\n",
    "    salary_dataframe['job_id'] = job_id\n",
    "    salary_dataframe['state'] = state\n",
    "    salary_dataframe['report_year'] = report_year\n",
    "    \n",
    "    salary_dataframe.to_csv(output_folder + \"\\\\\" + 'salary' + \"\\\\\"  + word_docx_filename + '.csv', index=False)\n",
    "\n",
    "def extract_experience_from_paragraph(job_id: str, output_folder: str, word_docx_filename: str, document_to_string: str):\n",
    "    \"\"\"Retrieve or assign a unique job ID based on the job title.\n",
    "    \n",
    "    Args:\n",
    "    list_of_experience: a list of all experience years collected from job descriptions\n",
    "    \n",
    "    Returns:\n",
    "    years_experience: a cleaned list of professional experience\n",
    "    \"\"\"\n",
    "    file_name_split = word_docx_filename.split('_')\n",
    "    state = file_name_split[1]\n",
    "    report_year = file_name_split[2].split('-')[0]\n",
    "    \n",
    "    years_experience = []\n",
    "    list_of_experience = re.findall(re.compile(r'(\\d+\\+?|\\d+\\s*[-–to]\\s*\\d+)\\s*(years?)'), document_to_string)\n",
    "    \n",
    "    for items in list_of_experience:\n",
    "        years = re.sub(r'[^a-zA-Z0-9-]', '', items[0])\n",
    "        \n",
    "        if '-' in years: \n",
    "            years_array = years.split('-') \n",
    "            ranged_item = list(range(int(years_array[0]), int(years_array[1]) + 1))\n",
    "            years_experience.extend(ranged_item)\n",
    "        elif int(years) > 15: \n",
    "            continue\n",
    "        else: years_experience.append(int(years))\n",
    "    \n",
    "    experience_db = pd.DataFrame()\n",
    "    experience_db['years_experience_recorded'] = years_experience\n",
    "    experience_db['job_id'] = job_id\n",
    "    experience_db['state'] = state\n",
    "    experience_db['report_year'] = report_year\n",
    "    \n",
    "    experience_db.to_csv(output_folder + \"\\\\\" + 'experience' + \"\\\\\"  + word_docx_filename + '.csv', index=False)\n",
    "\n",
    "def get_job_id(job_name: str, file_path: str) -> int:\n",
    "    \"\"\"Retrieve or assign a unique job ID based on the job title.\n",
    "\n",
    "    Args:\n",
    "        job_name (str): The name of the job to retrieve or create an ID for.\n",
    "        file_path (str): Path to the CSV file containing job categories.\n",
    "\n",
    "    Returns:\n",
    "        int: The job ID.\n",
    "    \"\"\"\n",
    "    job_id_db = pd.read_csv(file_path)\n",
    "    \n",
    "    if not (job_id_db['job_title'].eq(job_name)).any():  # If the job title doesn't exist in job_categories.csv\n",
    "        new_job_id = len(job_id_db)  # Assign the next ID\n",
    "        new_data = pd.DataFrame({'job_id': [new_job_id], 'job_title': [job_name]})\n",
    "        job_id_db = pd.concat([job_id_db, new_data], ignore_index=True)\n",
    "        job_id_db.to_csv(file_path, index=False)  # Save the updated list\n",
    "        return new_job_id\n",
    "    else:\n",
    "        # Return existing ID for the job title\n",
    "        return job_id_db.loc[job_id_db['job_title'] == job_name, 'job_id'].iloc[0]\n",
    "       \n",
    "def write_dictionary_to_cleansed_layer(measurement_dictionary: dict, job_id: str, output_folder: str, folder: str, word_docx_filename: str): \n",
    "    \"\"\"Write creates a dataframe object from dictionary passed, and writes it into a csv with the same file name\n",
    "    \n",
    "    Args:\n",
    "    measurement_dictionary (dict): frequency dictionary.\n",
    "    job_id (int): unique job id.\n",
    "    output_folder (str): path to cleansed layer.\n",
    "    folder (str): job specific folder in cleansed layer.\n",
    "    word_docx_filename (str): name of file being used\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    file_name_split = word_docx_filename.split('_')\n",
    "    state = file_name_split[1]\n",
    "    report_year = file_name_split[2].split('-')[0]\n",
    "    \n",
    "    for name, dictionary in measurement_dictionary.items():\n",
    "        \n",
    "        dataframe = pd.DataFrame(list(measurement_dictionary.items()), columns = [folder, 'frequency'])\n",
    "        dataframe['job_id'] = job_id  # Adding job_id column\n",
    "        dataframe['state'] = state  # Adding state column\n",
    "        dataframe['report_year'] = report_year  # Adding report year column\n",
    "        dataframe.to_csv(output_folder + \"\\\\\" + folder + \"\\\\\"  + word_docx_filename + '.csv', index=False)\n",
    "        \n",
    "def mark_phrases(word_doc_text: str, words_for_marking_desc: list) -> str:\n",
    "    ''' Function takes multiple worded phrases from dictionaries and replaced space with '-' in order to mark them before counting \n",
    "        \n",
    "        Args: \n",
    "        word_doc_text (str): word documented converted into a string \n",
    "        words_for_marking (list) : list of phrases that will be marked to count accurately \n",
    "        \n",
    "        Returns: \n",
    "        word_doc_text (str): word document with marked text '''\n",
    "    for phrase in words_for_marking_desc:\n",
    "\n",
    "        word_doc_text = re.sub(phrase, phrase.replace(' ', '-'), word_doc_text)\n",
    "        \n",
    "    return word_doc_text\n",
    "\n",
    "def find_special_characters(s: str):\n",
    "    '''\n",
    "    Function removes non-alphanumeric characters\n",
    "    \n",
    "    Args: \n",
    "    s (str): text based value \n",
    "    \n",
    "    Returns: \n",
    "    s (str): cleaned text \n",
    "    \n",
    "    '''\n",
    "    # This pattern matches any character that is not a letter or a number\n",
    "    pattern = re.compile('[^a-zA-Z0-9]')\n",
    "    # Find all non-alphanumeric characters in the string\n",
    "    special_chars = pattern.findall(s)\n",
    "    unique_special_chars = set(special_chars)\n",
    "    \n",
    "    for special_character in unique_special_chars: \n",
    "        s = s.replace(special_character, \"\\\\\" + special_character)\n",
    "    return s\n",
    "\n",
    "def create_key_glossary_from_dict_shells(dictionary_skeletons_folder: str):\n",
    "    # Specify the path to the file\n",
    "    glossary_path = dictionary_skeletons_folder + '\\dict_key_glossary.csv'\n",
    "    \n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.exists(glossary_path):\n",
    "        glossary_db = pd.read_csv(glossary_path)\n",
    "    else:\n",
    "        glossary_db = pd.DataFrame(columns=['keys'])\n",
    "        \n",
    "        for file in os.listdir(dictionary_skeletons_folder):\n",
    "            if file.endswith('.csv'):\n",
    "                keyword_list = pd.read_csv(dictionary_skeletons_folder + r\"\\\\\" +  file).iloc[:, 0].tolist()\n",
    "                new_df = pd.DataFrame(keyword_list, columns=['keys'])\n",
    "                glossary_db = pd.concat([glossary_db, new_df], ignore_index=True)\n",
    "        \n",
    "        glossary_db.to_csv(glossary_path, index=False)\n",
    "        \n",
    "    list_of_hypend_words = [x for x in glossary_db.iloc[:, 0].tolist() if \"-\" in x]\n",
    "    words_without_hyphens = list(map(lambda word: word.replace('-', ' '), list_of_hypend_words))\n",
    "    words_without_hyphens_desc = sorted(words_without_hyphens, key=len, reverse=True)\n",
    "    \n",
    "    return words_without_hyphens_desc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1c78d54-3258-4034-97bb-62f40c990b75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-analyst_dmv_2024-05-05_page1.docx\n",
      "data-analyst_dmv_2024-05-05_page10.docx\n",
      "data-analyst_dmv_2024-05-05_page11.docx\n",
      "data-analyst_dmv_2024-05-05_page12.docx\n",
      "data-analyst_dmv_2024-05-05_page2.docx\n",
      "data-analyst_dmv_2024-05-05_page3.docx\n",
      "data-analyst_dmv_2024-05-05_page4.docx\n",
      "data-analyst_dmv_2024-05-05_page5.docx\n",
      "data-analyst_dmv_2024-05-05_page6.docx\n",
      "data-analyst_dmv_2024-05-05_page7.docx\n",
      "data-analyst_dmv_2024-05-05_page8.docx\n",
      "data-analyst_dmv_2024-05-05_page9.docx\n",
      "software-engineer_dmv_2024-05-05_page1.docx\n",
      "software-engineer_dmv_2024-05-05_page10.docx\n",
      "software-engineer_dmv_2024-05-05_page11.docx\n",
      "software-engineer_dmv_2024-05-05_page12.docx\n",
      "software-engineer_dmv_2024-05-05_page13.docx\n",
      "software-engineer_dmv_2024-05-05_page14.docx\n",
      "software-engineer_dmv_2024-05-05_page15.docx\n",
      "software-engineer_dmv_2024-05-05_page16.docx\n",
      "software-engineer_dmv_2024-05-05_page17.docx\n",
      "software-engineer_dmv_2024-05-05_page18.docx\n",
      "software-engineer_dmv_2024-05-05_page19.docx\n",
      "software-engineer_dmv_2024-05-05_page2.docx\n",
      "software-engineer_dmv_2024-05-05_page3.docx\n",
      "software-engineer_dmv_2024-05-05_page4.docx\n",
      "software-engineer_dmv_2024-05-05_page5.docx\n",
      "software-engineer_dmv_2024-05-05_page6.docx\n",
      "software-engineer_dmv_2024-05-05_page7.docx\n",
      "software-engineer_dmv_2024-05-05_page8.docx\n",
      "software-engineer_dmv_2024-05-05_page9.docx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "input_folder = str(current_dir) + r\"\\cleansed_files\"\n",
    "output_folder = str(current_dir) + r\"\\curated_files\"\n",
    "job_category_folder = str(current_dir) + r\"\\artifacts\\dependencies\\job_categories.csv\"\n",
    "dictionary_skeletons_folder = str(current_dir) + r\"\\artifacts\\dictionary_shells\"\n",
    "\n",
    "# Pre-load dictionary shells\n",
    "dictionary_shells = {\n",
    "    \"education\": create_dictionary_from_csv(os.path.join(dictionary_skeletons_folder, \"education.csv\")),\n",
    "    \"programming_languages\": create_dictionary_from_csv(os.path.join(dictionary_skeletons_folder, \"programming_languages.csv\")),\n",
    "    \"personality_traits\": create_dictionary_from_csv(os.path.join(dictionary_skeletons_folder, \"personality_traits.csv\")),\n",
    "    \"skills\": create_dictionary_from_csv(os.path.join(dictionary_skeletons_folder, \"skills.csv\")),\n",
    "    \"software\": create_dictionary_from_csv(os.path.join(dictionary_skeletons_folder, \"software.csv\")),\n",
    "    \"security_clearance\": create_dictionary_from_csv(os.path.join(dictionary_skeletons_folder, \"security_clearance.csv\"))\n",
    "}\n",
    "\n",
    "words_for_marking_desc = create_key_glossary_from_dict_shells(dictionary_skeletons_folder)\n",
    "list_of_files = os.listdir(input_folder)\n",
    "unique_job_names = create_cleansed_enviornment(list_of_files, output_folder)\n",
    "\n",
    "for job_titles in unique_job_names:\n",
    "    \n",
    "    job_id = get_job_id(job_titles, job_category_folder)\n",
    "    \n",
    "    job_specific_files = [x for x in list_of_files if job_titles in x]\n",
    "    \n",
    "    for word_docx_filename in job_specific_files:\n",
    "        print(word_docx_filename)\n",
    "        \n",
    "        document_to_string = read_word(os.path.join(input_folder, word_docx_filename))\n",
    "        \n",
    "        document_to_string_v2 = mark_phrases(document_to_string, words_for_marking_desc)\n",
    "\n",
    "        extract_experience_from_paragraph(job_id, output_folder, word_docx_filename, document_to_string_v2)\n",
    "        extract_salary_from_paragraph(job_id, output_folder, word_docx_filename, document_to_string_v2)\n",
    "\n",
    "        # Reset dictionaries to initial template by deep copying\n",
    "        dictionaries = {k: copy.deepcopy(v) for k, v in dictionary_shells.items()}\n",
    "        \n",
    "        partition_folders = ['education', 'programming_languages', 'personality_traits', 'skillset', 'software', 'security_clearance']\n",
    "\n",
    "        for counter, (category, dictionary) in enumerate(dictionaries.items()):\n",
    "            for key in dictionary:\n",
    "                regex_key = find_special_characters(key)\n",
    "                pattern = r\"(?<=\\s)[\\.,(]*\" + regex_key + r\"[\\.,)]*(?=\\s)\"\n",
    "                dictionary[key] = len(re.findall(pattern, document_to_string_v2))\n",
    "            write_dictionary_to_cleansed_layer(dictionary, job_id, output_folder, partition_folders[counter], word_docx_filename[:-5])\n",
    "            counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aed8038-03c8-414a-8637-cca8eb58950d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9a309b9a-43fa-4183-a074-16933ab80925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d68d9e33-4392-45c3-aaab-dc546188d0c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reporting Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f920619d-ee1d-4768-82e1-45aa112093d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('headstart_pipeline_code') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb79f3b3-42b8-4625-bd53-3f1f66ee99d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Education Reporting Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "5e030258-dad3-4562-aba8-106901a53abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+---------+-----+-----------+\n",
      "|job_id|degree_category_id|frequency|state|report_year|\n",
      "+------+------------------+---------+-----+-----------+\n",
      "|     0| associates degree|      1.0|  dmv|       2024|\n",
      "|     0|  bachelors degree|     48.0|  dmv|       2024|\n",
      "|     0|        highschool|      0.0|  dmv|       2024|\n",
      "|     0|    masters degree|      9.0|  dmv|       2024|\n",
      "|     0|               phd|      3.0|  dmv|       2024|\n",
      "+------+------------------+---------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_input = str(current_dir) + r\"\\curated_files\"\n",
    "s3_output = str(current_dir) + r\"\\reporting_layer\"\n",
    "skeletons_mapping_folder = str(current_dir) + r\"\\artifacts\\category_mapping\"\n",
    "\n",
    "\n",
    "education = spark.read.option(\"header\", True).csv('education_test.csv')\n",
    "education_map = spark.read.option(\"header\", True).csv(skeletons_mapping_folder + r\"\\education_category_mapping.csv\")\n",
    "\n",
    "education.createOrReplaceTempView(\"education_database\") \n",
    "education_map.createOrReplaceTempView(\"edu_map\") \n",
    "\n",
    "mapped_database = spark.sql('''\n",
    "        SELECT ed.job_id, \n",
    "               em.degree_category_id, \n",
    "               SUM(ed.frequency) AS frequency, \n",
    "               ed.state, \n",
    "               ed.report_year\n",
    "        FROM education_database ed\n",
    "        LEFT JOIN edu_map em \n",
    "            ON ed.education = em.degree \n",
    "        GROUP BY em.degree_category_id, ed.job_id, ed.state, ed.report_year\n",
    "        ORDER BY ed.job_id ASC, em.degree_category_id, report_year DESC\n",
    "    ''')\n",
    "\n",
    "\n",
    "mapped_database.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bba4e6b-7de9-448a-af70-10bcb062e231",
   "metadata": {},
   "source": [
    "# Job Description Experience In Years For Positions Reporting Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c4cb960e-7fd6-4f73-9f2a-e5f5b764ecc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+---------+-----+-----------+\n",
      "|job_id|years_experience|frequency|state|report_year|\n",
      "+------+----------------+---------+-----+-----------+\n",
      "|     0|               0|        1|  dmv|       2024|\n",
      "|     0|               1|        5|  dmv|       2024|\n",
      "|     0|               2|       16|  dmv|       2024|\n",
      "|     0|               3|       22|  dmv|       2024|\n",
      "|     0|               4|       19|  dmv|       2024|\n",
      "|     0|               5|       18|  dmv|       2024|\n",
      "|     0|               6|        6|  dmv|       2024|\n",
      "|     0|               7|        4|  dmv|       2024|\n",
      "|     0|               8|        8|  dmv|       2024|\n",
      "|     0|               9|        4|  dmv|       2024|\n",
      "|     0|              10|        9|  dmv|       2024|\n",
      "|     0|              11|        3|  dmv|       2024|\n",
      "|     0|              12|        1|  dmv|       2024|\n",
      "|     0|              14|        2|  dmv|       2024|\n",
      "+------+----------------+---------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_input = str(current_dir) + r\"\\curated_files\"\n",
    "s3_output = str(current_dir) + r\"\\reporting_layer\"\n",
    "\n",
    "experience_db = spark.read.option(\"header\", True).csv('experience_test.csv')\n",
    "\n",
    "experience_db.createOrReplaceTempView('experience_database') \n",
    "\n",
    "result_db = spark.sql(''' SELECT job_id, \n",
    "                                 CAST(years_experience_recorded AS INT) AS years_experience, \n",
    "                                 COUNT(years_experience_recorded) AS frequency,\n",
    "                                 state, \n",
    "                                 report_year\n",
    "                          FROM experience_database \n",
    "                          GROUP BY years_experience_recorded, job_id, state, report_year\n",
    "                          ORDER BY job_id ASC, years_experience ASC, report_year DESC\n",
    "    ''')\n",
    "\n",
    "result_db.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99febec6-6816-485c-a369-b64bf037824f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Job Description Personality Trait Frequencies For Positions Reporting Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5d9af6c0-7a49-4f04-8117-c08ab2a78e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+--------------+-----+-----------+\n",
      "|job_id|personality_traits|sum(frequency)|state|report_year|\n",
      "+------+------------------+--------------+-----+-----------+\n",
      "|     0|          abrasive|           5.0|  dmv|       2024|\n",
      "|     0|          abrasive|          10.0|  dmv|       2023|\n",
      "|     1|          abrasive|           0.0|  dmv|       2024|\n",
      "|     0|            abrupt|           0.0|  dmv|       2024|\n",
      "|     1|            abrupt|           0.0|  dmv|       2024|\n",
      "|     0|      absentminded|           0.0|  dmv|       2024|\n",
      "|     1|      absentminded|           0.0|  dmv|       2024|\n",
      "|     0|        accessible|           2.0|  dmv|       2024|\n",
      "|     1|        accessible|           0.0|  dmv|       2024|\n",
      "|     0|            active|          22.0|  dmv|       2024|\n",
      "|     1|            active|           0.0|  dmv|       2024|\n",
      "|     0|         adaptable|           1.0|  dmv|       2024|\n",
      "|     1|         adaptable|           0.0|  dmv|       2024|\n",
      "|     0|         admirable|           0.0|  dmv|       2024|\n",
      "|     1|         admirable|           0.0|  dmv|       2024|\n",
      "|     0|       adventurous|           0.0|  dmv|       2024|\n",
      "|     1|       adventurous|           0.0|  dmv|       2024|\n",
      "|     0|        aggressive|           0.0|  dmv|       2024|\n",
      "|     1|        aggressive|           1.0|  dmv|       2024|\n",
      "|     0|         agonizing|           0.0|  dmv|       2024|\n",
      "+------+------------------+--------------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_input = str(current_dir) + r\"\\curated_files\"\n",
    "s3_output = str(current_dir) + r\"\\reporting_layer\"\n",
    "\n",
    "personality_db = spark.read.option(\"header\", True).csv('personality_test.csv')\n",
    "\n",
    "personality_db.createOrReplaceTempView(\"personality_db\")\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "                        SELECT job_id, \n",
    "                               personality_traits, \n",
    "                               SUM(frequency), \n",
    "                               state, \n",
    "                               report_year\n",
    "                        FROM personality_db \n",
    "                        GROUP BY personality_traits, job_id, state, report_year\n",
    "                        ORDER BY personality_traits ASC, job_id ASC, report_year DESC\n",
    "                        \n",
    "                      \"\"\")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926325f7-9ed9-4330-9e77-a71d72bc7fb9",
   "metadata": {},
   "source": [
    "# Programming Language Reporting Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "edccde97-d7f2-48e9-9431-40341d75aa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------+---------+-----+-----------+\n",
      "|job_id|programming_languages|frequency|state|report_year|\n",
      "+------+---------------------+---------+-----+-----------+\n",
      "|     0|                 abap|      0.0|  dmv|       2024|\n",
      "|     0|         actionscript|      0.0|  dmv|       2024|\n",
      "|     0|                  ada|      2.0|  dmv|       2024|\n",
      "|     0|          angelscript|      0.0|  dmv|       2024|\n",
      "|     0|                 apex|      0.0|  dmv|       2024|\n",
      "|     0|                  apl|      0.0|  dmv|       2024|\n",
      "|     0|          applescript|      0.0|  dmv|       2024|\n",
      "|     0|    assembly-language|      0.0|  dmv|       2024|\n",
      "|     0|                  awk|      0.0|  dmv|       2024|\n",
      "|     0|            ballerina|      0.0|  dmv|       2024|\n",
      "|     0|                bison|      0.0|  dmv|       2024|\n",
      "|     0|                   c#|      0.0|  dmv|       2024|\n",
      "|     0|                  c++|      1.0|  dmv|       2024|\n",
      "|     0|               ceylon|      0.0|  dmv|       2024|\n",
      "|     0|               chapel|      0.0|  dmv|       2024|\n",
      "|     0|              clojure|      0.0|  dmv|       2024|\n",
      "|     0|                cobol|      0.0|  dmv|       2024|\n",
      "|     0|          cobolscript|      0.0|  dmv|       2024|\n",
      "|     0|         coffeescript|      0.0|  dmv|       2024|\n",
      "|     0|          common-lisp|      0.0|  dmv|       2024|\n",
      "+------+---------------------+---------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_input = str(current_dir) + r\"\\curated_files\"\n",
    "s3_output = str(current_dir) + r\"\\reporting_layer\"\n",
    "\n",
    "programming_db = spark.read.option(\"header\", True).csv('programming_test.csv')\n",
    "programming_db.createOrReplaceTempView(\"programming_db\")\n",
    "\n",
    "result_df = spark.sql(\"\"\" \n",
    "                         SELECT job_id, \n",
    "                                programming_languages,\n",
    "                                SUM(frequency) AS frequency, \n",
    "                                state, \n",
    "                                report_year\n",
    "                         FROM programming_db\n",
    "                         GROUP BY programming_languages, job_id, state, report_year\n",
    "                         ORDER BY programming_languages ASC, job_id ASC, report_year DESC\n",
    "                         \n",
    "                      \"\"\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c912f14-7f84-48a3-b7f8-e41e7bc5f9eb",
   "metadata": {},
   "source": [
    "# Salary Data Reporting Layer Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "0175e278-8004-46da-9d57-49917a3c0d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pay_low_end: integer (nullable = true)\n",
      " |-- pay_high_end: integer (nullable = true)\n",
      " |-- job_id: integer (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- report_year: integer (nullable = true)\n",
      "\n",
      "+------+--------------+---------------+-----+-----------+\n",
      "|job_id|low_end_salary|high_end_salary|state|report_year|\n",
      "+------+--------------+---------------+-----+-----------+\n",
      "|     0|         39000|          59007|  dmv|       2024|\n",
      "|     0|         45400|          93000|  dmv|       2024|\n",
      "|     0|         45400|          93000|  dmv|       2024|\n",
      "|     0|         52100|         119000|  dmv|       2024|\n",
      "|     0|         52100|         119000|  dmv|       2024|\n",
      "|     0|         57500|         117900|  dmv|       2024|\n",
      "|     0|         68000|          78000|  dmv|       2024|\n",
      "|     0|         72000|         155000|  dmv|       2024|\n",
      "|     0|         72000|         155000|  dmv|       2024|\n",
      "|     0|         73100|         166000|  dmv|       2024|\n",
      "|     0|         75570|         128480|  dmv|       2024|\n",
      "|     0|         88984|         100000|  dmv|       2024|\n",
      "|     0|         94400|         198300|  dmv|       2024|\n",
      "|     0|        104700|         190400|  dmv|       2024|\n",
      "|     0|        113100|         174460|  dmv|       2024|\n",
      "|     0|        113100|         174460|  dmv|       2024|\n",
      "|     0|        114000|         135000|  dmv|       2024|\n",
      "|     0|        119000|         137000|  dmv|       2024|\n",
      "|     0|        130000|         140000|  dmv|       2024|\n",
      "|     0|        130000|         150000|  dmv|       2024|\n",
      "+------+--------------+---------------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_input = str(current_dir) + r\"\\curated_files\"\n",
    "s3_output = str(current_dir) + r\"\\reporting_layer\"\n",
    "\n",
    "salary_db = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv('salary_test.csv')\n",
    "salary_db.printSchema()\n",
    "\n",
    "salary_db.createOrReplaceTempView(\"salary_db\")\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "                        SELECT job_id, \n",
    "                               pay_low_end AS low_end_salary, \n",
    "                               pay_high_end AS high_end_salary, \n",
    "                               state, \n",
    "                               report_year\n",
    "                        FROM salary_db \n",
    "                        ORDER BY pay_low_end ASC\n",
    "                      \"\"\"\n",
    "                     )\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befec160-c024-4147-a455-b27b55c9726e",
   "metadata": {},
   "source": [
    "# Security Clearance Reporting Layer Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "cea04004-fab7-46ba-8d32-787ef15583ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+---------+-----+-----------+\n",
      "|job_id|  clearance_category|frequency|state|report_year|\n",
      "+------+--------------------+---------+-----+-----------+\n",
      "|     0|        public trust|        7|  dmv|       2024|\n",
      "|     0|        public trust|        6|  dmv|       2023|\n",
      "|     1|        public trust|        5|  dmv|       2024|\n",
      "|     2|        public trust|        4|  dmv|       2023|\n",
      "|     0|    secret clearance|       27|  dmv|       2024|\n",
      "|     0|special access pr...|        0|  dmv|       2024|\n",
      "|     0|top secret clearance|       21|  dmv|       2024|\n",
      "+------+--------------------+---------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_input = str(current_dir) + r\"\\curated_files\"\n",
    "s3_output = str(current_dir) + r\"\\reporting_layer\"\n",
    "skeletons_mapping_folder = str(current_dir) + r\"\\artifacts\\category_mapping\"\n",
    "\n",
    "\n",
    "security_clearance_db = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv('security_clearance_test.csv')\n",
    "security_clearance_mapping = spark.read.option(\"header\", True).csv(skeletons_mapping_folder + r\"\\security_clearance_mapping.csv\")\n",
    "\n",
    "security_clearance_db.createOrReplaceTempView(\"security_clearance_db\")\n",
    "security_clearance_mapping.createOrReplaceTempView(\"security_clearance_mapping_db\")\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "                        \n",
    "                        SELECT scdb.job_id,\n",
    "                               map.clearance_category, \n",
    "                               SUM(scdb.frequency) AS frequency,\n",
    "                               scdb.state,\n",
    "                               scdb.report_year\n",
    "                               FROM security_clearance_db scdb\n",
    "                                   LEFT JOIN security_clearance_mapping_db map \n",
    "                                   ON scdb.security_clearance = map.security_clearance\n",
    "                               GROUP BY map.clearance_category, scdb.job_id, scdb.state, scdb.report_year\n",
    "                               ORDER BY map.clearance_category ASC, scdb.job_id ASC, scdb.report_year DESC\n",
    "                      \"\"\"\n",
    "                     )\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe224d79-5ec9-4d85-8253-cf6154b97369",
   "metadata": {},
   "source": [
    "# Skillset Reporting Layer Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "b9a8f44e-26ad-47d2-8f3d-d2056bb2a80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+--------------------+---------+-----+-----------+\n",
      "|job_id|   skill_category|            skillset|frequency|state|report_year|\n",
      "+------+-----------------+--------------------+---------+-----+-----------+\n",
      "|     0|management skills|    active-listening|        0|  dmv|       2024|\n",
      "|     0|management skills|        adaptability|        0|  dmv|       2024|\n",
      "|     0|management skills|       assertiveness|        0|  dmv|       2024|\n",
      "|     0|management skills| attention-to-detail|       15|  dmv|       2024|\n",
      "|     0|management skills|cloud-storage-and...|        0|  dmv|       2024|\n",
      "|     0|management skills|       collaboration|       16|  dmv|       2024|\n",
      "|     0|management skills|          compassion|        0|  dmv|       2024|\n",
      "|     0|management skills|          confidence|        2|  dmv|       2024|\n",
      "|     0|management skills|          creativity|        3|  dmv|       2024|\n",
      "|     0|management skills|critical-observation|        0|  dmv|       2024|\n",
      "|     0|management skills|   critical-thinking|        4|  dmv|       2024|\n",
      "|     0|management skills|  cultural-awareness|        0|  dmv|       2024|\n",
      "|     0|management skills|           curiosity|        4|  dmv|       2024|\n",
      "|     0|management skills|customer-orientation|        0|  dmv|       2024|\n",
      "|     0|management skills|cybersecurity-basics|        0|  dmv|       2024|\n",
      "|     0|management skills|data-privacy-awar...|        0|  dmv|       2024|\n",
      "|     0|management skills|     decision-making|       12|  dmv|       2024|\n",
      "|     0|management skills|          dedication|        4|  dmv|       2024|\n",
      "|     0|management skills|    digital-literacy|        0|  dmv|       2024|\n",
      "|     0|management skills|digital-nomadism-...|        0|  dmv|       2024|\n",
      "+------+-----------------+--------------------+---------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_input = str(current_dir) + r\"\\curated_files\"\n",
    "s3_output = str(current_dir) + r\"\\reporting_layer\"\n",
    "skeletons_mapping_folder = str(current_dir) + r\"\\artifacts\\category_mapping\"\n",
    "\n",
    "\n",
    "skillset_db = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv('skillset_test.csv')\n",
    "skills_category_map = spark.read.option(\"header\", True).csv(skeletons_mapping_folder + r\"\\skills_category_mapping.csv\")\n",
    "\n",
    "skillset_db.createOrReplaceTempView(\"skillset_db\") \n",
    "skills_category_map.createOrReplaceTempView(\"skills_category_map\") \n",
    "\n",
    "result_df = spark.sql(\"\"\" \n",
    "                        SELECT skills.job_id,\n",
    "                               skills_map.skill_category,\n",
    "                               skills.skillset,\n",
    "                               SUM(skills.frequency) AS frequency, \n",
    "                               skills.state, \n",
    "                               skills.report_year\n",
    "                        FROM skillset_db skills\n",
    "                            LEFT JOIN skills_category_map skills_map\n",
    "                            ON skills.skillset = skills_map.skillset\n",
    "                        GROUP BY skills.skillset, skills.job_id, skills_map.skill_category, skills.state, skills.report_year\n",
    "                        ORDER BY skills_map.skill_category ASC, skills.skillset ASC, skills.job_id ASC, skills.report_year DESC\n",
    "                      \"\"\"\n",
    "                     )\n",
    "\n",
    "result_df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b2e7a-0bde-47af-8d4c-14d35ac48cd6",
   "metadata": {},
   "source": [
    "# Software Reporting Layer Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "02c124ae-8271-4751-abbb-c2f1654d4f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-----------------+---------+-----+-----------+\n",
      "|job_id|            software|software_category|frequency|state|report_year|\n",
      "+------+--------------------+-----------------+---------+-----+-----------+\n",
      "|     0|      autodesk-revit|        3d design|        0|  dmv|       2024|\n",
      "|     0|            sketchup|        3d design|        0|  dmv|       2024|\n",
      "|     0|       alibaba-cloud|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|                 aws|  cloud platforms|        7|  dmv|       2023|\n",
      "|     0|                 aws|  cloud platforms|       10|  dmv|       2024|\n",
      "|     1|                 aws|  cloud platforms|        8|  dmv|       2023|\n",
      "|     1|                 aws|  cloud platforms|        9|  dmv|       2024|\n",
      "|     0|               azure|  cloud platforms|        3|  dmv|       2024|\n",
      "|     0|azure-virtual-mac...|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|       cloud-foundry|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|        digitalocean|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|        google-cloud|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|google-cloud-func...|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|google-cloud-storage|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|google-compute-en...|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|              heroku|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|           ibm-cloud|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|           openstack|  cloud platforms|        0|  dmv|       2024|\n",
      "|     0|              oracle|  cloud platforms|        3|  dmv|       2024|\n",
      "|     0|   red-hat-openshift|  cloud platforms|        0|  dmv|       2024|\n",
      "+------+--------------------+-----------------+---------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_input = str(current_dir) + r\"\\curated_files\"\n",
    "s3_output = str(current_dir) + r\"\\reporting_layer\"\n",
    "skeletons_mapping_folder = str(current_dir) + r\"\\artifacts\\category_mapping\"\n",
    "\n",
    "\n",
    "software_db = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv('software_test.csv')\n",
    "software_map = spark.read.option(\"header\", True).csv(skeletons_mapping_folder + r\"\\software_category_mapping.csv\")\n",
    "\n",
    "\n",
    "software_db.createOrReplaceTempView(\"software_db\") \n",
    "software_map.createOrReplaceTempView(\"software_map\") \n",
    "    \n",
    "result_df = spark.sql(\"\"\"\n",
    "                        SELECT sdb.job_id, \n",
    "                               sdb.software,\n",
    "                               map.software_category, \n",
    "                               SUM(sdb.frequency) AS frequency, \n",
    "                               sdb.state, \n",
    "                               sdb.report_year \n",
    "                        FROM software_db sdb\n",
    "                            LEFT JOIN software_map map \n",
    "                            ON sdb.software = map.software_name\n",
    "                        GROUP BY sdb.software, sdb.job_id, map.software_category, sdb.state, sdb.report_year\n",
    "                        ORDER BY map.software_category ASC, sdb.software ASC, sdb.job_id ASC, sdb.report_year ASC\n",
    "                      \"\"\"\n",
    "                     )\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ed98d-4165-47d5-a1ba-e73e2a9703b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define regex patterns as constants\n",
    "PHONE_NUMBER_PATTERN = re.compile(r'\\(?\\b\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b')\n",
    "DATE_PATTERN = re.compile(r'\\(?\\b\\d{4}\\)?[-.\\s]?\\d{2}[-.\\s]?\\d{2}\\b')\n",
    "REMOVE_TOP_HTML_PATTERN = re.compile(r'</div><div id=\"jobDescriptionText\"')\n",
    "REMOVE_BOTTOM_HTML_PATTERN = re.compile(r'</div></div>')\n",
    "MERGED_TAG_PATTERN = re.compile(r'<(\\w{1,2})(\\w+)')\n",
    "FIRST_LINE_PATTERN = re.compile(r' class=\"[^\"]*\">\\s')\n",
    "\n",
    "# Define tags and characters to remove\n",
    "TAGS_TO_REMOVE = re.compile(r'|'.join([\n",
    "    '<div>', '</div>', '<p>', '</p>', '<br>', '</br>', '<ul>', '</ul>',\n",
    "    '<i>', '</i>', '<b>', '</b>', '<li>', '</li>', '\\n', '<i>', \"'\", \n",
    "    '<h4>', '</h4>', '<h3>', '</h3>', '<h2>', '</h2>', \"’\", r'/', r'\\.00\\b', '  +'\n",
    "]))\n",
    "\n",
    "def clean_job_description_paragraph(job_desc_list):\n",
    "    job_description_string = []\n",
    "\n",
    "    for job_desc_html in job_desc_list:\n",
    "        try:\n",
    "            # Extract the relevant part of the HTML\n",
    "            job_desc_html_part = REMOVE_TOP_HTML_PATTERN.split(job_desc_html)[1].split(REMOVE_BOTTOM_HTML_PATTERN.pattern)[0]\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "        # Remove specified tags and characters\n",
    "        job_desc_html_part = TAGS_TO_REMOVE.sub(' ', job_desc_html_part)\n",
    "\n",
    "        # Apply regex filters\n",
    "        for regex_filter in [FIRST_LINE_PATTERN, PHONE_NUMBER_PATTERN, DATE_PATTERN]:\n",
    "            job_desc_html_part = regex_filter.sub('', job_desc_html_part)\n",
    "\n",
    "        # Refine the job description\n",
    "        refined_job_desc = MERGED_TAG_PATTERN.sub(r'\\2', job_desc_html_part)\n",
    "\n",
    "        # Append to the result list with a formatted header\n",
    "        job_description_string.append(\"\\u0332\".join(\"Job Description:  \") + refined_job_desc + \"\\n\\n -------------------------------------------------------------------------------------- \\n\\n \")\n",
    "\n",
    "    return ''.join(job_description_string).lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf65c5-6ab6-49ab-99f7-111d5eb881f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50781a-d7df-4cf9-b535-0e0a9bcc9789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae1c266-3c8a-46e3-8fb9-56e20b1e7233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4c5855-9164-4be3-94ca-8eafe4a30bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965b9bb-3ec3-4a81-b356-a398f1ac1c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f933707a-0e57-4212-a146-a9702a235728",
   "metadata": {},
   "outputs": [],
   "source": [
    "        for job_name in all_job_names: \n",
    "            job_name_prefix = prefix + job_name + \"/\"\n",
    "            job_name_response = s3.list_objects_v2(Bucket=bucket_name, Prefix=job_name_prefix)\n",
    "            \n",
    "            if 'Contents' in job_name_response:\n",
    "                # Folder exists\n",
    "                logger.info(job_name + \" folder exists and code will begin executing\")\n",
    "            else:\n",
    "                logger.info(job_name + \" folder not found, will add it now\")   \n",
    "                s3.put_object(Bucket=bucket_name, Key=job_name_prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
