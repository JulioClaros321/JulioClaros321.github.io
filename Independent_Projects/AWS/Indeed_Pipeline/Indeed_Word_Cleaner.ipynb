{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "92b94fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your UUID is: 41e424a0-9334-49fb-b227-d8f31d475a16\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import docx\n",
    "from docx import Document\n",
    "import re\n",
    "import csv \n",
    "import uuid\n",
    "\n",
    "myuuid = uuid.uuid4()\n",
    "\n",
    "print('Your UUID is: ' + str(myuuid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1ed2993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "raw_files = str(current_dir) + r\"\\raw_files\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b71d9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_word(path):\n",
    "    '''Creates a string object from a word document of job descriptions div containers'''\n",
    "    doc = Document(path)\n",
    "    full_text = []\n",
    "\n",
    "    # Iterate document object and extract text\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "\n",
    "    # Join and create string object \n",
    "    return '\\n'.join(full_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e668b3-8368-4aee-a3d6-48c1846a059a",
   "metadata": {},
   "source": [
    "# Each word document collected contains about 75 job description div containers, below is how one looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cce1914a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_of_files = os.listdir(raw_files)\n",
    "document_to_string = read_word(raw_files+ \"//\" + list_of_files[0])\n",
    "separate_job_objects = document_to_string.split('\\n\\n--------------------')\n",
    "#separate_job_objects[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103fe3e4-8c21-455f-9d79-98db6b7e67e8",
   "metadata": {},
   "source": [
    "# We want to extract the job description portion and clean up all remaining html tags and grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "24b729a9-59eb-4772-b360-f472eae09419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_job_description_paragraph(job_desc_list):\n",
    "    \n",
    "    job_description_string = ''\n",
    "    \n",
    "    phone_number_pattern = r'\\(?\\b\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b'\n",
    "    date_pattern = r'\\(?\\b\\d{4}\\)?[-.\\s]?\\d{2}[-.\\s]?\\d{2}\\b'\n",
    "    remove_top_html_pattern = r'</div><div id=\"jobDescriptionText\"'\n",
    "    remove_bottom_html_pattern = r'</div></div>'\n",
    "    merged_tag_pattern = r'<(\\w{1,2})(\\w+)'\n",
    "    replacement = r'\\2'\n",
    "    first_line_pattern = re.compile(r' class=\"[^\"]*\">\\s')\n",
    "    \n",
    "    tags_to_remove = ['<div>', '</div>', '<p>', '</p>', '<br>', '</br>', '<ul>', '</ul>', '<i>', '</i>', '<b>', '</b>', '<li>', '</li>', '\\n', '\\n+', '<i>', \"'\", '<h4>', \n",
    "                        '</h4>', '</h3>', '<h3>', '<h2>', '</h2>', \"’\", r'/', r'\\.00\\b','  +']\n",
    "    \n",
    "    \n",
    "    for job_desc_html in job_desc_list:\n",
    "        try:\n",
    "            job_desc_html_v2 = job_desc_html.split(remove_top_html_pattern)[1]\n",
    "            job_desc_html_v3 = job_desc_html_v2.split(remove_bottom_html_pattern)[0]\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "\n",
    "\n",
    "        for items in tags_to_remove: \n",
    "            job_desc_html_v3 = re.sub(items, ' ', job_desc_html_v3)\n",
    "\n",
    "        \n",
    "        for regex_fliters in [first_line_pattern, phone_number_pattern, date_pattern]:\n",
    "            job_desc_html_v3 = re.sub(regex_fliters, '', job_desc_html_v3)\n",
    "\n",
    "        \n",
    "        refined_job_desc = re.sub(merged_tag_pattern, replacement, job_desc_html_v3)\n",
    "\n",
    "        job_description_string += \"\\u0332\".join(\"Job Description:  \") + refined_job_desc +  \"\\n\\n -------------------------------------------------------------------------------------- \\n\\n \"\n",
    "        \n",
    "    return job_description_string.lower()\n",
    "\n",
    "input_folder = str(current_dir) + r\"\\raw_files\" \n",
    "output_folder = str(current_dir) + r\"\\cleansed_files\"\n",
    "\n",
    "list_of_files = os.listdir(input_folder)\n",
    "\n",
    "list_of_files\n",
    "for word_docx in list_of_files: \n",
    "    \n",
    "    document_to_string = read_word(raw_files + r\"\\\\\" +  word_docx)\n",
    "    separate_job_objects = document_to_string.split('\\n\\n--------------------') # Separate the job from a divider marker my bot set\n",
    "    \n",
    "    repaired_doc = clean_job_description_paragraph(separate_job_objects)\n",
    "    \n",
    "    cleansed_page = Document()\n",
    "    cleansed_page.add_paragraph(repaired_doc)\n",
    "    \n",
    "    \n",
    "    new_file_path = os.path.join(output_folder, word_docx)\n",
    "    cleansed_page.save(new_file_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf07d407-8e44-4237-8742-97eb0aad87c2",
   "metadata": {},
   "source": [
    "# This Cleansed Layer will not take the cleaned up job description paragraphs and use dictionaries to extract word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a0871dbf-ed15-40fc-b1c2-ecff6d23d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary_from_csv(path_to_csv: str) -> dict:\n",
    "    ''' Creates environment folders and partitioning folders when new job title is created \n",
    "    \n",
    "        Args: \n",
    "        path_to_csv (str): string path to csv file needed to make a frequency dictionary\n",
    "        \n",
    "        Returns:\n",
    "        freq_dictionary (dict): blank frequency dictionary '''\n",
    "    \n",
    "    freq_dictionary = {} \n",
    "\n",
    "    with open(path_to_csv, mode='r', newline='') as file: \n",
    "        reader = csv.reader(file)\n",
    "        next(reader, None) # Skips header\n",
    "        for row in reader: \n",
    "            if row:\n",
    "                freq_dictionary[row[0]] = 0\n",
    "    \n",
    "    return freq_dictionary\n",
    "\n",
    "def create_cleansed_enviornment(list_of_files_in_raw: list, output_folder: str) -> list:\n",
    "    ''' Creates environment folders and partitioning folders when new job title is created \n",
    "    \n",
    "        Args: \n",
    "        list_of_files_in_raw (list): list of all files collected using os.listdir\n",
    "        \n",
    "        Returns:\n",
    "        all_job_names (list): list of all unique names collected from raw files listed'''\n",
    "        \n",
    "    partition_folders = ['education', 'programming_languages', 'personality_traits', 'skillset', 'software', 'security_clearance', 'experience', 'salary']\n",
    "    all_job_names = []\n",
    "    \n",
    "    for word_docx_filename in list_of_files_in_raw:\n",
    "\n",
    "        position_title = word_docx_filename.split('_')[0]\n",
    "    \n",
    "        if position_title not in all_job_names: all_job_names.append(position_title)\n",
    "            \n",
    "\n",
    "    for items in partition_folders:\n",
    "        if not os.path.exists(output_folder + \"//\" + items):\n",
    "            os.makedirs(output_folder + \"//\" + items)\n",
    "        \n",
    "\n",
    "        \n",
    "    return all_job_names\n",
    "\n",
    "def extract_salary_from_paragraph(job_id: str, output_folder: str, word_docx_filename: str, document_to_string: str):\n",
    "    ''' Cleans salary tuple in order to convert it into two dataframe columns after\n",
    "        \n",
    "        Args: \n",
    "        salary_tuple: tuple of low end, high end, and hourly/salary pay grades \n",
    "        \n",
    "        Returns: \n",
    "        Tuple: a tuple of job low/high end pay ranges '''\n",
    "    \n",
    "    salary_tuple = re.findall(r'(\\$?\\d{1,3}(?:k|,\\d{1,3}|\\d{1,3}))\\s*?(?:to|-)\\s*?(\\$?\\d{1,3}(?:k|,\\d{1,3}|\\d{1,3}))(?:\\s*(?:per\\s+|a\\s+)?(hour|annually|year|yearly))?', document_to_string)\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' # remove special characters\n",
    "    low_end_pay = [] \n",
    "    high_end_pay = [] \n",
    "    \n",
    "    \n",
    "    for matches in salary_tuple: \n",
    "        low = re.sub(pattern, '', matches[0]) # remove special characters\n",
    "        high = re.sub(pattern, '', matches[1]) # remove special characters\n",
    "        \n",
    "        low = re.sub('k', '000', low) # replace k with 000\n",
    "        high = re.sub('k', '000', high) \n",
    "        \n",
    "        \n",
    "        if int(low) < 20000 or int(high) < 20000:\n",
    "            \n",
    "            if matches[2] == 'year' or matches[2] == 'annually':\n",
    "                \n",
    "                if len(low) < 5: low = int(low) * 1000\n",
    "                if len(high) < 5: high = int(high) * 1000\n",
    "                \n",
    "                if int(low) < 20000 or int(high) < 20000:\n",
    "                    continue\n",
    "                    \n",
    "            elif matches[2] == 'hour' or matches[2] == 'hourly':\n",
    "                low = int(low) * 40 * 52 # Convert to salary\n",
    "                high = int(high) * 40 * 52 # Convert to salary\n",
    "                \n",
    "            else: \n",
    "                continue\n",
    "        \n",
    "        low_end_pay.append(int(low))\n",
    "        high_end_pay.append(int(high))\n",
    "        \n",
    "    salary_dataframe = pd.DataFrame()\n",
    "    salary_dataframe['pay_low_end'] = low_end_pay\n",
    "    salary_dataframe['pay_high_end'] = high_end_pay\n",
    "    salary_dataframe['job_id'] = job_id\n",
    "    \n",
    "    salary_dataframe.to_csv(output_folder + \"\\\\\" + 'salary' + \"\\\\\"  + word_docx_filename + '.csv', index=False)\n",
    "\n",
    "def extract_experience_from_paragraph(job_id: str, output_folder: str, word_docx_filename: str, document_to_string: str):\n",
    "    \"\"\"Retrieve or assign a unique job ID based on the job title.\n",
    "    \n",
    "    Args:\n",
    "    list_of_experience: a list of all experience years collected from job descriptions\n",
    "    \n",
    "    Returns:\n",
    "    years_experience: a cleaned list of professional experience\n",
    "    \"\"\"\n",
    "    years_experience = []\n",
    "    list_of_experience = re.findall(re.compile(r'(\\d+\\+?|\\d+\\s*[-–to]\\s*\\d+)\\s*(years?)'), document_to_string)\n",
    "    \n",
    "    for items in list_of_experience:\n",
    "        years = re.sub(r'[^a-zA-Z0-9-]', '', items[0])\n",
    "        \n",
    "        if '-' in years: \n",
    "            years_array = years.split('-') \n",
    "            ranged_item = list(range(int(years_array[0]), int(years_array[1]) + 1))\n",
    "            years_experience.extend(ranged_item)\n",
    "        elif int(years) > 15: \n",
    "            continue\n",
    "        else: years_experience.append(int(years))\n",
    "    \n",
    "    experience_db = pd.DataFrame()\n",
    "    experience_db['years_experience_recorded'] = years_experience\n",
    "    experience_db['job_id'] = job_id\n",
    "    \n",
    "    experience_db.to_csv(output_folder + \"\\\\\" + 'experience' + \"\\\\\"  + word_docx_filename + '.csv', index=False)\n",
    "\n",
    "def get_job_id(job_name: str, file_path: str) -> int:\n",
    "    \"\"\"Retrieve or assign a unique job ID based on the job title.\n",
    "    \n",
    "    Args:\n",
    "    job_name (str): The name of the job to retrieve or create an ID for.\n",
    "    filepath (str): Path to the CSV file containing job categories.\n",
    "    \n",
    "    Returns:\n",
    "    int: The job ID.\n",
    "    \"\"\"\n",
    "    \n",
    "    job_id_db = pd.read_csv(file_path)\n",
    "    \n",
    "    if not (job_id_db['job_title'].eq(job_name)).any(): # If doesnt exists in job_categories.csv\n",
    "        \n",
    "        # Set new id value\n",
    "        new_job_id = len(job_id_db)\n",
    "        new_data = pd.DataFrame({'job_id': new_job_id, 'job_title': job_name})\n",
    "        #job_id_db = job_id_db.append({'job_id': new_job_id, 'job_title': job_name}, ignore_index=True)\n",
    "        job_id_db = pd.concat([job_id_db, new_data], ignore_index=True)\n",
    "        \n",
    "        # Order them numerically\n",
    "        job_id_db = job_id_db.sort_values(by=[\"job_id\"], ascending=True, ignore_index=True) \n",
    "        \n",
    "        # Save file back as updated csv file \n",
    "        job_id_db.to_csv(filepath, index=False)\n",
    "        \n",
    "        return len(job_id_db)\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        job_id_col = job_id_db[\"job_id\"]\n",
    "        id = job_id_db.loc[job_id_db[\"job_title\"] == job_name][\"job_id\"].iloc[0]\n",
    "        return id\n",
    "       \n",
    "def write_dictionary_to_cleansed_layer(measurement_dictionary: dict, job_id: str, output_folder: str, folder: str, word_docx_filename: str): \n",
    "    \"\"\"Write creates a dataframe object from dictionary passed, and writes it into a csv with the same file name\n",
    "    \n",
    "    Args:\n",
    "    measurement_dictionary (dict): frequency dictionary.\n",
    "    job_id (int): unique job id.\n",
    "    output_folder (str): path to cleansed layer.\n",
    "    folder (str): job specific folder in cleansed layer.\n",
    "    word_docx_filename (str): name of file being used\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    file_name_split = word_docx_filename.split('_')\n",
    "    state = file_name_split[1]\n",
    "    report_year = file_name_split[2].split('-')[0]\n",
    "    \n",
    "    for name, dictionary in measurement_dictionary.items():\n",
    "        \n",
    "        dataframe = pd.DataFrame(list(measurement_dictionary.items()), columns = [folder, 'frequency'])\n",
    "        dataframe['job_id'] = job_id  # Adding job_id column\n",
    "        dataframe['state'] = state  # Adding state column\n",
    "        dataframe['report_year'] = report_year  # Adding report year column\n",
    "        dataframe.to_csv(output_folder + \"\\\\\" + folder + \"\\\\\"  + word_docx_filename + '.csv', index=False)\n",
    "        \n",
    "def mark_phrases(word_doc_text: str, words_for_marking_desc: list) -> str:\n",
    "    ''' Function takes multiple worded phrases from dictionaries and replaced space with '-' in order to mark them before counting \n",
    "        \n",
    "        Args: \n",
    "        word_doc_text (str): word documented converted into a string \n",
    "        words_for_marking (list) : list of phrases that will be marked to count accurately \n",
    "        \n",
    "        Returns: \n",
    "        word_doc_text (str): word document with marked text '''\n",
    "    for phrase in words_for_marking_desc:\n",
    "\n",
    "        word_doc_text = re.sub(phrase, phrase.replace(' ', '-'), word_doc_text)\n",
    "        \n",
    "    return word_doc_text\n",
    "\n",
    "def find_special_characters(s):\n",
    "    # This pattern matches any character that is not a letter or a number\n",
    "    pattern = re.compile('[^a-zA-Z0-9]')\n",
    "    # Find all non-alphanumeric characters in the string\n",
    "    special_chars = pattern.findall(s)\n",
    "    unique_special_chars = set(special_chars)\n",
    "    \n",
    "    for special_character in unique_special_chars: \n",
    "        s = s.replace(special_character, \"\\\\\" + special_character)\n",
    "    return s\n",
    "\n",
    "def create_key_glossary_from_dict_shells(dictionary_skeletons_folder: str):\n",
    "    # Specify the path to the file\n",
    "    glossary_path = dictionary_skeletons_folder + '\\dict_key_glossary.csv'\n",
    "    \n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.exists(glossary_path):\n",
    "        glossary_db = pd.read_csv(glossary_path)\n",
    "    else:\n",
    "        glossary_db = pd.DataFrame(columns=['keys'])\n",
    "        \n",
    "        for file in os.listdir(dictionary_skeletons_folder):\n",
    "            if file.endswith('.csv'):\n",
    "                keyword_list = pd.read_csv(dictionary_skeletons_folder + r\"\\\\\" +  file).iloc[:, 0].tolist()\n",
    "                new_df = pd.DataFrame(keyword_list, columns=['keys'])\n",
    "                glossary_db = pd.concat([glossary_db, new_df], ignore_index=True)\n",
    "        \n",
    "        glossary_db.to_csv(glossary_path, index=False)\n",
    "        \n",
    "    list_of_hypend_words = [x for x in glossary_db.iloc[:, 0].tolist() if \"-\" in x]\n",
    "    words_without_hyphens = list(map(lambda word: word.replace('-', ' '), list_of_hypend_words))\n",
    "    words_without_hyphens_desc = sorted(words_without_hyphens, key=len, reverse=True)\n",
    "    return words_without_hyphens_desc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a1c78d54-3258-4034-97bb-62f40c990b75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data analyst_dmv_2024-05-05_page1.docx\n",
      "data analyst_dmv_2024-05-05_page10.docx\n",
      "data analyst_dmv_2024-05-05_page11.docx\n",
      "data analyst_dmv_2024-05-05_page12.docx\n",
      "data analyst_dmv_2024-05-05_page2.docx\n",
      "data analyst_dmv_2024-05-05_page3.docx\n",
      "data analyst_dmv_2024-05-05_page4.docx\n",
      "data analyst_dmv_2024-05-05_page5.docx\n",
      "data analyst_dmv_2024-05-05_page6.docx\n",
      "data analyst_dmv_2024-05-05_page7.docx\n",
      "data analyst_dmv_2024-05-05_page8.docx\n",
      "data analyst_dmv_2024-05-05_page9.docx\n",
      "software engineer_dmv_2024-05-05_page1.docx\n",
      "software engineer_dmv_2024-05-05_page10.docx\n",
      "software engineer_dmv_2024-05-05_page11.docx\n",
      "software engineer_dmv_2024-05-05_page12.docx\n",
      "software engineer_dmv_2024-05-05_page13.docx\n",
      "software engineer_dmv_2024-05-05_page14.docx\n",
      "software engineer_dmv_2024-05-05_page15.docx\n",
      "software engineer_dmv_2024-05-05_page16.docx\n",
      "software engineer_dmv_2024-05-05_page17.docx\n",
      "software engineer_dmv_2024-05-05_page18.docx\n",
      "software engineer_dmv_2024-05-05_page19.docx\n",
      "software engineer_dmv_2024-05-05_page2.docx\n",
      "software engineer_dmv_2024-05-05_page3.docx\n",
      "software engineer_dmv_2024-05-05_page4.docx\n",
      "software engineer_dmv_2024-05-05_page5.docx\n",
      "software engineer_dmv_2024-05-05_page6.docx\n",
      "software engineer_dmv_2024-05-05_page7.docx\n",
      "software engineer_dmv_2024-05-05_page8.docx\n",
      "software engineer_dmv_2024-05-05_page9.docx\n"
     ]
    }
   ],
   "source": [
    "input_folder = str(current_dir) + r\"\\cleansed_files\"\n",
    "output_folder = str(current_dir) + r\"\\curated_files\"\n",
    "job_category_folder = str(current_dir) + r\"\\artifacts\\dependencies\\job_categories.csv\"\n",
    "dictionary_skeletons_folder = str(current_dir) + r\"\\artifacts\\dictionary_shells\"\n",
    "\n",
    "words_for_marking_desc = create_key_glossary_from_dict_shells(dictionary_skeletons_folder)\n",
    "unique_job_names = create_cleansed_enviornment(list_of_files, output_folder)\n",
    "list_of_files = os.listdir(input_folder)\n",
    "\n",
    "\n",
    "for job_titles in unique_job_names: \n",
    "\n",
    "    \n",
    "    education_dictionary = create_dictionary_from_csv(dictionary_skeletons_folder + r\"\\education.csv\")\n",
    "    prog_lang_dictionary = create_dictionary_from_csv(dictionary_skeletons_folder + r\"\\programming_languages.csv\")\n",
    "    personality_traits_dict = create_dictionary_from_csv(dictionary_skeletons_folder + r\"\\personality_traits.csv\")\n",
    "    skills_dict = create_dictionary_from_csv(dictionary_skeletons_folder + r\"\\skills.csv\")\n",
    "    software_dict = create_dictionary_from_csv(dictionary_skeletons_folder + r\"\\software.csv\")\n",
    "    security_clearance = create_dictionary_from_csv(dictionary_skeletons_folder + r\"\\security_clearance.csv\")\n",
    "    \n",
    "    job_id = get_job_id(job_titles, job_category_folder)\n",
    "    job_specific_files = [x for x in list_of_files if job_titles in x]\n",
    "    \n",
    "    for word_docx_filename in job_specific_files:\n",
    "        print(word_docx_filename)\n",
    "        counter = 0\n",
    "        \n",
    "        document_to_string = read_word(input_folder + r\"\\\\\" +  word_docx_filename)\n",
    "        \n",
    "        extract_experience_from_paragraph(job_id, output_folder, word_docx_filename, document_to_string)\n",
    "\n",
    "        extract_salary_from_paragraph(job_id, output_folder, word_docx_filename, document_to_string)\n",
    "       \n",
    "        document_to_string_v2 = mark_phrases(document_to_string, words_for_marking_desc)\n",
    "        \n",
    "        \n",
    "        for dictionaries in [education_dictionary, prog_lang_dictionary, personality_traits_dict, skills_dict, software_dict, security_clearance]: \n",
    "            for keys in dictionaries:\n",
    "                regex_key =  find_special_characters(keys)\n",
    "                value = len(re.findall(r\"\\.?\\s\\,?\\(?\" + regex_key + r\"\\,?\\)?\\.?\\s\", document_to_string_v2))\n",
    "                dictionaries[keys] = value\n",
    "                \n",
    "            \n",
    "            partition_folders = ['education', 'programming_languages', 'personality_traits', 'skillset', 'software', 'security_clearance']\n",
    "            write_dictionary_to_cleansed_layer(dictionaries, job_id, output_folder, partition_folders[counter], word_docx_filename[:-4])\n",
    "            counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a309b9a-43fa-4183-a074-16933ab80925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cloud infrastructure management',\n",
       " 'linux system administration',\n",
       " 'personal finance management',\n",
       " 'health and safety awareness',\n",
       " 'online collaboration tools',\n",
       " 'secret security clearance',\n",
       " 'quality assurance testing',\n",
       " 'virtual meeting etiquette',\n",
       " 'cloud storage and sharing',\n",
       " 'editing and proofreading',\n",
       " 'artificial intelligence',\n",
       " 'social media management',\n",
       " 'diversity and inclusion',\n",
       " 'environmental awareness',\n",
       " 'digital nomadism skills',\n",
       " 'public trust clearance',\n",
       " 'mobile app development',\n",
       " 'computational thinking',\n",
       " 'performance management',\n",
       " 'emotional intelligence',\n",
       " 'stakeholder management',\n",
       " 'data privacy awareness',\n",
       " 'azure virtual machines',\n",
       " 'google cloud functions',\n",
       " 'microsoft office suite',\n",
       " 'blockchain technology',\n",
       " 'e commerce management',\n",
       " 'regulatory compliance',\n",
       " 'operations management',\n",
       " 'written communication',\n",
       " 'time zone sensitivity',\n",
       " 'google compute engine',\n",
       " 'software development',\n",
       " 'network architecture',\n",
       " 'statistical analysis',\n",
       " 'business development',\n",
       " 'verbal communication',\n",
       " 'interpersonal skills',\n",
       " 'critical observation',\n",
       " 'willingness to learn',\n",
       " 'customer orientation',\n",
       " 'information literacy',\n",
       " 'cybersecurity basics',\n",
       " 'google cloud storage',\n",
       " 'adobe creative cloud',\n",
       " 'microsoft sql server',\n",
       " 'high school diploma',\n",
       " 'database management',\n",
       " 'accounting software',\n",
       " 'digital photography',\n",
       " 'attention to detail',\n",
       " 'conflict resolution',\n",
       " 'resource allocation',\n",
       " 'self paced learning',\n",
       " 'highschool diploma',\n",
       " 'easily discouraged',\n",
       " 'project management',\n",
       " 'data visualization',\n",
       " 'financial modeling',\n",
       " 'strategic planning',\n",
       " 'marketing strategy',\n",
       " 'product management',\n",
       " 'cultural awareness',\n",
       " 'azure blob storage',\n",
       " 'azure sql database',\n",
       " 'microsoft onedrive',\n",
       " 'bachelor s degree',\n",
       " 'assembly language',\n",
       " 'seo/sem marketing',\n",
       " 'digital marketing',\n",
       " 'technical writing',\n",
       " 'quantum computing',\n",
       " 'augmented reality',\n",
       " 'technical support',\n",
       " 'work life balance',\n",
       " 'stress management',\n",
       " 'critical thinking',\n",
       " 'change management',\n",
       " 'dispute mediation',\n",
       " 'personal branding',\n",
       " 'e learning skills',\n",
       " 'red hat openshift',\n",
       " 'oracle jdeveloper',\n",
       " 'wolfram language',\n",
       " 'secret clearance',\n",
       " 'machine learning',\n",
       " 'video production',\n",
       " 'devops practices',\n",
       " 'project planning',\n",
       " 'customer service',\n",
       " 'active listening',\n",
       " 'ethical judgment',\n",
       " 'digital literacy',\n",
       " 'google cloud sql',\n",
       " 'jupyter notebook',\n",
       " 'google workspace',\n",
       " 'oracle erp cloud',\n",
       " 'google analytics',\n",
       " 'norton antivirus',\n",
       " 'one dimensional',\n",
       " 'unself critical',\n",
       " 'shell scripting',\n",
       " 'secret security',\n",
       " 'cloud computing',\n",
       " 'web development',\n",
       " 'content writing',\n",
       " 'email marketing',\n",
       " 'virtual reality',\n",
       " 'ethical hacking',\n",
       " 'time management',\n",
       " 'self motivation',\n",
       " 'self discipline',\n",
       " 'problem solving',\n",
       " 'decision making',\n",
       " 'feedback giving',\n",
       " 'risk management',\n",
       " 'public speaking',\n",
       " 'open mindedness',\n",
       " 'service mindset',\n",
       " 'online research',\n",
       " 'azure functions',\n",
       " 'microsoft teams',\n",
       " 'oracle database',\n",
       " 'mcafee security',\n",
       " 'masters degree',\n",
       " 'self indulgent',\n",
       " 'small thinking',\n",
       " 'self conscious',\n",
       " 'self sufficent',\n",
       " 'top secret sci',\n",
       " 'graphic design',\n",
       " 'cloud security',\n",
       " 'sales strategy',\n",
       " 'github actions',\n",
       " 'android studio',\n",
       " 'autodesk revit',\n",
       " 'muddle headed',\n",
       " 'narrow minded',\n",
       " 'single minded',\n",
       " 'strong willed',\n",
       " 'high spirited',\n",
       " 'multi leveled',\n",
       " 'self critical',\n",
       " 'self defacing',\n",
       " 'object pascal',\n",
       " 'data analysis',\n",
       " 'team building',\n",
       " 'alibaba cloud',\n",
       " 'apache hadoop',\n",
       " 'cloud foundry',\n",
       " 'mongodb atlas',\n",
       " 'visual studio',\n",
       " 'intellij idea',\n",
       " 'visual studio',\n",
       " 'aptana studio',\n",
       " 'sprout social',\n",
       " 'money minded',\n",
       " 'power hungry',\n",
       " 'well meaning',\n",
       " 'big thinking',\n",
       " 'old fashined',\n",
       " 'clear headed',\n",
       " 'good natured',\n",
       " 'self denying',\n",
       " 'self reliant',\n",
       " 'well rounded',\n",
       " 'public trust',\n",
       " 'complex data',\n",
       " 'ux/ui design',\n",
       " 'crm software',\n",
       " 'cad software',\n",
       " 'goal setting',\n",
       " 'supply chain',\n",
       " 'google cloud',\n",
       " 'vmware cloud',\n",
       " 'gitlab ci/cd',\n",
       " 'apache kafka',\n",
       " 'sublime text',\n",
       " 'google drive',\n",
       " 'high handed',\n",
       " 'weak willed',\n",
       " 'high minded',\n",
       " 'objective c',\n",
       " 'common lisp',\n",
       " 'q clearance',\n",
       " 'l clearance',\n",
       " 'erp systems',\n",
       " '3d printing',\n",
       " 'data mining',\n",
       " 'remote work',\n",
       " 'light table',\n",
       " 'komodo edit',\n",
       " 'zend studio',\n",
       " 'eclipse che',\n",
       " 'cisco webex',\n",
       " 'fun loving',\n",
       " 'many sided',\n",
       " 'korn shell',\n",
       " 'top secret',\n",
       " 'e literacy',\n",
       " 'aws lambda',\n",
       " 'amazon rds',\n",
       " 'komodo ide',\n",
       " 'qt creator',\n",
       " 'aws cloud9',\n",
       " 'one sided',\n",
       " 'well bred',\n",
       " 'well read',\n",
       " 'excel vba',\n",
       " 'ibm cloud',\n",
       " 'ms office',\n",
       " 'wing ide',\n",
       " 'zoho crm',\n",
       " 'power bi',\n",
       " 'sap erp',\n",
       " 'ts sci',\n",
       " 'ad hoc']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d68d9e33-4392-45c3-aaab-dc546188d0c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reporting Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f920619d-ee1d-4768-82e1-45aa112093d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('headstart_pipeline_code') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb79f3b3-42b8-4625-bd53-3f1f66ee99d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Education Reporting Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5e030258-dad3-4562-aba8-106901a53abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+---------+\n",
      "|job_id|degree_category_id|frequency|\n",
      "+------+------------------+---------+\n",
      "|     0|  bachelors degree|     53.0|\n",
      "|     0|        highschool|      0.0|\n",
      "|     0|    masters degree|     16.0|\n",
      "|     0|               phd|      0.0|\n",
      "|     4|  bachelors degree|     30.0|\n",
      "|     4|        highschool|      1.0|\n",
      "|     4|    masters degree|     16.0|\n",
      "|     4|               phd|     12.0|\n",
      "+------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_input = str(current_dir) + r\"\\curated_files\"\n",
    "s3_output = str(current_dir) + r\"\\reporting_layer\"\n",
    "skeletons_mapping_folder = str(current_dir) + r\"\\artifacts\\category_mapping\"\n",
    "\n",
    "\n",
    "education = spark.read.option(\"header\", True).csv('education_test.csv')\n",
    "education_map = spark.read.option(\"header\", True).csv(skeletons_mapping_folder + r\"\\education_category_mapping.csv\")\n",
    "\n",
    "education.createOrReplaceTempView(\"education_database\") \n",
    "education_map.createOrReplaceTempView(\"edu_map\") \n",
    "\n",
    "mapped_database = spark.sql('''\n",
    "        SELECT ed.job_id, em.degree_category_id, SUM(ed.frequency) AS frequency\n",
    "        FROM education_database ed\n",
    "        LEFT JOIN edu_map em \n",
    "            ON ed.education = em.degree \n",
    "        GROUP BY em.degree_category_id, ed.job_id\n",
    "        ORDER BY ed.job_id ASC, em.degree_category_id\n",
    "    ''')\n",
    "\n",
    "\n",
    "mapped_database.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bba4e6b-7de9-448a-af70-10bcb062e231",
   "metadata": {},
   "source": [
    "# Job Description Experience In Years For Positions Reporting Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4cb960e-7fd6-4f73-9f2a-e5f5b764ecc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+---------+\n",
      "|job_id|years_experience|frequency|\n",
      "+------+----------------+---------+\n",
      "|     0|               1|        4|\n",
      "|     0|               2|       11|\n",
      "|     0|               3|       17|\n",
      "|     0|               4|        9|\n",
      "|     0|               5|       15|\n",
      "|     0|               6|        7|\n",
      "|     0|               7|        3|\n",
      "|     0|               8|        8|\n",
      "|     0|               9|        4|\n",
      "|     0|              10|        9|\n",
      "|     0|              11|        3|\n",
      "|     0|              12|        3|\n",
      "|     0|              13|        2|\n",
      "|     0|              14|        3|\n",
      "|     0|              15|        2|\n",
      "|     0|              16|        1|\n",
      "|     1|               3|        1|\n",
      "|     1|               4|        2|\n",
      "|     1|               5|        3|\n",
      "|     1|               6|        1|\n",
      "|     1|               8|        2|\n",
      "|     1|               9|        1|\n",
      "|     1|              10|        2|\n",
      "|     1|              14|        1|\n",
      "|     2|               1|        4|\n",
      "|     2|               2|       11|\n",
      "|     2|               3|       16|\n",
      "|     2|               4|        7|\n",
      "|     2|               5|       12|\n",
      "|     2|               6|        6|\n",
      "|     2|               7|        3|\n",
      "|     2|               8|        6|\n",
      "|     2|               9|        3|\n",
      "|     2|              10|        7|\n",
      "|     2|              11|        3|\n",
      "|     2|              12|        3|\n",
      "|     2|              13|        2|\n",
      "|     2|              14|        2|\n",
      "|     2|              15|        2|\n",
      "|     2|              16|        1|\n",
      "+------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_input = str(current_dir) + r\"\\curated_files\"\n",
    "s3_output = str(current_dir) + r\"\\reporting_layer\"\n",
    "\n",
    "experience_db = spark.read.option(\"header\", True).csv('experience_test.csv')\n",
    "\n",
    "experience_db.createOrReplaceTempView('experience_database') \n",
    "\n",
    "result_db = spark.sql(''' SELECT job_id, CAST(years_experience_recorded AS INT) AS years_experience, COUNT(years_experience_recorded) AS frequency\n",
    "                          FROM experience_database \n",
    "                          GROUP BY years_experience_recorded, job_id\n",
    "                          ORDER BY job_id ASC, years_experience ASC\n",
    "    ''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99febec6-6816-485c-a369-b64bf037824f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Job Description Personality Trait Frequencies For Positions Reporting Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9af6c0-7a49-4f04-8117-c08ab2a78e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8169a98-e4bb-4bc6-b83c-ea59b6187352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edccde97-d7f2-48e9-9431-40341d75aa66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32004977-5f6a-46e9-a1b8-9460a5cd8ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0175e278-8004-46da-9d57-49917a3c0d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91c8693-a59e-4f6f-8422-d8b27b6cf625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cea04004-fab7-46ba-8d32-787ef15583ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "eb036fbd-a8d7-49b8-9cfd-abf484f18a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b9a8f44e-26ad-47d2-8f3d-d2056bb2a80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('000', '002', ''),\n",
       " ('$1000', '$5000', ''),\n",
       " ('$90k', '150k', 'year'),\n",
       " ('$90k', '150k', 'year'),\n",
       " ('$140k', '200k', 'year'),\n",
       " ('$1000', '$5000', ''),\n",
       " ('$140k', '200k', 'year'),\n",
       " ('800', '53', ''),\n",
       " ('800', '53', ''),\n",
       " ('800', '53', ''),\n",
       " ('$57,737', '$98,153', '')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3f5619aa-93f1-4ff3-bdc6-02c83c8c11de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([90000, 90000, 140000, 140000, 57737],\n",
       " [150000, 150000, 200000, 200000, 98153])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_salary(salary_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "02c124ae-8271-4751-abbb-c2f1654d4f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Hello': 3, 'to': 2, 'world': 1, 'everyone': 1, 'This': 1, 'is': 1, 'an': 1, 'example': 1, 'demonstrate': 1, 'how': 1, 'count': 1, 'word': 1, 'frequencies': 1, 'including': 1, 'words': 1, 'like': 1, 'co-op': 1, 'and': 1, \"O'Reilly\": 1, 'again': 1})\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Example text\n",
    "text = \"Hello world! Hello everyone. This is an example to demonstrate how to count word frequencies, including words like co-op and O'Reilly. Hello again.\"\n",
    "\n",
    "# Regex pattern to include words, apostrophes in words, and hyphens\n",
    "pattern = r'\\b[\\w\\'-]+\\b'\n",
    "\n",
    "# Use re.findall to extract words based on the pattern\n",
    "words = re.findall(pattern, text)\n",
    "\n",
    "# Use Counter to create a dictionary of word frequencies\n",
    "word_frequencies = Counter(words)\n",
    "\n",
    "# Output the frequency dictionary\n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "100ed98d-4165-47d5-a1ba-e73e2a9703b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hello': 3,\n",
       " 'world': 1,\n",
       " 'everyone': 1,\n",
       " 'This': 1,\n",
       " 'is': 1,\n",
       " 'an': 1,\n",
       " 'example': 1,\n",
       " 'to': 2,\n",
       " 'demonstrate': 1,\n",
       " 'how': 1,\n",
       " 'count': 1,\n",
       " 'word': 1,\n",
       " 'frequencies': 1,\n",
       " 'including': 1,\n",
       " 'words': 1,\n",
       " 'like': 1,\n",
       " 'co-op': 1,\n",
       " 'and': 1,\n",
       " \"O'Reilly\": 1,\n",
       " 'again': 1}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eedf65c5-6ab6-49ab-99f7-111d5eb881f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific words/phrases counts: {'data science': 1, 'machine learning': 2, 'python': 1, 'learning': 0}\n",
      "Remaining words count: Counter({'and': 1, 'are': 1, 'popular': 1, 'fields': 1, 'many': 1, 'use': 1, 'for': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Example dictionary and text\n",
    "words_phrases_dict = {\n",
    "    \"data science\": 0,\n",
    "    \"machine learning\": 0,\n",
    "    \"python\": 0,\n",
    "    \"learning\": 0\n",
    "}\n",
    "text = \"Data science and machine learning are popular fields. Many use python for machine learning.\"\n",
    "text = text.lower()\n",
    "sorted_keys = sorted(words_phrases_dict.keys(), key=len, reverse=True)\n",
    "\n",
    "for key in sorted_keys:\n",
    "    pattern = re.escape(key)\n",
    "    matches = re.findall(pattern, text)\n",
    "    words_phrases_dict[key] += len(matches)\n",
    "    text = re.sub(pattern, \"\", text)\n",
    "\n",
    "# Optional: count remaining words\n",
    "remaining_words = re.findall(r'\\b\\w+\\b', text)\n",
    "remaining_count = Counter(remaining_words)\n",
    "\n",
    "# Output the results\n",
    "print(\"Specific words/phrases counts:\", words_phrases_dict)\n",
    "print(\"Remaining words count:\", remaining_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50781a-d7df-4cf9-b535-0e0a9bcc9789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
