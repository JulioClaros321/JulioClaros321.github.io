{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b94fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your UUID is: 5adc5d61-6d95-4fe7-a4e6-ee5d9ba6ed79\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import pyspark\n",
    "import docx\n",
    "from docx import Document\n",
    "import re\n",
    "import csv \n",
    "import uuid\n",
    "\n",
    "myuuid = uuid.uuid4()\n",
    "\n",
    "print('Your UUID is: ' + str(myuuid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed2993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files = r\"C:\\Users\\jclaros\\Downloads\\Python Folder\\Personal\\Indeed Scraper\\raw_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b71d9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_word(path):\n",
    "    '''Creates a string object from a word document of job descriptions div containers'''\n",
    "    doc = Document(path)\n",
    "    full_text = []\n",
    "\n",
    "    # Iterate document object and extract text\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "\n",
    "    # Join and create string object \n",
    "    return '\\n'.join(full_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e668b3-8368-4aee-a3d6-48c1846a059a",
   "metadata": {},
   "source": [
    "# Each word document collected contains about 75 job description div containers, below is how one looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cce1914a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_of_files = os.listdir(raw_files)\n",
    "document_to_string = read_word(raw_files+ \"//\" + list_of_files[0])\n",
    "separate_job_objects = document_to_string.split('\\n\\n--------------------')\n",
    "test = separate_job_objects[49]\n",
    "#separate_job_objects[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103fe3e4-8c21-455f-9d79-98db6b7e67e8",
   "metadata": {},
   "source": [
    "# We want to extract the job description portion and clean up all remaining html tags and grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24b729a9-59eb-4772-b360-f472eae09419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_job_description_paragraph(job_desc_list):\n",
    "    \n",
    "    job_description_string = ''\n",
    "    \n",
    "    phone_number_pattern = r'\\(?\\b\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b'\n",
    "    date_pattern = r'\\(?\\b\\d{4}\\)?[-.\\s]?\\d{2}[-.\\s]?\\d{2}\\b'\n",
    "    remove_top_html_pattern = r'</div><div id=\"jobDescriptionText\"'\n",
    "    remove_bottom_html_pattern = r'</div></div>'\n",
    "    merged_tag_pattern = r'<(\\w{1,2})(\\w+)'\n",
    "    replacement = r'\\2'\n",
    "    first_line_pattern = re.compile(r' class=\"[^\"]*\">\\s')\n",
    "    \n",
    "    tags_to_remove = ['<div>', '</div>', '<p>', '</p>', '<br>', '</br>', '<ul>', '</ul>', '<i>', '</i>', '<b>', '</b>', '<li>', '</li>', '\\n', '\\n+', '<i>', \"'\", '<h4>', \n",
    "                        '</h4>', '</h3>', '<h3>', '<h2>', '</h2>', \"â€™\", r'/', r'\\.00\\b','  +']\n",
    "    \n",
    "    \n",
    "    for job_desc_html in job_desc_list:\n",
    "        try:\n",
    "            job_desc_html_v2 = job_desc_html.split(remove_top_html_pattern)[1]\n",
    "            job_desc_html_v3 = job_desc_html_v2.split(remove_bottom_html_pattern)[0]\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "\n",
    "\n",
    "        for items in tags_to_remove: \n",
    "            job_desc_html_v3 = re.sub(items, ' ', job_desc_html_v3)\n",
    "\n",
    "        \n",
    "        for regex_fliters in [first_line_pattern, phone_number_pattern, date_pattern]:\n",
    "            job_desc_html_v3 = re.sub(regex_fliters, '', job_desc_html_v3)\n",
    "\n",
    "        \n",
    "        refined_job_desc = re.sub(merged_tag_pattern, replacement, job_desc_html_v3)\n",
    "\n",
    "        job_description_string += \"\\u0332\".join(\"Job Description:  \") + refined_job_desc +  \"\\n\\n -------------------------------------------------------------------------------------- \\n\\n \"\n",
    "        \n",
    "    return job_description_string.lower()\n",
    "\n",
    "input_folder = r'C:\\Users\\jclaros\\Downloads\\Python Folder\\Personal\\Indeed Scraper\\raw_files' \n",
    "output_folder = r'C:\\Users\\jclaros\\Downloads\\Python Folder\\Personal\\Indeed Scraper\\cleansed_files'\n",
    "\n",
    "list_of_files = os.listdir(input_folder)\n",
    "\n",
    "list_of_files\n",
    "for word_docx in list_of_files: \n",
    "    \n",
    "    document_to_string = read_word(raw_files + r\"\\\\\" +  word_docx)\n",
    "    separate_job_objects = document_to_string.split('\\n\\n--------------------') # Separate the job from a divider marker my bot set\n",
    "    \n",
    "    repaired_doc = clean_job_description_paragraph(separate_job_objects)\n",
    "    \n",
    "    cleansed_page = Document()\n",
    "    cleansed_page.add_paragraph(repaired_doc)\n",
    "    \n",
    "    \n",
    "    new_file_path = os.path.join(output_folder, word_docx)\n",
    "    cleansed_page.save(new_file_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf07d407-8e44-4237-8742-97eb0aad87c2",
   "metadata": {},
   "source": [
    "# This Cleansed Layer will not take the cleaned up job description paragraphs and use dictionaries to extract word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a0871dbf-ed15-40fc-b1c2-ecff6d23d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary_from_csv(path_to_csv: str) -> dict:\n",
    "    ''' Creates environment folders and partitioning folders when new job title is created \n",
    "    \n",
    "        Args: \n",
    "        path_to_csv (str): string path to csv file needed to make a frequency dictionary\n",
    "        \n",
    "        Returns:\n",
    "        freq_dictionary (dict): blank frequency dictionary '''\n",
    "    \n",
    "    freq_dictionary = {} \n",
    "\n",
    "    with open(path_to_csv, mode='r', newline='') as file: \n",
    "        reader = csv.reader(file)\n",
    "        next(reader, None) # Skips header\n",
    "        for row in reader: \n",
    "            if row:\n",
    "                freq_dictionary[row[0]] = 0\n",
    "    \n",
    "    return freq_dictionary\n",
    "\n",
    "def create_cleansed_enviornment(list_of_files_in_raw: list, output_folder: str) -> list:\n",
    "    ''' Creates environment folders and partitioning folders when new job title is created \n",
    "    \n",
    "        Args: \n",
    "        list_of_files_in_raw (list): list of all files collected using os.listdir\n",
    "        \n",
    "        Returns:\n",
    "        all_job_names (list): list of all unique names collected from raw files listed'''\n",
    "        \n",
    "    partition_folders = ['education', 'programming_languages', 'personality_traits', 'skillset', 'software', 'security_clearance', 'experience', 'salary']\n",
    "    all_job_names = []\n",
    "    \n",
    "    for word_docx_filename in list_of_files_in_raw:\n",
    "\n",
    "        position_title = word_docx_filename.split('_')[0]\n",
    "    \n",
    "        if position_title not in all_job_names: all_job_names.append(position_title)\n",
    "            \n",
    "\n",
    "    for items in partition_folders:\n",
    "        if not os.path.exists(output_folder + \"//\" + items):\n",
    "            os.makedirs(output_folder + \"//\" + items)\n",
    "        \n",
    "\n",
    "        \n",
    "    return all_job_names\n",
    "\n",
    "def clean_salary(job_id: str, output_folder: str, word_docx_filename: str, salary_tuple: tuple):\n",
    "    ''' Cleans salary tuple in order to convert it into two dataframe columns after\n",
    "        \n",
    "        Args: \n",
    "        salary_tuple: tuple of low end, high end, and hourly/salary pay grades \n",
    "        \n",
    "        Returns: \n",
    "        Tuple: a tuple of job low/high end pay ranges '''\n",
    "    \n",
    "    pattern = r'[^a-zA-Z0-9\\s]' # remove special characters\n",
    "    low_end_pay = [] \n",
    "    high_end_pay = [] \n",
    "\n",
    "    \n",
    "    for matches in salary_tuple: \n",
    "        low = re.sub(pattern, '', matches[0]) # remove special characters\n",
    "        high = re.sub(pattern, '', matches[1]) # remove special characters\n",
    "        \n",
    "        low = re.sub('k', '000', low) # replace k with 000\n",
    "        high = re.sub('k', '000', high) \n",
    "        \n",
    "        \n",
    "        if int(low) < 20000 or int(high) < 20000:\n",
    "            \n",
    "            if matches[2] == 'year' or matches[2] == 'annually':\n",
    "                \n",
    "                if len(low) < 5: low = int(low) * 1000\n",
    "                if len(high) < 5: high = int(high) * 1000\n",
    "                \n",
    "                if int(low) < 20000 or int(high) < 20000:\n",
    "                    continue\n",
    "                    \n",
    "            elif matches[2] == 'hour' or matches[2] == 'hourly':\n",
    "                low = int(low) * 40 * 52 # Convert to salary\n",
    "                high = int(high) * 40 * 52 # Convert to salary\n",
    "                \n",
    "            else: \n",
    "                continue\n",
    "        \n",
    "        low_end_pay.append(int(low))\n",
    "        high_end_pay.append(int(high))\n",
    "        \n",
    "    salary_dataframe = pd.DataFrame()\n",
    "    salary_dataframe['pay_low_end'] = low_end_pay\n",
    "    salary_dataframe['pay_high_end'] = high_end_pay\n",
    "    salary_dataframe['job_id'] = job_id\n",
    "    \n",
    "    salary_dataframe.to_csv(output_folder + \"\\\\\" + 'salary' + \"\\\\\"  + word_docx_filename + '.csv', index=False)\n",
    "\n",
    "def repair_experience(job_id: str, output_folder: str, word_docx_filename: str, list_of_experience: list):\n",
    "    print(\"JOB ID\" + str(job_id))\n",
    "    \"\"\"Retrieve or assign a unique job ID based on the job title.\n",
    "    \n",
    "    Args:\n",
    "    list_of_experience: a list of all experience years collected from job descriptions\n",
    "    \n",
    "    Returns:\n",
    "    years_experience: a cleaned list of professional experience\n",
    "    \"\"\"\n",
    "    years_experience = []\n",
    "    \n",
    "    for items in list_of_experience:\n",
    "        years = re.sub(r'[^a-zA-Z0-9-]', '', items[0])\n",
    "        \n",
    "        if '-' in years: \n",
    "            years_array = years.split('-') \n",
    "            ranged_item = list(range(int(years_array[0]), int(years_array[1]) + 1))\n",
    "            years_experience.extend(ranged_item)\n",
    "        elif int(years) > 15: \n",
    "            continue\n",
    "        else: years_experience.append(int(years))\n",
    "    \n",
    "    experience_db = pd.DataFrame()\n",
    "    experience_db['years_experience_recorded'] = years_experience\n",
    "    experience_db['job_id'] = job_id\n",
    "    \n",
    "    experience_db.to_csv(output_folder + \"\\\\\" + 'experience' + \"\\\\\"  + word_docx_filename + '.csv', index=False)\n",
    "\n",
    "def get_job_id(job_name: str, file_path: str) -> int:\n",
    "    \"\"\"Retrieve or assign a unique job ID based on the job title.\n",
    "    \n",
    "    Args:\n",
    "    job_name (str): The name of the job to retrieve or create an ID for.\n",
    "    filepath (str): Path to the CSV file containing job categories.\n",
    "    \n",
    "    Returns:\n",
    "    int: The job ID.\n",
    "    \"\"\"\n",
    "    \n",
    "    job_id_db = pd.read_csv(file_path)\n",
    "    \n",
    "    if not (job_id_db['job_title'].eq(job_name)).any(): # If doesnt exists in job_categories.csv\n",
    "        \n",
    "        # Set new id value\n",
    "        new_job_id = len(job_id_db)\n",
    "        job_id_db = job_id_db.append({'job_id': new_job_id, 'job_title': job_name}, ignore_index=True)\n",
    "        \n",
    "        # Order them numerically\n",
    "        job_id_db = job_id_db.sort_values(by=[\"job_id\"], ascending=True, ignore_index=True) \n",
    "        \n",
    "        # Save file back as updated csv file \n",
    "        job_id_db.to_csv(filepath, index=False)\n",
    "        \n",
    "        return len(job_id_db)\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        job_id_col = job_id_db[\"job_id\"]\n",
    "        id = job_id_db.loc[job_id_db[\"job_title\"] == job_name][\"job_id\"].iloc[0]\n",
    "        return id\n",
    "       \n",
    "def write_dictionary_to_cleansed_layer(measurement_dictionary: dict, job_id: str, output_folder: str, folder: str): \n",
    "    \n",
    "    for name, dictionary in measurement_dictionary.items():\n",
    "        \n",
    "        dataframe = pd.DataFrame(list(measurement_dictionary.items()), columns = [folder, 'frequency'])\n",
    "        dataframe['job_id'] = job_id  # Adding job_id column\n",
    "        dataframe.to_csv(output_folder + \"\\\\\" + folder + \"\\\\\"  + word_docx_filename + '.csv', index=False)\n",
    "        \n",
    "def mark_phrases(list_of_measurement_dictionaries: list, word_doc_text: str) -> str:\n",
    "    ''' Function takes multiple worded phrases from dictionaries and replaced space with '-' in order to mark them before counting \n",
    "        \n",
    "        Args: \n",
    "        list_of_measurement_dictionaries (list): list of dictionaries from csvs \n",
    "        word_doc_text (str): word documented converted into a string \n",
    "        \n",
    "        Returns: \n",
    "        word_doc_text (str): word document with marked text '''\n",
    "    \n",
    "    word_list = [] \n",
    "    for dictionaries in list_of_measurement_dictionaries: \n",
    "        temp_list = []\n",
    "        keys_list = list(dictionaries.keys())\n",
    "        for word in keys_list: \n",
    "            if '-' in word: \n",
    "                repaired_word = word.replace('-', ' ') \n",
    "                temp_list.append(repaired_word)\n",
    "        word_list += temp_list\n",
    "        \n",
    "        word_list_desc = sorted(word_list, key=len, reverse=True)\n",
    "        \n",
    "    for phrase in word_list_desc:\n",
    "        \n",
    "        word_doc_text = re.sub(phrase, phrase.replace(' ', '-'), word_doc_text)\n",
    "        \n",
    "    return word_doc_text\n",
    "\n",
    "def find_special_characters(s):\n",
    "    # This pattern matches any character that is not a letter or a number\n",
    "    pattern = re.compile('[^a-zA-Z0-9]')\n",
    "    # Find all non-alphanumeric characters in the string\n",
    "    special_chars = pattern.findall(s)\n",
    "    unique_special_chars = set(special_chars)\n",
    "    \n",
    "    for special_character in unique_special_chars: \n",
    "        s = s.replace(special_character, \"\\\\\" + special_character)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a1c78d54-3258-4034-97bb-62f40c990b75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data analyst_dc_2024-03-22_page1.docx\n",
      "JOB ID0\n",
      "data analyst_dc_2024-03-22_page10.docx\n",
      "JOB ID0\n",
      "data analyst_dc_2024-03-22_page11.docx\n",
      "JOB ID0\n",
      "data analyst_dc_2024-03-22_page12.docx\n",
      "JOB ID0\n",
      "data analyst_dc_2024-03-22_page2.docx\n",
      "JOB ID0\n",
      "data analyst_dc_2024-03-22_page3.docx\n",
      "JOB ID0\n",
      "data analyst_dc_2024-03-22_page4.docx\n",
      "JOB ID0\n",
      "data analyst_dc_2024-03-22_page5.docx\n",
      "JOB ID0\n",
      "data analyst_dc_2024-03-22_page6.docx\n",
      "JOB ID0\n",
      "data analyst_dc_2024-03-22_page7.docx\n",
      "JOB ID0\n",
      "data analyst_dc_2024-03-22_page8.docx\n",
      "JOB ID0\n",
      "data analyst_dc_2024-03-22_page9.docx\n",
      "JOB ID0\n",
      "data engineer_dc_2024-03-22_page1.docx\n",
      "JOB ID1\n",
      "data engineer_dc_2024-03-22_page2.docx\n",
      "JOB ID1\n",
      "devops_dc_2024-03-22_page1.docx\n",
      "JOB ID2\n",
      "devops_dc_2024-03-22_page2.docx\n",
      "JOB ID2\n"
     ]
    }
   ],
   "source": [
    "input_folder = r'C:\\Users\\jclaros\\Downloads\\Python Folder\\Personal\\Indeed Scraper\\cleansed_files'\n",
    "output_folder = r'C:\\Users\\jclaros\\Downloads\\Python Folder\\Personal\\Indeed Scraper\\curated_files'\n",
    "filepath = r'C:\\Users\\jclaros\\Downloads\\Python Folder\\Personal\\Indeed Scraper\\artifacts\\dictionaries_skeletons\\job_categories.csv'\n",
    "artifacts_folder = r'C:\\Users\\jclaros\\Downloads\\Python Folder\\Personal\\Indeed Scraper\\artifacts'\n",
    "\n",
    "    \n",
    "list_of_files = os.listdir(input_folder)\n",
    "\n",
    "unique_job_names = create_cleansed_enviornment(list_of_files, output_folder)\n",
    "\n",
    "for job_titles in unique_job_names: \n",
    "\n",
    "    \n",
    "    education_dictionary = create_dictionary_from_csv(artifacts_folder + r\"\\dictionaries_skeletons\\education.csv\")\n",
    "    prog_lang_dictionary = create_dictionary_from_csv(artifacts_folder + r\"\\dictionaries_skeletons\\programming_languages.csv\")\n",
    "    personality_traits_dict = create_dictionary_from_csv(artifacts_folder + r\"\\dictionaries_skeletons\\personality_traits.csv\")\n",
    "    skills_dict = create_dictionary_from_csv(artifacts_folder + r\"\\dictionaries_skeletons\\skills.csv\")\n",
    "    software_dict = create_dictionary_from_csv(artifacts_folder + r\"\\dictionaries_skeletons\\software.csv\")\n",
    "    security_clearance = create_dictionary_from_csv(artifacts_folder + r\"\\dictionaries_skeletons\\security_clearance.csv\")\n",
    "    \n",
    "    job_id = get_job_id(job_titles, filepath)\n",
    "    \n",
    "    job_specific_files = [x for x in list_of_files if job_titles in x]\n",
    "    \n",
    "    for word_docx_filename in job_specific_files:\n",
    "        print(word_docx_filename)\n",
    "        counter = 0\n",
    "        \n",
    "        document_to_string = read_word(input_folder + r\"\\\\\" +  word_docx_filename)\n",
    "        \n",
    "        experience_regex = re.findall(re.compile(r'(\\d+\\+?|\\d+\\s*[-â€“to]\\s*\\d+)\\s*(years?)'), document_to_string)\n",
    "        repair_experience(job_id, output_folder, word_docx_filename, experience_regex)\n",
    "\n",
    "        salary_regex = re.findall(r'(\\$?\\d{1,3}(?:k|,\\d{1,3}|\\d{1,3}))\\s*?(?:to|-)\\s*?(\\$?\\d{1,3}(?:k|,\\d{1,3}|\\d{1,3}))(?:\\s*(?:per\\s+|a\\s+)?(hour|annually|year|yearly))?', document_to_string)\n",
    "        clean_salary(job_id, output_folder, word_docx_filename, salary_regex)\n",
    "       \n",
    "        document_to_string_v2 = mark_phrases([education_dictionary, prog_lang_dictionary, personality_traits_dict, skills_dict, software_dict, security_clearance], document_to_string)\n",
    "        \n",
    "\n",
    "        separated_list_of_words = document_to_string_v2.split(\" \") # Separate the job from a divider marker my bot set\n",
    "        \n",
    "        \n",
    "        for dictionaries in [education_dictionary, prog_lang_dictionary, personality_traits_dict, skills_dict, software_dict, security_clearance]: \n",
    "            for keys in dictionaries:\n",
    "                regex_key =  find_special_characters(keys)\n",
    "                value = len(re.findall(r\"\\.?\\s\\,?\\(?\" + regex_key + r\"\\,?\\)?\\.?\\s\", document_to_string_v2))\n",
    "                dictionaries[keys] = value\n",
    "                \n",
    "            \n",
    "            partition_folders = ['education', 'programming_languages', 'personality_traits', 'skillset', 'software', 'security_clearance']\n",
    "            write_dictionary_to_cleansed_layer(dictionaries, job_id, output_folder, partition_folders[counter])\n",
    "            counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d3023-aa52-4446-aef8-06d8ee54aa0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7b1856-c573-4927-9033-ed2858a327fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e030258-dad3-4562-aba8-106901a53abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0543313-fe06-4e70-97b8-626e0980b343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb960e-7fd6-4f73-9f2a-e5f5b764ecc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3844ee-f408-40a3-8fb3-0646ce0bc6f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9af6c0-7a49-4f04-8117-c08ab2a78e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8169a98-e4bb-4bc6-b83c-ea59b6187352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edccde97-d7f2-48e9-9431-40341d75aa66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32004977-5f6a-46e9-a1b8-9460a5cd8ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0175e278-8004-46da-9d57-49917a3c0d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a91c8693-a59e-4f6f-8422-d8b27b6cf625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('000', '002', ''),\n",
       " ('$1000', '$5000', ''),\n",
       " ('$90k', '150k', 'year'),\n",
       " ('$90k', '150k', 'year'),\n",
       " ('$140k', '200k', 'year'),\n",
       " ('$1000', '$5000', ''),\n",
       " ('$140k', '200k', 'year'),\n",
       " ('800', '53', ''),\n",
       " ('800', '53', ''),\n",
       " ('800', '53', ''),\n",
       " ('$57,737', '$98,153', '')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cea04004-fab7-46ba-8d32-787ef15583ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "eb036fbd-a8d7-49b8-9cfd-abf484f18a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_id(job_name: str, filepath: str) -> int:ase\n",
    "    job_id_db = pd.read_csv(filepath)\n",
    "    \n",
    "    # Check if job_name exists in the DataFrame\n",
    "    if not (job_id_db['job_title'] == job_name).any():\n",
    "        # Assign new job ID and append new row\n",
    "        new_job_id = len(job_id_db)\n",
    "        job_id_db = job_id_db.append({'job_id': new_job_id, 'job_title': job_name}, ignore_index=True)\n",
    "        \n",
    "        # Save the updated DataFrame back to CSV\n",
    "        job_id_db.to_csv(filepath, index=False)\n",
    "        \n",
    "        return new_job_id\n",
    "    else:\n",
    "        # Return the existing job ID\n",
    "        return job_id_db.loc[job_id_db['job_title'] == job_name, 'job_id'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b9a8f44e-26ad-47d2-8f3d-d2056bb2a80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('000', '002', ''),\n",
       " ('$1000', '$5000', ''),\n",
       " ('$90k', '150k', 'year'),\n",
       " ('$90k', '150k', 'year'),\n",
       " ('$140k', '200k', 'year'),\n",
       " ('$1000', '$5000', ''),\n",
       " ('$140k', '200k', 'year'),\n",
       " ('800', '53', ''),\n",
       " ('800', '53', ''),\n",
       " ('800', '53', ''),\n",
       " ('$57,737', '$98,153', '')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3f5619aa-93f1-4ff3-bdc6-02c83c8c11de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([90000, 90000, 140000, 140000, 57737],\n",
       " [150000, 150000, 200000, 200000, 98153])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_salary(salary_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "02c124ae-8271-4751-abbb-c2f1654d4f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Hello': 3, 'to': 2, 'world': 1, 'everyone': 1, 'This': 1, 'is': 1, 'an': 1, 'example': 1, 'demonstrate': 1, 'how': 1, 'count': 1, 'word': 1, 'frequencies': 1, 'including': 1, 'words': 1, 'like': 1, 'co-op': 1, 'and': 1, \"O'Reilly\": 1, 'again': 1})\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Example text\n",
    "text = \"Hello world! Hello everyone. This is an example to demonstrate how to count word frequencies, including words like co-op and O'Reilly. Hello again.\"\n",
    "\n",
    "# Regex pattern to include words, apostrophes in words, and hyphens\n",
    "pattern = r'\\b[\\w\\'-]+\\b'\n",
    "\n",
    "# Use re.findall to extract words based on the pattern\n",
    "words = re.findall(pattern, text)\n",
    "\n",
    "# Use Counter to create a dictionary of word frequencies\n",
    "word_frequencies = Counter(words)\n",
    "\n",
    "# Output the frequency dictionary\n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "100ed98d-4165-47d5-a1ba-e73e2a9703b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hello': 3,\n",
       " 'world': 1,\n",
       " 'everyone': 1,\n",
       " 'This': 1,\n",
       " 'is': 1,\n",
       " 'an': 1,\n",
       " 'example': 1,\n",
       " 'to': 2,\n",
       " 'demonstrate': 1,\n",
       " 'how': 1,\n",
       " 'count': 1,\n",
       " 'word': 1,\n",
       " 'frequencies': 1,\n",
       " 'including': 1,\n",
       " 'words': 1,\n",
       " 'like': 1,\n",
       " 'co-op': 1,\n",
       " 'and': 1,\n",
       " \"O'Reilly\": 1,\n",
       " 'again': 1}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eedf65c5-6ab6-49ab-99f7-111d5eb881f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific words/phrases counts: {'data science': 1, 'machine learning': 2, 'python': 1, 'learning': 0}\n",
      "Remaining words count: Counter({'and': 1, 'are': 1, 'popular': 1, 'fields': 1, 'many': 1, 'use': 1, 'for': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Example dictionary and text\n",
    "words_phrases_dict = {\n",
    "    \"data science\": 0,\n",
    "    \"machine learning\": 0,\n",
    "    \"python\": 0,\n",
    "    \"learning\": 0\n",
    "}\n",
    "text = \"Data science and machine learning are popular fields. Many use python for machine learning.\"\n",
    "text = text.lower()\n",
    "sorted_keys = sorted(words_phrases_dict.keys(), key=len, reverse=True)\n",
    "\n",
    "for key in sorted_keys:\n",
    "    pattern = re.escape(key)\n",
    "    matches = re.findall(pattern, text)\n",
    "    words_phrases_dict[key] += len(matches)\n",
    "    text = re.sub(pattern, \"\", text)\n",
    "\n",
    "# Optional: count remaining words\n",
    "remaining_words = re.findall(r'\\b\\w+\\b', text)\n",
    "remaining_count = Counter(remaining_words)\n",
    "\n",
    "# Output the results\n",
    "print(\"Specific words/phrases counts:\", words_phrases_dict)\n",
    "print(\"Remaining words count:\", remaining_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50781a-d7df-4cf9-b535-0e0a9bcc9789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
