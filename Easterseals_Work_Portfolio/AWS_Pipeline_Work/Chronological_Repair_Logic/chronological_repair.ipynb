{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47d2a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime, time\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "import logging\n",
    "import boto3\n",
    "from pyspark.sql.functions import col\n",
    "import re\n",
    "from pyspark.sql.functions import *\n",
    "import logging\n",
    "import pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql.types import DateType\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "698e9d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('ChronologicalRepair') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665b8a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef4d6308",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+-----------------+---------------+-------------+\n",
      "|   child_id|enrollment_status|status_start_date|status_end_date|pool_end_date|\n",
      "+-----------+-----------------+-----------------+---------------+-------------+\n",
      "|jakeeverson|         Enrolled|         8/1/2022|     12/31/2099|     1/1/2023|\n",
      "|jakeeverson|         Enrolled|         8/1/2022|     12/31/2099|     2/1/2023|\n",
      "|jakeeverson|         Enrolled|         8/1/2022|       1/1/2023|     3/1/2023|\n",
      "|jakeeverson|         Enrolled|         2/2/2023|     12/31/2099|     4/1/2023|\n",
      "|jakeeverson|         Enrolled|         2/2/2023|     12/31/2099|     5/1/2023|\n",
      "|jakeeverson|         Enrolled|         2/2/2023|       6/1/2023|     6/1/2023|\n",
      "|jakeeverson|         Enrolled|         8/1/2022|       2/1/2023|     7/1/2023|\n",
      "|jakeeverson|         Enrolled|         2/2/2023|      7/20/2023|     8/1/2023|\n",
      "| cathywoods|         Enrolled|        10/1/2019|     12/31/2099|     1/1/2023|\n",
      "| cathywoods|         Enrolled|        10/1/2019|       2/1/2023|     2/1/2023|\n",
      "| cathywoods|         Enrolled|        10/1/2019|     12/31/2099|     3/1/2023|\n",
      "| cathywoods|         Enrolled|        10/1/2019|      3/30/2023|     4/1/2023|\n",
      "+-----------+-----------------+-----------------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Example data of what the cleansed layer may look like on a smaller scale'''\n",
    "\n",
    "filtered_output = spark.read.option(\"header\",True).csv(\"test_cleansed_layer.csv\")\n",
    "filtered_output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fae08db7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+-----------------+---------------+-------------+--------------------+\n",
      "|   child_id|enrollment_status|status_start_date|status_end_date|pool_end_date|           chrono_id|\n",
      "+-----------+-----------------+-----------------+---------------+-------------+--------------------+\n",
      "|jakeeverson|         Enrolled|         8/1/2022|     12/31/2099|     1/1/2023|jakeeverson 8/1/2...|\n",
      "|jakeeverson|         Enrolled|         8/1/2022|     12/31/2099|     2/1/2023|jakeeverson 8/1/2...|\n",
      "|jakeeverson|         Enrolled|         8/1/2022|       1/1/2023|     3/1/2023|jakeeverson 8/1/2...|\n",
      "|jakeeverson|         Enrolled|         2/2/2023|     12/31/2099|     4/1/2023|jakeeverson 2/2/2...|\n",
      "|jakeeverson|         Enrolled|         2/2/2023|     12/31/2099|     5/1/2023|jakeeverson 2/2/2...|\n",
      "|jakeeverson|         Enrolled|         2/2/2023|       6/1/2023|     6/1/2023|jakeeverson 2/2/2...|\n",
      "|jakeeverson|         Enrolled|         8/1/2022|       2/1/2023|     7/1/2023|jakeeverson 8/1/2...|\n",
      "|jakeeverson|         Enrolled|         2/2/2023|      7/20/2023|     8/1/2023|jakeeverson 2/2/2...|\n",
      "| cathywoods|         Enrolled|        10/1/2019|     12/31/2099|     1/1/2023|cathywoods 10/1/2...|\n",
      "| cathywoods|         Enrolled|        10/1/2019|       2/1/2023|     2/1/2023|cathywoods 10/1/2...|\n",
      "| cathywoods|         Enrolled|        10/1/2019|     12/31/2099|     3/1/2023|cathywoods 10/1/2...|\n",
      "| cathywoods|         Enrolled|        10/1/2019|      3/30/2023|     4/1/2023|cathywoods 10/1/2...|\n",
      "+-----------+-----------------+-----------------+---------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We make a new column chronoID which is a string created from concating svoc_id, status start date, end date, \n",
    "and pool end date. This plays a role when we filter the cleansed layer and we only want those unique events since \n",
    "each chrono id is unique for each individual event for every student as id,status start,status end, pool end date \n",
    "will never be the same for one student or the same student with different events\n",
    "'''\n",
    "\n",
    "filtered_output = filtered_output.withColumn(\"chrono_id\", concat(filtered_output.child_id,\n",
    "                                                                lit(\" \"), \n",
    "                                                                filtered_output.status_start_date,\n",
    "                                                                lit(\" \"), \n",
    "                                                                filtered_output.status_end_date,\n",
    "                                                                lit(\" \"), \n",
    "                                                                filtered_output.pool_end_date))\n",
    "filtered_output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa415cf6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+-----------------+---------------+-------------+--------------------+\n",
      "|   child_id|enrollment_status|status_start_date|status_end_date|pool_end_date|           chrono_id|\n",
      "+-----------+-----------------+-----------------+---------------+-------------+--------------------+\n",
      "| cathywoods|         Enrolled|        10/1/2019|      3/30/2023|     4/1/2023|cathywoods 10/1/2...|\n",
      "| cathywoods|         Enrolled|        10/1/2019|     12/31/2099|     3/1/2023|cathywoods 10/1/2...|\n",
      "| cathywoods|         Enrolled|        10/1/2019|       2/1/2023|     2/1/2023|cathywoods 10/1/2...|\n",
      "| cathywoods|         Enrolled|        10/1/2019|     12/31/2099|     1/1/2023|cathywoods 10/1/2...|\n",
      "|jakeeverson|         Enrolled|         2/2/2023|      7/20/2023|     8/1/2023|jakeeverson 2/2/2...|\n",
      "|jakeeverson|         Enrolled|         2/2/2023|       6/1/2023|     6/1/2023|jakeeverson 2/2/2...|\n",
      "|jakeeverson|         Enrolled|         2/2/2023|     12/31/2099|     5/1/2023|jakeeverson 2/2/2...|\n",
      "|jakeeverson|         Enrolled|         2/2/2023|     12/31/2099|     4/1/2023|jakeeverson 2/2/2...|\n",
      "|jakeeverson|         Enrolled|         8/1/2022|       2/1/2023|     7/1/2023|jakeeverson 8/1/2...|\n",
      "|jakeeverson|         Enrolled|         8/1/2022|       1/1/2023|     3/1/2023|jakeeverson 8/1/2...|\n",
      "|jakeeverson|         Enrolled|         8/1/2022|     12/31/2099|     2/1/2023|jakeeverson 8/1/2...|\n",
      "|jakeeverson|         Enrolled|         8/1/2022|     12/31/2099|     1/1/2023|jakeeverson 8/1/2...|\n",
      "+-----------+-----------------+-----------------+---------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We sort the dataframe by ordering by child_id, then status start_date ASC, then most recent pool date DESC\n",
    "This comes in handy when we remove deplicates of status_start_date, keeping the first instance which would be the most recent \n",
    "pool based on our ordering style '''\n",
    "\n",
    "\n",
    "filtered_output.createOrReplaceTempView(\"dummy_source\")\n",
    "\n",
    "\n",
    "filtered_output = spark.sql(\"\"\"SELECT *\n",
    "                        FROM dummy_source\n",
    "                        ORDER BY child_id ASC, status_start_date ASC, pool_end_date DESC\"\"\")\n",
    "\n",
    "filtered_output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a1ab711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-----------------+---------------+-------------+--------------------+\n",
      "|  child_id|enrollment_status|status_start_date|status_end_date|pool_end_date|           chrono_id|\n",
      "+----------+-----------------+-----------------+---------------+-------------+--------------------+\n",
      "|cathywoods|         Enrolled|        10/1/2019|      3/30/2023|     4/1/2023|cathywoods 10/1/2...|\n",
      "+----------+-----------------+-----------------+---------------+-------------+--------------------+\n",
      "\n",
      "+-----------+-----------------+-----------------+---------------+-------------+--------------------+\n",
      "|   child_id|enrollment_status|status_start_date|status_end_date|pool_end_date|           chrono_id|\n",
      "+-----------+-----------------+-----------------+---------------+-------------+--------------------+\n",
      "|jakeeverson|         Enrolled|         2/2/2023|      7/20/2023|     8/1/2023|jakeeverson 2/2/2...|\n",
      "|jakeeverson|         Enrolled|         8/1/2022|       2/1/2023|     7/1/2023|jakeeverson 8/1/2...|\n",
      "+-----------+-----------------+-----------------+---------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This chunk of code starts of by making a list of unique student_ids (child_id_list) using collect() method\n",
    "then uses a for loop to iterate through every student id and makes a unqiue data frame for every student in the loop to \n",
    "manipulate using the filter function. After removing duplicates based on subset = status_start_date then reordering\n",
    "it collects the unique chronoIDs we generated at the beginning unless it detects error in chronological order of a student\n",
    "'''\n",
    "\n",
    "\n",
    "child_id_list = list(set(child_id[0] for child_id in filtered_output.select('child_id').collect()))\n",
    "error_ids = []\n",
    "chrono_ids = []\n",
    "\n",
    "for id in child_id_list:\n",
    "    \n",
    "    #create unique dataframe for student based on id in for loop\n",
    "    child_profile = (filtered_output.filter(filtered_output.child_id.isin(id)))\\\n",
    "        .dropDuplicates(subset=[\"status_start_date\"]).orderBy(col(\"status_start_date\").asc())\n",
    "    \n",
    "    chrono_ids.extend([chrono[0] for chrono in child_profile.select('chrono_id').collect()])\n",
    "    \n",
    "    child_profile.show()\n",
    "    \n",
    "    #if student has more than one entry in cleansed, check for chronological order\n",
    "    if child_profile.count() > 1:\n",
    "        counter = 0\n",
    "        start_date = [start[0] for start in child_profile.select('status_start_date').collect()]\n",
    "        end_date = [end[0] for end in child_profile.select('status_end_date').collect()]\n",
    "\n",
    "        while counter < child_profile.count() - 1:\n",
    "            if start_date[counter + 1] <=  end_date[counter]:\n",
    "                if id not in error_ids:\n",
    "                    error_ids.append(id)\n",
    "\n",
    "            counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b8d89bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cathywoods 10/1/2019 3/30/2023 4/1/2023',\n",
       " 'jakeeverson 2/2/2023 7/20/2023 8/1/2023',\n",
       " 'jakeeverson 8/1/2022 2/1/2023 7/1/2023']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' The chronoIDs that were saved to a list as a result of the for loop '''\n",
    "\n",
    "chrono_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71580b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+-----------------+---------------+-------------+--------------------+\n",
      "|   child_id|enrollment_status|status_start_date|status_end_date|pool_end_date|           chrono_id|\n",
      "+-----------+-----------------+-----------------+---------------+-------------+--------------------+\n",
      "| cathywoods|         Enrolled|        10/1/2019|      3/30/2023|     4/1/2023|cathywoods 10/1/2...|\n",
      "|jakeeverson|         Enrolled|         2/2/2023|      7/20/2023|     8/1/2023|jakeeverson 2/2/2...|\n",
      "|jakeeverson|         Enrolled|         8/1/2022|       2/1/2023|     7/1/2023|jakeeverson 8/1/2...|\n",
      "+-----------+-----------------+-----------------+---------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Apply the same rule, now we filter the \"cleansed\" dataframe by our chronoids list that we generated that we can use in our\n",
    "aws write method. Clean up columns commented out for visual purposes.\n",
    "'''\n",
    "\n",
    "correct_dataframe = filtered_output.filter(filtered_output.chrono_id.isin(chrono_ids))\\\n",
    "                   #.drop(\"pool_end_date\").drop(\"chronoID\")\n",
    "correct_dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51323e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
